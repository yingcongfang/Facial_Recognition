+ source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ DATAPROC_DIR=/usr/local/share/google/dataproc
++ DATAPROC_TMP_DIR=/tmp/dataproc
++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
++ INSTALL_GCS_CONNECTOR=1
++ INSTALL_BIGQUERY_CONNECTOR=1
++ ENABLE_HDFS=1
++ ENABLE_HDFS_PERMISSIONS=false
++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
++ HADOOP_CONF_DIR=/etc/hadoop/conf
++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
++ HDFS_MASTER_MEMORY_FRACTION=0.4
++ NODEMANAGER_MEMORY_FRACTION=0.8
++ NUM_WORKERS=10
++ WORKERS=()
++ CORES_PER_MAP_TASK=1.0
++ CORES_PER_REDUCE_TASK=2.0
++ CORES_PER_APP_MASTER=2.0
++ HDFS_DATA_DIRS_PERM=700
++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
++ SPARK_CONF_DIR=/etc/spark/conf
++ SPARK_WORKER_MEMORY_FRACTION=0.8
++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
++ SPARK_DAEMON_MEMORY_FRACTION=0.15
++ SPARK_EXECUTORS_PER_VM=2
++ TEZ_CONF_DIR=/etc/tez/conf
++ TEZ_LIB_DIR=/usr/lib/tez
+ source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
++ is_centos
+++ . /etc/os-release
++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
++++ NAME='Debian GNU/Linux'
++++ VERSION_ID=10
++++ VERSION='10 (buster)'
++++ VERSION_CODENAME=buster
++++ ID=debian
++++ HOME_URL=https://www.debian.org/
++++ SUPPORT_URL=https://www.debian.org/support
++++ BUG_REPORT_URL=https://bugs.debian.org/
+++ echo debian
++ [[ debian == \c\e\n\t\o\s ]]
++ return 1
++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
+++ APT_SENTINEL=apt.lastupdate
++ readonly EXIT_CODE_INTERNAL_ERROR=1
++ EXIT_CODE_INTERNAL_ERROR=1
++ readonly EXIT_CODE_CLIENT_ERROR=2
++ EXIT_CODE_CLIENT_ERROR=2
+ source /usr/local/share/google/dataproc/bdutil/cluster_properties.sh
+ source /usr/local/share/google/dataproc/bdutil/components/components-helpers.sh
+ source /usr/local/share/google/dataproc/bdutil/components/startup-script-components.sh
+ run_with_logger --tag google-dataproc-startup
+ local tag=
+ local pid=948
+ [[ --tag == \-\-\t\a\g ]]
+ tag=google-dataproc-startup
+ shift 2
+ [[ 0 -eq 0 ]]
+ exec
++ logger -s -t 'google-dataproc-startup[948]'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + BACKGROUND_PROCESSES=()
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + BACKGROUND_COMMANDS=()
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + cd /tmp
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + trap logstacktrace ERR
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + loginfo 'Starting Dataproc startup script'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + echo 'Starting Dataproc startup script'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: Starting Dataproc startup script
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + set -a
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_metadata_project_id
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_dataproc_metadata DATAPROC_METADATA_PROJECT_ID ../project/project-id
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r var_name=DATAPROC_METADATA_PROJECT_ID
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r attr_name=../project/project-id
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ [[ -v DATAPROC_METADATA_PROJECT_ID ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ /usr/share/google/get_metadata_value ../project/project-id
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + PROJECT=qwiklabs-gcp-01-d65f02c71c6a
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_metadata_dataproc_region
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_dataproc_metadata DATAPROC_METADATA_REGION attributes/dataproc-region
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r var_name=DATAPROC_METADATA_REGION
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r attr_name=attributes/dataproc-region
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ [[ -v DATAPROC_METADATA_REGION ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ /usr/share/google/get_metadata_value attributes/dataproc-region
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + REGION=us-central1
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_metadata_zone
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_dataproc_metadata DATAPROC_METADATA_ZONE zone
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r var_name=DATAPROC_METADATA_ZONE
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r attr_name=zone
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ [[ -v DATAPROC_METADATA_ZONE ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ /usr/share/google/get_metadata_value zone
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + ZONE=projects/147286270203/zones/us-central1-f
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_metadata_bucket
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_dataproc_metadata DATAPROC_METADATA_BUCKET attributes/dataproc-bucket
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r var_name=DATAPROC_METADATA_BUCKET
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r attr_name=attributes/dataproc-bucket
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ [[ -v DATAPROC_METADATA_BUCKET ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ /usr/share/google/get_metadata_value attributes/dataproc-bucket
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + CONFIGBUCKET=student-01-71e17b39d43c-image-26164
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_metadata_temp_bucket
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_dataproc_metadata DATAPROC_METADATA_TEMP_BUCKET attributes/dataproc-temp-bucket
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r var_name=DATAPROC_METADATA_TEMP_BUCKET
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r attr_name=attributes/dataproc-temp-bucket
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ [[ -v DATAPROC_METADATA_TEMP_BUCKET ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ /usr/share/google/get_metadata_value attributes/dataproc-temp-bucket
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + TEMP_BUCKET=dataproc-temp-us-central1-147286270203-nrwqwbuk
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_metadata_role
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r var_name=DATAPROC_METADATA_ROLE
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r attr_name=attributes/dataproc-role
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ [[ -v DATAPROC_METADATA_ROLE ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ /usr/share/google/get_metadata_value attributes/dataproc-role
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + ROLE=Master
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_metadata_cluster_name
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_dataproc_metadata DATAPROC_METADATA_CLUSTER_NAME attributes/dataproc-cluster-name
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r var_name=DATAPROC_METADATA_CLUSTER_NAME
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r attr_name=attributes/dataproc-cluster-name
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ [[ -v DATAPROC_METADATA_CLUSTER_NAME ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ /usr/share/google/get_metadata_value attributes/dataproc-cluster-name
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + CLUSTER_NAME=student-01-71e17b39d43c-qwiklab
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_metadata_cluster_uuid
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_dataproc_metadata DATAPROC_METADATA_CLUSTER_UUID attributes/dataproc-cluster-uuid
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r var_name=DATAPROC_METADATA_CLUSTER_UUID
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r attr_name=attributes/dataproc-cluster-uuid
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ [[ -v DATAPROC_METADATA_CLUSTER_UUID ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ /usr/share/google/get_metadata_value attributes/dataproc-cluster-uuid
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + CLUSTER_UUID=1508fba4-d944-4b05-b441-abe36766e1e0
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_metadata_worker_count
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_dataproc_metadata DATAPROC_METADATA_WORKER_COUNT attributes/dataproc-worker-count
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r var_name=DATAPROC_METADATA_WORKER_COUNT
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r attr_name=attributes/dataproc-worker-count
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ [[ -v DATAPROC_METADATA_WORKER_COUNT ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ /usr/share/google/get_metadata_value attributes/dataproc-worker-count
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + WORKER_COUNT=2
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_metadata_master
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r var_name=DATAPROC_METADATA_MASTER
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r attr_name=attributes/dataproc-master
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ [[ -v DATAPROC_METADATA_MASTER ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ /usr/share/google/get_metadata_value attributes/dataproc-master
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + DATAPROC_MASTER=student-01-71e17b39d43c-qwiklab-m
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_metadata_master_additional
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r var_name=DATAPROC_METADATA_MASTER_ADDITIONAL
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r attr_name=attributes/dataproc-master-additional
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ [[ -v DATAPROC_METADATA_MASTER_ADDITIONAL ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ /usr/share/google/get_metadata_value attributes/dataproc-master-additional
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + DATAPROC_MASTER_ADDITIONAL=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + MASTER_COUNT=1
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ hostname -s
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + MY_HOSTNAME=student-01-71e17b39d43c-qwiklab-m
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ hostname -f
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + MY_FULL_HOSTNAME=student-01-71e17b39d43c-qwiklab-m.us-central1-f.c.qwiklabs-gcp-01-d65f02c71c6a.internal
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ dnsdomainname
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + DOMAIN=us-central1-f.c.qwiklabs-gcp-01-d65f02c71c6a.internal
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ echo student-01-71e17b39d43c-qwiklab-m
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ sed -r 's/-([mw](-[0-9]*)?)$//'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + PREFIX=student-01-71e17b39d43c-qwiklab
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + DATAPROC_ETC_DIR=/etc/google-dataproc
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + UNINSTALL_TMP_DIR=/tmp/dataproc/uninstall
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + UNINSTALL_PRE_ACTIVATE_TMP_DIR=/tmp/dataproc/uninstall-pre-activate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + KEYTAB_DIR=/etc/security/keytab
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + CLUSTER_STAGING_FOLDER=gs://student-01-71e17b39d43c-image-26164/google-cloud-dataproc-metainfo/1508fba4-d944-4b05-b441-abe36766e1e0
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + CLUSTER_TEMP_FOLDER=gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + COMPONENT_SERVICES=(hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mariadb-server mysql-server spark-history-server)
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + DEFAULT_COMPONENTS=(hdfs yarn mapreduce mysql hive-metastore hive-server2 spark)
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + set +a
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + mkdir -p /tmp/dataproc
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + mkdir -p /tmp/dataproc/commands
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + mkdir -p /tmp/dataproc/components
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + mkdir -p /tmp/dataproc/uninstall
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + mkdir -p /tmp/dataproc/uninstall-pre-activate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + merge_java_properties /tmp/cluster/properties/dataproc.properties /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r src=/tmp/cluster/properties/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r dest=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r 'header=\n# User-supplied properties.'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ ! -f /tmp/cluster/properties/dataproc.properties ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + echo -e '\n# User-supplied properties.'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + cat /tmp/cluster/properties/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + loginfo 'Merged /tmp/cluster/properties/dataproc.properties.'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + echo 'Merged /tmp/cluster/properties/dataproc.properties.'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: Merged /tmp/cluster/properties/dataproc.properties.
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + merge_java_properties /etc/google-dataproc/dataproc.custom.properties /etc/google-dataproc/dataproc.properties '\n# Custom image supplied properties'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r src=/etc/google-dataproc/dataproc.custom.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r dest=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r 'header=\n# Custom image supplied properties'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ ! -f /etc/google-dataproc/dataproc.custom.properties ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + loginfo 'Skipping merging /etc/google-dataproc/dataproc.custom.properties, file does not exist.'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + echo 'Skipping merging /etc/google-dataproc/dataproc.custom.properties, file does not exist.'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: Skipping merging /etc/google-dataproc/dataproc.custom.properties, file does not exist.
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.components.activate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r property_name=dataproc.components.activate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local -r property_name=dataproc.components.activate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ property_value=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ echo ''
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ property_value=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ echo ''
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + OPTIONAL_COMPONENTS_VALUE=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + add_default_components hdfs yarn mapreduce mysql hive-metastore hive-server2 spark
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + default_components=("$@")
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local default_components
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + for component in "${default_components[@]}"
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + is_component_explicitly_unselected hdfs
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r component=hdfs
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local deactivated_components
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_components_to_deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ tr '[:upper:]' '[:lower:]'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r property_name=dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local -r property_name=dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ grep '^dataproc.components.deactivate=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ property_value=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ echo ''
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ property_value=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ echo ''
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + deactivated_components=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ '' == *hdfs* ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + add_optional_component hdfs
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r component=hdfs
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ '' == *hdfs* ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ -z '' ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + OPTIONAL_COMPONENTS_VALUE=hdfs
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + echo dataproc.components.activate=hdfs
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + for component in "${default_components[@]}"
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + is_component_explicitly_unselected yarn
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r component=yarn
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local deactivated_components
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_components_to_deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ tr '[:upper:]' '[:lower:]'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r property_name=dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local -r property_name=dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ grep '^dataproc.components.deactivate=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ property_value=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ echo ''
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ property_value=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ echo ''
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + deactivated_components=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ '' == *yarn* ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + add_optional_component yarn
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r component=yarn
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ hdfs == *yarn* ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ -z hdfs ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + OPTIONAL_COMPONENTS_VALUE+=' yarn'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=hdfs yarn/g' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + for component in "${default_components[@]}"
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + is_component_explicitly_unselected mapreduce
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r component=mapreduce
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local deactivated_components
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_components_to_deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ tr '[:upper:]' '[:lower:]'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r property_name=dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local -r property_name=dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ grep '^dataproc.components.deactivate=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ property_value=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ echo ''
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ property_value=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ echo ''
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + deactivated_components=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ '' == *mapreduce* ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + add_optional_component mapreduce
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r component=mapreduce
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ hdfs yarn == *mapreduce* ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ -z hdfs yarn ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + OPTIONAL_COMPONENTS_VALUE+=' mapreduce'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=hdfs yarn mapreduce/g' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + for component in "${default_components[@]}"
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + is_component_explicitly_unselected mysql
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r component=mysql
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local deactivated_components
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_components_to_deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ tr '[:upper:]' '[:lower:]'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r property_name=dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local -r property_name=dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ grep '^dataproc.components.deactivate=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ property_value=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ echo ''
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ property_value=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ echo ''
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + deactivated_components=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ '' == *mysql* ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + add_optional_component mysql
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r component=mysql
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ hdfs yarn mapreduce == *mysql* ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ -z hdfs yarn mapreduce ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + OPTIONAL_COMPONENTS_VALUE+=' mysql'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=hdfs yarn mapreduce mysql/g' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + for component in "${default_components[@]}"
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + is_component_explicitly_unselected hive-metastore
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r component=hive-metastore
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local deactivated_components
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_components_to_deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r property_name=dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ tr '[:upper:]' '[:lower:]'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local -r property_name=dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ grep '^dataproc.components.deactivate=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ property_value=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ echo ''
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ property_value=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ echo ''
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + deactivated_components=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ '' == *hive-metastore* ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + add_optional_component hive-metastore
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r component=hive-metastore
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ hdfs yarn mapreduce mysql == *hive-metastore* ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ -z hdfs yarn mapreduce mysql ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + OPTIONAL_COMPONENTS_VALUE+=' hive-metastore'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=hdfs yarn mapreduce mysql hive-metastore/g' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + for component in "${default_components[@]}"
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + is_component_explicitly_unselected hive-server2
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r component=hive-server2
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local deactivated_components
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_components_to_deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r property_name=dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local -r property_name=dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ grep '^dataproc.components.deactivate=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ tr '[:upper:]' '[:lower:]'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ property_value=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ echo ''
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ property_value=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ echo ''
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + deactivated_components=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ '' == *hive-server2* ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + add_optional_component hive-server2
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r component=hive-server2
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ hdfs yarn mapreduce mysql hive-metastore == *hive-server2* ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ -z hdfs yarn mapreduce mysql hive-metastore ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + OPTIONAL_COMPONENTS_VALUE+=' hive-server2'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=hdfs yarn mapreduce mysql hive-metastore hive-server2/g' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + for component in "${default_components[@]}"
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + is_component_explicitly_unselected spark
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r component=spark
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local deactivated_components
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_components_to_deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r property_name=dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ tr '[:upper:]' '[:lower:]'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local -r property_name=dataproc.components.deactivate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ grep '^dataproc.components.deactivate=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ property_value=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ echo ''
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ property_value=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ echo ''
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + deactivated_components=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ '' == *spark* ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + add_optional_component spark
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r component=spark
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ hdfs yarn mapreduce mysql hive-metastore hive-server2 == *spark* ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ -z hdfs yarn mapreduce mysql hive-metastore hive-server2 ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + OPTIONAL_COMPONENTS_VALUE+=' spark'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=hdfs yarn mapreduce mysql hive-metastore hive-server2 spark/g' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + is_version_at_least 2.0 2.0
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r version1=2.0
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r version2=2.0
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + loginfo 'Comparing if version 2.0 is at least version 2.0 '
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + echo 'Comparing if version 2.0 is at least version 2.0 '
<13>Mar 21 02:43:06 google-dataproc-startup[948]: Comparing if version 2.0 is at least version 2.0 
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + compare_versions 2.0 2.0
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r version1=2.0
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r version2=2.0
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + validate_version 2.0
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r version=2.0
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ 2.0 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + validate_version 2.0
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r version=2.0
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ 2.0 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ 2.0 == \2\.\0 ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + case $? in
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + add_optional_component miniconda3
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r component=miniconda3
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ hdfs yarn mapreduce mysql hive-metastore hive-server2 spark == *miniconda3* ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ -z hdfs yarn mapreduce mysql hive-metastore hive-server2 spark ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + OPTIONAL_COMPONENTS_VALUE+=' miniconda3'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3/g' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + (( 1 > 1 ))
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + is_component_selected kafka-server
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r component=kafka-server
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local activated_components
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_components_to_activate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.components.activate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r property_name=dataproc.components.activate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ tr '[:upper:]' '[:lower:]'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local -r property_name=dataproc.components.activate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ property_value='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ echo 'hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ property_value='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ echo 'hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + activated_components='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3 == *kafka-server* ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + COMPONENTS_TO_ACTIVATE=(${OPTIONAL_COMPONENTS_VALUE})
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + is_component_selected hdfs
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r component=hdfs
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local activated_components
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_components_to_activate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.components.activate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r property_name=dataproc.components.activate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ tr '[:upper:]' '[:lower:]'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local -r property_name=dataproc.components.activate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ property_value='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ echo 'hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ property_value='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ echo 'hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + activated_components='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3 == *hdfs* ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + export HDFS_ENABLED=true
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + HDFS_ENABLED=true
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + is_component_selected kerberos
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local -r component=kerberos
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + local activated_components
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_components_to_activate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.components.activate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r property_name=dataproc.components.activate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ tr '[:upper:]' '[:lower:]'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local -r property_name=dataproc.components.activate
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ property_value='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ echo 'hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ property_value='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ echo 'hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + activated_components='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3 == *kerberos* ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + export KERBEROS_ENABLED=false
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + KERBEROS_ENABLED=false
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + [[ false == \t\r\u\e ]]
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + HBASE_CONF_DIR=/etc/hbase/conf
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + PIG_CONF_DIR=/etc/pig/conf
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + ZOOKEEPER_CONF_DIR=/etc/zookeeper/conf
<13>Mar 21 02:43:06 google-dataproc-startup[948]: + HADOOP_2_PORTS=(50010 50020 50070 50090)
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ get_property_in_xml /tmp/cluster/properties/core.xml fs.defaultFS
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r path=/tmp/cluster/properties/core.xml
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r property=fs.defaultFS
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local -r default_value=
<13>Mar 21 02:43:06 google-dataproc-startup[948]: ++ local val
<13>Mar 21 02:43:06 google-dataproc-startup[948]: +++ bdconfig get_property_value --configuration_file /tmp/cluster/properties/core.xml --name fs.defaultFS
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ val=None
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ [[ None == \N\o\n\e ]]
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ val=
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ echo ''
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + default_fs=
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + [[ -n '' ]]
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + [[ true == \f\a\l\s\e ]]
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + (( 1 > 1 ))
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + export HDFS_ROOT_URI=hdfs://student-01-71e17b39d43c-qwiklab-m
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + HDFS_ROOT_URI=hdfs://student-01-71e17b39d43c-qwiklab-m
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + [[ Master == \M\a\s\t\e\r ]]
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + hostname=student-01-71e17b39d43c-qwiklab-m
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + [[ false == \t\r\u\e ]]
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + for i in "${!MASTER_HOSTNAMES[@]}"
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + [[ student-01-71e17b39d43c-qwiklab-m == \s\t\u\d\e\n\t\-\0\1\-\7\1\e\1\7\b\3\9\d\4\3\c\-\q\w\i\k\l\a\b\-\m ]]
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + export MASTER_INDEX=0
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + MASTER_INDEX=0
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + break
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + (( 2 == 0 ))
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + (( 1 > 1 ))
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + ARTIFACTS_TO_UNINSTALL=(${DATAPROC_MASTER_HA_SERVICES} ${DATAPROC_WORKER_SERVICES})
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + SERVICES=(${DATAPROC_MASTER_SERVICES} ${DATAPROC_MASTER_EXCLUSIVE_SERVICES} ${DATAPROC_MASTER_STANDALONE_SERVICES})
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + loginfo 'Generating helper scripts'
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + echo 'Generating helper scripts'
<13>Mar 21 02:43:07 google-dataproc-startup[948]: Generating helper scripts
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + cat
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ (( i = 0 ))
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ (( i < 1 ))
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ echo MASTER_HOSTNAME_0=student-01-71e17b39d43c-qwiklab-m
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ (( i++ ))
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ (( i < 1 ))
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + cat
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ cat /usr/local/share/google/dataproc/bdutil/configure_keys.sh /usr/local/share/google/dataproc/bdutil/configure_hadoop.sh /usr/local/share/google/dataproc/bdutil/configure_connectors.sh /usr/local/share/google/dataproc/bdutil/configure_tez.sh /usr/local/share/google/dataproc/bdutil/configure_zookeeper.sh /usr/local/share/google/dataproc/bdutil/configure_docker.sh /usr/local/share/google/dataproc/bdutil/configure_metadata_proxy.sh
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + cp -r /usr/local/share/google/dataproc/bdutil/conf/bq-mapred-template.xml /usr/local/share/google/dataproc/bdutil/conf/capacity-scheduler-template.xml /usr/local/share/google/dataproc/bdutil/conf/core-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/core-template.xml /usr/local/share/google/dataproc/bdutil/conf/distcp-template.xml /usr/local/share/google/dataproc/bdutil/conf/gcs-core-template.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-simplification-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-simplification-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-template.xml /usr/local/share/google/dataproc/bdutil/conf/hive-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hive-template.xml /usr/local/share/google/dataproc/bdutil/conf/knox /usr/local/share/google/dataproc/bdutil/conf/mapred-template.xml /usr/local/share/google/dataproc/b
<13>Mar 21 02:43:07 google-dataproc-startup[948]: dutil/conf/yarn-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-template.xml /tmp
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + cp /usr/local/share/google/dataproc/bdutil/configure_mrv2_mem.py /tmp
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + chmod +x configure_mrv2_mem.py
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + loginfo 'Running helper scripts'
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + echo 'Running helper scripts'
<13>Mar 21 02:43:07 google-dataproc-startup[948]: Running helper scripts
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.localssd.mount.enable
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ local -r property_name=dataproc.localssd.mount.enable
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.localssd.mount.enable
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ local -r property_name=dataproc.localssd.mount.enable
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ grep '^dataproc.localssd.mount.enable=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ property_value=
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ echo ''
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ property_value=
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ echo ''
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + MOUNT_DISKS_ENABLED=
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + [[ '' == \f\a\l\s\e ]]
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + DATAPROC_MOUNT_SERVICE_FILE=/usr/lib/systemd/system/google-dataproc-disk-mount.service
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + cat
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + chmod +x /usr/local/share/google/dataproc/bdutil/mount_disks.sh
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + chmod 644 /usr/lib/systemd/system/google-dataproc-disk-mount.service
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + systemctl enable google-dataproc-disk-mount
<13>Mar 21 02:43:07 google-dataproc-startup[948]: Created symlink /etc/systemd/system/multi-user.target.wants/google-dataproc-disk-mount.service → /lib/systemd/system/google-dataproc-disk-mount.service.
<13>Mar 21 02:43:07 google-dataproc-startup[948]: Created symlink /etc/systemd/system/hadoop-hdfs-namenode.service.wants/google-dataproc-disk-mount.service → /lib/systemd/system/google-dataproc-disk-mount.service.
<13>Mar 21 02:43:07 google-dataproc-startup[948]: Created symlink /etc/systemd/system/hadoop-hdfs-datanode.service.wants/google-dataproc-disk-mount.service → /lib/systemd/system/google-dataproc-disk-mount.service.
<13>Mar 21 02:43:07 google-dataproc-startup[948]: Created symlink /etc/systemd/system/hadoop-yarn-resourcemanager.service.wants/google-dataproc-disk-mount.service → /lib/systemd/system/google-dataproc-disk-mount.service.
<13>Mar 21 02:43:07 google-dataproc-startup[948]: Created symlink /etc/systemd/system/hadoop-yarn-nodemanager.service.wants/google-dataproc-disk-mount.service → /lib/systemd/system/google-dataproc-disk-mount.service.
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + systemctl start google-dataproc-disk-mount
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + bash configuration_script.sh
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/dataproc_env.sh
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ CLUSTER_NAME=student-01-71e17b39d43c-qwiklab
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ CLUSTER_UUID=1508fba4-d944-4b05-b441-abe36766e1e0
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ CONFIGBUCKET=student-01-71e17b39d43c-image-26164
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ HDFS_ROOT_URI=hdfs://student-01-71e17b39d43c-qwiklab-m
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ MASTER_HOSTNAME_0=student-01-71e17b39d43c-qwiklab-m
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ MASTER_HOSTNAMES=(student-01-71e17b39d43c-qwiklab-m)
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ NUM_MASTERS=1
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ NUM_WORKERS=2
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ PREFIX=student-01-71e17b39d43c-qwiklab
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ PROJECT=qwiklabs-gcp-01-d65f02c71c6a
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ ROLE=Master
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ ENABLE_HDFS=1
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ NUM_WORKERS=10
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ WORKERS=()
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + set +a
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ is_centos
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ . /etc/os-release
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ VERSION_ID=10
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ VERSION='10 (buster)'
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ VERSION_CODENAME=buster
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ ID=debian
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ echo debian
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ return 1
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + readonly METADATA_HOST=metadata.google.internal
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + METADATA_HOST=metadata.google.internal
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + readonly INSTANCE_METADATA_PATH=computeMetadata/v1/instance
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + INSTANCE_METADATA_PATH=computeMetadata/v1/instance
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + readonly IDENTITY_KEY=service-accounts/default/identity
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + IDENTITY_KEY=service-accounts/default/identity
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + readonly GUEST_ATTRIBUTE_PATH=computeMetadata/v1/instance/guest-attributes/dataproc-signed-keys
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + GUEST_ATTRIBUTE_PATH=computeMetadata/v1/instance/guest-attributes/dataproc-signed-keys
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + readonly SERIAL_DEVICE=/dev/ttyS3
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + SERIAL_DEVICE=/dev/ttyS3
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + readonly GENERATE_KEYS_SCRIPT=/usr/bin/google-dataproc-generate-cluster-keys.sh
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + GENERATE_KEYS_SCRIPT=/usr/bin/google-dataproc-generate-cluster-keys.sh
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.encryption.keygen.enabled
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ local -r property_name=dataproc.encryption.keygen.enabled
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.encryption.keygen.enabled
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ local -r property_name=dataproc.encryption.keygen.enabled
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ grep '^dataproc.encryption.keygen.enabled=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ property_value=
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ echo ''
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ property_value=
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ echo ''
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + cluster_keys_enabled=
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ get_dataproc_property_or_default dataproc.encryption.keygen.rotation_hours 6
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ local -r property_name=dataproc.encryption.keygen.rotation_hours
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ local -r default_value=6
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ local actual_value
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ get_dataproc_property dataproc.encryption.keygen.rotation_hours
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ local -r property_name=dataproc.encryption.keygen.rotation_hours
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.encryption.keygen.rotation_hours
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ local -r property_name=dataproc.encryption.keygen.rotation_hours
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ local property_value
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++++ grep '^dataproc.encryption.keygen.rotation_hours=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++++ cut -d = -f 2-
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++++ tail -n 1
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ property_value=
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ echo ''
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ property_value=
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ echo ''
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ actual_value=
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ [[ -n '' ]]
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ echo 6
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + cluster_keys_rotation_hours=6
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + cat
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + chmod u+rwx /usr/bin/google-dataproc-generate-cluster-keys.sh
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + cat
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + cat
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + [[ '' == \t\r\u\e ]]
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + rm /etc/udev/rules.d/80-ttyS3.rules
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + udevadm trigger
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + chown root:dialout /dev/ttyS3
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + set -e
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + loginfo 'Running configure_hadoop.sh'
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + echo 'Running configure_hadoop.sh'
<13>Mar 21 02:43:07 google-dataproc-startup[948]: Running configure_hadoop.sh
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + export HADOOP_TMP_DIR=/hadoop/tmp
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + HADOOP_TMP_DIR=/hadoop/tmp
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + mkdir -p /hadoop/tmp
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + export DEFAULT_NUM_MAPS=100
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + DEFAULT_NUM_MAPS=100
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + export DEFAULT_NUM_REDUCES=40
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + DEFAULT_NUM_REDUCES=40
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ grep -c processor /proc/cpuinfo
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + NUM_CORES=2
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + export NUM_CORES
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ python -c 'print int(2 // 1.0)'
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + MAP_SLOTS=2
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + export MAP_SLOTS
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ python -c 'print int(2 // 2.0)'
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + REDUCE_SLOTS=1
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + export REDUCE_SLOTS
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ awk '/^Mem:/{print $2}'
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ free -m
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + TOTAL_MEM=7454
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ python -c 'print int(7454 *     0.4)'
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + HADOOP_MR_MASTER_MEM_MB=2981
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + [[ -x configure_mrv2_mem.py ]]
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ mktemp /tmp/mrv2_XXX_tmp_env.sh
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + TEMP_ENV_FILE=/tmp/mrv2_fxy_tmp_env.sh
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + ./configure_mrv2_mem.py --output_file /tmp/mrv2_fxy_tmp_env.sh --total_memory 7454 --available_memory_ratio 0.8 --total_cores 2 --cores_per_map 1.0 --cores_per_reduce 2.0 --cores_per_app_master 2.0
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + source /tmp/mrv2_fxy_tmp_env.sh
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ export YARN_MIN_MEM_MB=512
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ YARN_MIN_MEM_MB=512
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ export YARN_MAX_MEM_MB=5632
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ YARN_MAX_MEM_MB=5632
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ export NODEMANAGER_MEM_MB=5632
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ NODEMANAGER_MEM_MB=5632
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ export APP_MASTER_MEM_MB=5632
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ APP_MASTER_MEM_MB=5632
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ export CORES_PER_APP_MASTER_ROUNDED=2
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ CORES_PER_APP_MASTER_ROUNDED=2
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ export APP_MASTER_JAVA_OPTS=-Xmx4505m
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ APP_MASTER_JAVA_OPTS=-Xmx4505m
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ export MAP_MEM_MB=2560
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ MAP_MEM_MB=2560
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ export CORES_PER_MAP_ROUNDED=1
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ CORES_PER_MAP_ROUNDED=1
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ export MAP_JAVA_OPTS=-Xmx2048m
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ MAP_JAVA_OPTS=-Xmx2048m
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ export REDUCE_MEM_MB=5632
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ REDUCE_MEM_MB=5632
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ export CORES_PER_REDUCE_ROUNDED=2
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ CORES_PER_REDUCE_ROUNDED=2
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ export REDUCE_JAVA_OPTS=-Xmx4505m
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ REDUCE_JAVA_OPTS=-Xmx4505m
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ python -c 'print int(7454 / 4)'
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + HADOOP_CLIENT_MEM_MB=1863
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + cat
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + is_version_at_least 2.0 1.4
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + local -r version1=2.0
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + local -r version2=1.4
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + loginfo 'Comparing if version 2.0 is at least version 1.4 '
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + echo 'Comparing if version 2.0 is at least version 1.4 '
<13>Mar 21 02:43:07 google-dataproc-startup[948]: Comparing if version 2.0 is at least version 1.4 
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + compare_versions 2.0 1.4
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + local -r version1=2.0
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + local -r version2=1.4
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + validate_version 2.0
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + local -r version=2.0
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + [[ 2.0 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + validate_version 1.4
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + local -r version=1.4
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + [[ 1.4 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + [[ 2.0 == \1\.\4 ]]
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + local IFS=.
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + ver1=($version1)
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + ver2=($version2)
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + local i ver1 ver2
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + (( i=2 ))
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + (( i<2 ))
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + (( i=0 ))
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + (( i<2 ))
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + [[ -z 1 ]]
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + (( 10#2 > 10#1 ))
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + return 1
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + case $? in
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + cat
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + cat
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + cat
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + cat
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + DATA_DIRS_ARRAY=($(get_data_dirs))
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ get_data_dirs
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ local -a mount_points
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ mapfile -t mount_points
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ find '/mnt/[0-9]*/' -maxdepth 0
<13>Mar 21 02:43:07 google-dataproc-startup[948]: find: ‘/mnt/[0-9]*/’: No such file or directory
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ true
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ (( 0 ))
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ echo /
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ return
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + MAPRED_DIRS=/hadoop/mapred
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + MAPRED_DIRS_ARRAY=(${MAPRED_DIRS})
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + MAPRED_LOCAL_DIRS_ARRAY=(${MAPRED_LOCAL_DIRS})
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + YARN_DIRS=/hadoop/yarn
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + YARN_DIRS_ARRAY=(${YARN_DIRS})
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ get_yarn_nm_local_dirs
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ data_dirs=($(get_data_dirs))
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ get_data_dirs
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ local -a mount_points
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ mapfile -t mount_points
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ find '/mnt/[0-9]*/' -maxdepth 0
<13>Mar 21 02:43:07 google-dataproc-startup[948]: find: ‘/mnt/[0-9]*/’: No such file or directory
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ true
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ (( 0 ))
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ echo /
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ return
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ local -r data_dirs
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ echo /hadoop/yarn/nm-local-dir
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + NODEMANAGER_LOCAL_DIRS_ARRAY=(${NODEMANAGER_LOCAL_DIRS})
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + mkdir -p /hadoop/mapred/local /hadoop/yarn/nm-local-dir
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + chgrp hadoop -L -R /hadoop /hadoop/tmp
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + chown -L -R mapred:hadoop /hadoop/mapred
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + chown -L -R yarn:hadoop /hadoop/yarn
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + chmod g+rwx -R /hadoop /hadoop/mapred /hadoop/yarn
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + chmod 777 -R /hadoop/tmp
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + export MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + export NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + YARN_ENV_FILE=/etc/hadoop/conf/yarn-env.sh
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + [[ -f /etc/hadoop/conf/yarn-env.sh ]]
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + cat
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ get_dataproc_property simplified.scaling.enable
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ local -r property_name=simplified.scaling.enable
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties simplified.scaling.enable
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ local -r property_name=simplified.scaling.enable
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ grep '^simplified.scaling.enable=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ property_value=true
<13>Mar 21 02:43:07 google-dataproc-startup[948]: +++ echo true
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ property_value=true
<13>Mar 21 02:43:07 google-dataproc-startup[948]: ++ echo true
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + SIMPLIFIED_SCALING_ENABLED=true
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + [[ true != \t\r\u\e ]]
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + [[ 1 -eq 1 ]]
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + touch /etc/hadoop/conf/nodes_include /etc/hadoop/conf/nodes_exclude
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + chown root:hadoop /etc/hadoop/conf/nodes_include /etc/hadoop/conf/nodes_exclude
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + [[ 1 -gt 1 ]]
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + readonly CORE_TEMPLATE=core-template.xml
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + CORE_TEMPLATE=core-template.xml
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + readonly YARN_TEMPLATE=yarn-template.xml
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + YARN_TEMPLATE=yarn-template.xml
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + merge_hadoop_configurations core-site.xml core-template.xml
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + local -r config=core-site.xml
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + local -r template=core-template.xml
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file core-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + merge_hadoop_configurations mapred-site.xml mapred-template.xml
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + local -r config=mapred-site.xml
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + local -r template=mapred-template.xml
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file mapred-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + merge_hadoop_configurations yarn-site.xml yarn-template.xml
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + local -r config=yarn-site.xml
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + local -r template=yarn-template.xml
<13>Mar 21 02:43:07 google-dataproc-startup[948]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/yarn-site.xml --source_configuration_file yarn-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + merge_hadoop_configurations capacity-scheduler.xml capacity-scheduler-template.xml
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r config=capacity-scheduler.xml
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r template=capacity-scheduler-template.xml
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/capacity-scheduler.xml --source_configuration_file capacity-scheduler-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + merge_hadoop_configurations distcp-default.xml distcp-template.xml
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r config=distcp-default.xml
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r template=distcp-template.xml
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/distcp-default.xml --source_configuration_file distcp-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + is_version_at_least 2.0 1.4
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r version1=2.0
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r version2=1.4
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + loginfo 'Comparing if version 2.0 is at least version 1.4 '
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + echo 'Comparing if version 2.0 is at least version 1.4 '
<13>Mar 21 02:43:08 google-dataproc-startup[948]: Comparing if version 2.0 is at least version 1.4 
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + compare_versions 2.0 1.4
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r version1=2.0
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r version2=1.4
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + validate_version 2.0
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r version=2.0
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + [[ 2.0 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + validate_version 1.4
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r version=1.4
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + [[ 1.4 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + [[ 2.0 == \1\.\4 ]]
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local IFS=.
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + ver1=($version1)
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + ver2=($version2)
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local i ver1 ver2
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + (( i=2 ))
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + (( i<2 ))
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + (( i=0 ))
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + (( i<2 ))
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + [[ -z 1 ]]
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + (( 10#2 > 10#1 ))
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + return 1
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + case $? in
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + set_hadoop_property mapred-site.xml mapreduce.application.classpath '$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,
<13>Mar 21 02:43:08 google-dataproc-startup[948]:       $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*,
<13>Mar 21 02:43:08 google-dataproc-startup[948]:       /usr/local/share/google/dataproc/lib/*'
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r config=mapred-site.xml
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r property_name=mapreduce.application.classpath
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r 'property_value=$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,
<13>Mar 21 02:43:08 google-dataproc-startup[948]:       $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*,
<13>Mar 21 02:43:08 google-dataproc-startup[948]:       /usr/local/share/google/dataproc/lib/*'
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/hadoop/conf/mapred-site.xml --name mapreduce.application.classpath --value '$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,
<13>Mar 21 02:43:08 google-dataproc-startup[948]:       $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*,
<13>Mar 21 02:43:08 google-dataproc-startup[948]:       /usr/local/share/google/dataproc/lib/*' --clobber
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + set_hadoop_property yarn-site.xml yarn.application.classpath '$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,
<13>Mar 21 02:43:08 google-dataproc-startup[948]:       $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,$HADOOP_MAPRED_HOME/*,
<13>Mar 21 02:43:08 google-dataproc-startup[948]:       $HADOOP_MAPRED_HOME/lib/*,$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*,
<13>Mar 21 02:43:08 google-dataproc-startup[948]:       /usr/local/share/google/dataproc/lib/*'
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r config=yarn-site.xml
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r property_name=yarn.application.classpath
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r 'property_value=$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,
<13>Mar 21 02:43:08 google-dataproc-startup[948]:       $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,$HADOOP_MAPRED_HOME/*,
<13>Mar 21 02:43:08 google-dataproc-startup[948]:       $HADOOP_MAPRED_HOME/lib/*,$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*,
<13>Mar 21 02:43:08 google-dataproc-startup[948]:       /usr/local/share/google/dataproc/lib/*'
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.application.classpath --value '$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,
<13>Mar 21 02:43:08 google-dataproc-startup[948]:       $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,$HADOOP_MAPRED_HOME/*,
<13>Mar 21 02:43:08 google-dataproc-startup[948]:       $HADOOP_MAPRED_HOME/lib/*,$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*,
<13>Mar 21 02:43:08 google-dataproc-startup[948]:       /usr/local/share/google/dataproc/lib/*' --clobber
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + is_version_at_least 2.0 2.0
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r version1=2.0
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r version2=2.0
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + loginfo 'Comparing if version 2.0 is at least version 2.0 '
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + echo 'Comparing if version 2.0 is at least version 2.0 '
<13>Mar 21 02:43:08 google-dataproc-startup[948]: Comparing if version 2.0 is at least version 2.0 
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + compare_versions 2.0 2.0
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r version1=2.0
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r version2=2.0
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + validate_version 2.0
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r version=2.0
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + [[ 2.0 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + validate_version 2.0
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r version=2.0
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + [[ 2.0 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + [[ 2.0 == \2\.\0 ]]
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + case $? in
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + cat
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + set_hadoop_property yarn-site.xml yarn.nodemanager.env-whitelist JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME,LD_LIBRARY_PATH
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r config=yarn-site.xml
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r property_name=yarn.nodemanager.env-whitelist
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r property_value=JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME,LD_LIBRARY_PATH
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.nodemanager.env-whitelist --value JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME,LD_LIBRARY_PATH --clobber
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + set_hadoop_property yarn-site.xml yarn.nm.liveness-monitor.expiry-interval-ms 15000
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r config=yarn-site.xml
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r property_name=yarn.nm.liveness-monitor.expiry-interval-ms
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r property_value=15000
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.nm.liveness-monitor.expiry-interval-ms --value 15000 --clobber
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + set_hadoop_property yarn-site.xml yarn.log-aggregation-enable false
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r config=yarn-site.xml
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r property_name=yarn.log-aggregation-enable
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r property_value=false
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.log-aggregation-enable --value false --clobber
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + ZK_QUORUM=student-01-71e17b39d43c-qwiklab-m:2181,:2181,:2181
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + [[ 1 -gt 1 ]]
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + set_hadoop_property hdfs-site.xml dfs.namenode.file.close.num-committed-allowed 1
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r config=hdfs-site.xml
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r property_name=dfs.namenode.file.close.num-committed-allowed
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r property_value=1
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.file.close.num-committed-allowed --value 1 --clobber
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + set_hadoop_property core-site.xml hadoop.http.filter.initializers org.apache.hadoop.security.HttpCrossOriginFilterInitializer,org.apache.hadoop.http.lib.StaticUserWebFilter
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r config=core-site.xml
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r property_name=hadoop.http.filter.initializers
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r property_value=org.apache.hadoop.security.HttpCrossOriginFilterInitializer,org.apache.hadoop.http.lib.StaticUserWebFilter
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/hadoop/conf/core-site.xml --name hadoop.http.filter.initializers --value org.apache.hadoop.security.HttpCrossOriginFilterInitializer,org.apache.hadoop.http.lib.StaticUserWebFilter --clobber
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + set_hadoop_property yarn-site.xml yarn.resourcemanager.webapp.cross-origin.enabled true
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r config=yarn-site.xml
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r property_name=yarn.resourcemanager.webapp.cross-origin.enabled
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r property_value=true
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.resourcemanager.webapp.cross-origin.enabled --value true --clobber
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + set_hadoop_property yarn-site.xml yarn.timeline-service.http-cross-origin.enabled true
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r config=yarn-site.xml
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r property_name=yarn.timeline-service.http-cross-origin.enabled
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r property_value=true
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.http-cross-origin.enabled --value true --clobber
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + set_hadoop_property yarn-site.xml yarn.timeline-service.enabled true
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r config=yarn-site.xml
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r property_name=yarn.timeline-service.enabled
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r property_value=true
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.enabled --value true --clobber
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + set_hadoop_property yarn-site.xml yarn.timeline-service.hostname student-01-71e17b39d43c-qwiklab-m
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r config=yarn-site.xml
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r property_name=yarn.timeline-service.hostname
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r property_value=student-01-71e17b39d43c-qwiklab-m
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.hostname --value student-01-71e17b39d43c-qwiklab-m --clobber
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + set_hadoop_property yarn-site.xml yarn.timeline-service.bind-host 0.0.0.0
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r config=yarn-site.xml
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r property_name=yarn.timeline-service.bind-host
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r property_value=0.0.0.0
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.bind-host --value 0.0.0.0 --clobber
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + set_hadoop_property yarn-site.xml yarn.resourcemanager.system-metrics-publisher.enabled true
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r config=yarn-site.xml
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r property_name=yarn.resourcemanager.system-metrics-publisher.enabled
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + local -r property_value=true
<13>Mar 21 02:43:08 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.resourcemanager.system-metrics-publisher.enabled --value true --clobber
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + set_hadoop_property yarn-site.xml yarn.timeline-service.generic-application-history.enabled true
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + local -r config=yarn-site.xml
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + local -r property_name=yarn.timeline-service.generic-application-history.enabled
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + local -r property_value=true
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.generic-application-history.enabled --value true --clobber
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ get_dataproc_property am.primary_only
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ local -r property_name=am.primary_only
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ local -r property_name=am.primary_only
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ property_value=false
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ echo false
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ property_value=false
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ echo false
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ get_metadata_datanode_enabled
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ get_dataproc_metadata DATAPROC_METADATA_DATANODE_ENABLED attributes/dataproc-datanode-enabled
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ local -r var_name=DATAPROC_METADATA_DATANODE_ENABLED
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ local -r attr_name=attributes/dataproc-datanode-enabled
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ [[ -v DATAPROC_METADATA_DATANODE_ENABLED ]]
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ /usr/share/google/get_metadata_value attributes/dataproc-datanode-enabled
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + DATAPROC_DATANODE_ENABLED=false
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + [[ false == \t\r\u\e ]]
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + [[ true == \t\r\u\e ]]
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + [[ 1 -gt 1 ]]
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + readonly YARN_SIMPLIFICATION_MIXINS=yarn-simplification-mixins.xml
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + YARN_SIMPLIFICATION_MIXINS=yarn-simplification-mixins.xml
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + merge_hadoop_configurations yarn-site.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-mixins.xml
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + local -r config=yarn-site.xml
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + local -r template=/usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-mixins.xml
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/yarn-site.xml --source_configuration_file /usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-mixins.xml --resolve_environment_variables --create_if_absent --clobber
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + is_component_selected kerberos
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + local -r component=kerberos
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + local activated_components
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ get_components_to_activate
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.components.activate
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ tr '[:upper:]' '[:lower:]'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ local -r property_name=dataproc.components.activate
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ local -r property_name=dataproc.components.activate
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ property_value='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ echo 'hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ property_value='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ echo 'hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + activated_components='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + [[ hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3 == *kerberos* ]]
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ get_dataproc_property docker.yarn.enable
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ local -r property_name=docker.yarn.enable
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties docker.yarn.enable
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ local -r property_name=docker.yarn.enable
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++++ grep '^docker.yarn.enable=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ property_value=
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ echo ''
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ property_value=
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ echo ''
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + enable_docker_yarn=
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + is_version_at_least 2.0 2.0
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + local -r version1=2.0
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + local -r version2=2.0
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + loginfo 'Comparing if version 2.0 is at least version 2.0 '
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + echo 'Comparing if version 2.0 is at least version 2.0 '
<13>Mar 21 02:43:09 google-dataproc-startup[948]: Comparing if version 2.0 is at least version 2.0 
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + compare_versions 2.0 2.0
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + local -r version1=2.0
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + local -r version2=2.0
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + validate_version 2.0
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + local -r version=2.0
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + [[ 2.0 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + validate_version 2.0
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + local -r version=2.0
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + [[ 2.0 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + [[ 2.0 == \2\.\0 ]]
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + case $? in
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + is_component_selected docker-ce
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + local -r component=docker-ce
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + local activated_components
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ get_components_to_activate
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ tr '[:upper:]' '[:lower:]'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.components.activate
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ local -r property_name=dataproc.components.activate
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ local -r property_name=dataproc.components.activate
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ property_value='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ echo 'hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ property_value='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ echo 'hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + activated_components='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + [[ hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3 == *docker-ce* ]]
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + set -e
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + loginfo 'Running configure_connectors.sh'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + echo 'Running configure_connectors.sh'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: Running configure_connectors.sh
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + stripped_zone=us-central1-f
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + readonly stripped_zone
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + bdconfig set_property --configuration_file gcs-core-template.xml --name fs.gs.application.name.suffix --value '-dataproc zone/us-central1-f' --clobber
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file gcs-core-template.xml --resolve_environment_variables --create_if_absent --noclobber
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file bq-mapred-template.xml --resolve_environment_variables --create_if_absent --noclobber
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + set -e
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + loginfo 'Running configure_tez.sh'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + echo 'Running configure_tez.sh'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: Running configure_tez.sh
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + readonly CONFIG_CHECKSUM=23fbfca8f7b8e142395c6bb4676427ae
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + CONFIG_CHECKSUM=23fbfca8f7b8e142395c6bb4676427ae
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ ls /usr/lib/tez/tez-ui-0.9.2.war
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + TEZ_UI_WAR=/usr/lib/tez/tez-ui-0.9.2.war
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + configure_war /usr/lib/tez/tez-ui-0.9.2.war
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + local -r tez_war=/usr/lib/tez/tez-ui-0.9.2.war
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + local tmp_dir
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ mktemp -d
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + tmp_dir=/tmp/tmp.76fBfZaaWh
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + unzip -q /usr/lib/tez/tez-ui-0.9.2.war -d /tmp/tmp.76fBfZaaWh
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + local -r tez_configs=/tmp/tmp.76fBfZaaWh/config/configs.env
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ md5sum /tmp/tmp.76fBfZaaWh/config/configs.env
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ cut -d ' ' -f 1
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + [[ 23fbfca8f7b8e142395c6bb4676427ae != \2\3\f\b\f\c\a\8\f\7\b\8\e\1\4\2\3\9\5\c\6\b\b\4\6\7\6\4\2\7\a\e ]]
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + [[ -f /tmp/tmp.76fBfZaaWh/config/configs.env ]]
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + sed -i 's#\(.*\)//timeline: "http://localhost:8188"\(.*\)#\1timeline: "http://student-01-71e17b39d43c-qwiklab-m:8188"\2#' /tmp/tmp.76fBfZaaWh/config/configs.env
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + sed -i 's#\(.*\)//rm: "http://localhost:8088"\(.*\)#\1rm: "http://student-01-71e17b39d43c-qwiklab-m:8088"\2#' /tmp/tmp.76fBfZaaWh/config/configs.env
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + local optional_components_value
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.components.activate
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ local -r property_name=dataproc.components.activate
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ local -r property_name=dataproc.components.activate
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ property_value='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: +++ echo 'hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ property_value='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ echo 'hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + optional_components_value='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + [[ hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3 == *\k\n\o\x* ]]
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + cd /tmp/tmp.76fBfZaaWh
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + zip -q /usr/lib/tez/tez-ui-0.9.2.war -r ./META-INF ./WEB-INF ./assets ./config ./fonts ./index.html
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + cd ..
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + rm -rf /tmp/tmp.76fBfZaaWh
<13>Mar 21 02:43:09 google-dataproc-startup[948]: ++ stat /usr/lib/tez/tez-common-0.9.2.jar --format=%Y
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + touch -d @1615457574 /usr/lib/tez/tez-ui-0.9.2.war
<13>Mar 21 02:43:09 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-names --value tez --clobber
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-on-disk-path.tez --value /usr/lib/tez/tez-ui-0.9.2.war --clobber
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-web-path.tez --value /tez-ui --clobber
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.history.logging.service.class --value org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService --clobber
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.tez-ui.history-url.base --value http://student-01-71e17b39d43c-qwiklab-m:8188/tez-ui/ --clobber
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.am.node-blacklisting.enabled --value false --clobber
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + set -e
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + readonly ZOOKEEPER_CONFIG=/etc/zookeeper/conf/zoo.cfg
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + ZOOKEEPER_CONFIG=/etc/zookeeper/conf/zoo.cfg
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + readonly ZOOKEEPER_DATA_DIR=/var/lib/zookeeper/
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + ZOOKEEPER_DATA_DIR=/var/lib/zookeeper/
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + (( i = 0 ))
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + (( i < 1 ))
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + echo server.0=student-01-71e17b39d43c-qwiklab-m:2888:3888
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + (( i++ ))
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + (( i < 1 ))
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + echo autopurge.purgeInterval=168
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + [[ Master == \M\a\s\t\e\r ]]
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ uname -n
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ sed -e 's/.*-m-//'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + MY_ID=student-01-71e17b39d43c-qwiklab-m
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + echo student-01-71e17b39d43c-qwiklab-m
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + set -euxo pipefail
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/components/shared/docker.sh
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ readonly DOCKER_PATH=/var/lib/docker
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ DOCKER_PATH=/var/lib/docker
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ readonly GCR_CREDENTIAL_HELPER_VERSION=2.0.0
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ GCR_CREDENTIAL_HELPER_VERSION=2.0.0
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + is_version_at_least 2.0 1.5
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r version1=2.0
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r version2=1.5
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + loginfo 'Comparing if version 2.0 is at least version 1.5 '
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + echo 'Comparing if version 2.0 is at least version 1.5 '
<13>Mar 21 02:43:10 google-dataproc-startup[948]: Comparing if version 2.0 is at least version 1.5 
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + compare_versions 2.0 1.5
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r version1=2.0
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r version2=1.5
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + validate_version 2.0
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r version=2.0
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + [[ 2.0 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + validate_version 1.5
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r version=1.5
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + [[ 1.5 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + [[ 2.0 == \1\.\5 ]]
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local IFS=.
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + ver1=($version1)
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + ver2=($version2)
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local i ver1 ver2
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + (( i=2 ))
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + (( i<2 ))
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + (( i=0 ))
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + (( i<2 ))
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + [[ -z 1 ]]
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + (( 10#2 > 10#1 ))
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + return 1
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + case $? in
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + is_component_selected docker-ce
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r component=docker-ce
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local activated_components
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ get_components_to_activate
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ tr '[:upper:]' '[:lower:]'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.components.activate
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ local -r property_name=dataproc.components.activate
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ local -r property_name=dataproc.components.activate
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ property_value='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ echo 'hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ property_value='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ echo 'hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + activated_components='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + [[ hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3 == *docker-ce* ]]
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + readonly RUN_SCRIPT_PATH=/usr/local/share/google/dataproc/metadata-proxy.sh
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + RUN_SCRIPT_PATH=/usr/local/share/google/dataproc/metadata-proxy.sh
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + readonly POLLING_SCRIPT_PATH=/usr/local/share/google/dataproc/poll-metadata.sh
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + POLLING_SCRIPT_PATH=/usr/local/share/google/dataproc/poll-metadata.sh
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + readonly PROXY_SERVICE_CONF=/usr/lib/systemd/system/metadata-proxy.service
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + PROXY_SERVICE_CONF=/usr/lib/systemd/system/metadata-proxy.service
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + readonly POLLING_SERVICE_CONF=/usr/lib/systemd/system/metadata-credentials-polling.service
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + POLLING_SERVICE_CONF=/usr/lib/systemd/system/metadata-credentials-polling.service
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + readonly METADATA_ADDRESS=169.254.169.254
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + METADATA_ADDRESS=169.254.169.254
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + readonly METADATA_PORT=80
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + METADATA_PORT=80
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + readonly METADATA_PASSTHROUGH_PORT=987
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + METADATA_PASSTHROUGH_PORT=987
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + readonly METADATA_PROXY_PORT=988
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + METADATA_PROXY_PORT=988
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + readonly 'TOKEN_SOURCE_METADATA=TOKEN FROM METADATA ATTRIBUTES'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + TOKEN_SOURCE_METADATA='TOKEN FROM METADATA ATTRIBUTES'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + readonly 'TOKEN_SOURCE_GCS=TOKEN FROM GCS'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + TOKEN_SOURCE_GCS='TOKEN FROM GCS'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ /usr/share/google/get_metadata_value attributes/dataproc-bucket
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + readonly CONFIGBUCKET=student-01-71e17b39d43c-image-26164
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + CONFIGBUCKET=student-01-71e17b39d43c-image-26164
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ /usr/share/google/get_metadata_value attributes/dataproc-cluster-uuid
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + readonly CLUSTER_UUID=1508fba4-d944-4b05-b441-abe36766e1e0
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + CLUSTER_UUID=1508fba4-d944-4b05-b441-abe36766e1e0
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ /usr/share/google/get_metadata_value id
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + readonly INSTANCE_UUID=7693933198110821492
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + INSTANCE_UUID=7693933198110821492
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + readonly TOKEN_METADATA_ATTRIBUTE=attributes/dataproc-injected-credentials
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + TOKEN_METADATA_ATTRIBUTE=attributes/dataproc-injected-credentials
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + readonly IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.metadata.proxy.enabled
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ local -r property_name=dataproc.metadata.proxy.enabled
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.metadata.proxy.enabled
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ local -r property_name=dataproc.metadata.proxy.enabled
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ grep '^dataproc.metadata.proxy.enabled=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ property_value=
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ echo ''
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ property_value=
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ echo ''
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + metadata_proxy_enabled=
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.exclusive.user
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ local -r property_name=dataproc.exclusive.user
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.exclusive.user
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ local -r property_name=dataproc.exclusive.user
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ grep '^dataproc.exclusive.user=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ property_value=
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ echo ''
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ property_value=
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ echo ''
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + dataproc_exclusive_user=
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ get_token_source
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ local initial_credentials
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ /usr/share/google/get_metadata_value attributes/dataproc-injected-credentials
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ initial_credentials=
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ readonly initial_credentials
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ [[ -n '' ]]
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ echo 'TOKEN FROM GCS'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + readonly 'METADATA_PROXY_TOKEN_SOURCE=TOKEN FROM GCS'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + METADATA_PROXY_TOKEN_SOURCE='TOKEN FROM GCS'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ get_token_path
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ [[ TOKEN FROM GCS == \T\O\K\E\N\ \F\R\O\M\ \M\E\T\A\D\A\T\A\ \A\T\T\R\I\B\U\T\E\S ]]
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ echo gs://student-01-71e17b39d43c-image-26164/google-cloud-dataproc-metainfo/1508fba4-d944-4b05-b441-abe36766e1e0/7693933198110821492/access-token
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + readonly TOKEN_PATH=gs://student-01-71e17b39d43c-image-26164/google-cloud-dataproc-metainfo/1508fba4-d944-4b05-b441-abe36766e1e0/7693933198110821492/access-token
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + TOKEN_PATH=gs://student-01-71e17b39d43c-image-26164/google-cloud-dataproc-metainfo/1508fba4-d944-4b05-b441-abe36766e1e0/7693933198110821492/access-token
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + [[ '' == \t\r\u\e ]]
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + loginfo 'Populating initial cluster member list'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + echo 'Populating initial cluster member list'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: Populating initial cluster member list
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.worker.custom.init.actions.mode
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ local -r property_name=dataproc.worker.custom.init.actions.mode
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.worker.custom.init.actions.mode
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ local -r property_name=dataproc.worker.custom.init.actions.mode
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ grep '^dataproc.worker.custom.init.actions.mode=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ property_value=RUN_BEFORE_SERVICES
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ echo RUN_BEFORE_SERVICES
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ property_value=RUN_BEFORE_SERVICES
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ echo RUN_BEFORE_SERVICES
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + WORKER_CUSTOM_INIT_ACTIONS_MODE=RUN_BEFORE_SERVICES
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + WORKER_COUNT=2
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ get_dataproc_property simplified.scaling.enable
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ local -r property_name=simplified.scaling.enable
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties simplified.scaling.enable
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ local -r property_name=simplified.scaling.enable
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ grep '^simplified.scaling.enable=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ property_value=true
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ echo true
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ property_value=true
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ echo true
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + SIMPLIFIED_SCALING_ENABLED=true
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + [[ true != \t\r\u\e ]]
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ get_dataproc_property_or_default yarn.log-aggregation.enabled false
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ local -r property_name=yarn.log-aggregation.enabled
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ local -r default_value=false
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ local actual_value
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ get_dataproc_property yarn.log-aggregation.enabled
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ local -r property_name=yarn.log-aggregation.enabled
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ get_java_property /etc/google-dataproc/dataproc.properties yarn.log-aggregation.enabled
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ local -r property_name=yarn.log-aggregation.enabled
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ local property_value
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++++ grep '^yarn.log-aggregation.enabled=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++++ tail -n 1
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++++ cut -d = -f 2-
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ property_value=true
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ echo true
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ property_value=true
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ echo true
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ actual_value=true
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ [[ -n true ]]
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ echo true
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + LOG_AGGREGATION_ENABLED=true
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + readonly LOG_AGGREGATION_ENABLED
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + [[ true == \t\r\u\e ]]
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + loginfo 'Enabling YARN log aggregation.'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + echo 'Enabling YARN log aggregation.'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: Enabling YARN log aggregation.
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + set_property_yarn_site yarn.log-aggregation-enable true
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r name=yarn.log-aggregation-enable
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r value=true
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + set_property_in_xml /etc/hadoop/conf/yarn-site.xml yarn.log-aggregation-enable true
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r xml_file=/etc/hadoop/conf/yarn-site.xml
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r name=yarn.log-aggregation-enable
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r value=true
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.log-aggregation-enable --value true --create_if_absent --clobber
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + [[ false == \t\r\u\e ]]
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + readonly YARN_LOG_SERVER_SCHEMA=http
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + YARN_LOG_SERVER_SCHEMA=http
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + readonly YARN_LOG_SERVER_PORT=19888
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + YARN_LOG_SERVER_PORT=19888
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + readonly YARN_LOG_SERVER_URL=http://student-01-71e17b39d43c-qwiklab-m:19888/jobhistory/logs
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + YARN_LOG_SERVER_URL=http://student-01-71e17b39d43c-qwiklab-m:19888/jobhistory/logs
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + set_property_yarn_site yarn.log.server.url http://student-01-71e17b39d43c-qwiklab-m:19888/jobhistory/logs
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r name=yarn.log.server.url
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r value=http://student-01-71e17b39d43c-qwiklab-m:19888/jobhistory/logs
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + set_property_in_xml /etc/hadoop/conf/yarn-site.xml yarn.log.server.url http://student-01-71e17b39d43c-qwiklab-m:19888/jobhistory/logs
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r xml_file=/etc/hadoop/conf/yarn-site.xml
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r name=yarn.log.server.url
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r value=http://student-01-71e17b39d43c-qwiklab-m:19888/jobhistory/logs
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.log.server.url --value http://student-01-71e17b39d43c-qwiklab-m:19888/jobhistory/logs --create_if_absent --clobber
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + [[ -n dataproc-temp-us-central1-147286270203-nrwqwbuk ]]
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + set_property_yarn_site yarn.nodemanager.remote-app-log-dir gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/yarn-logs
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r name=yarn.nodemanager.remote-app-log-dir
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r value=gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/yarn-logs
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + set_property_in_xml /etc/hadoop/conf/yarn-site.xml yarn.nodemanager.remote-app-log-dir gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/yarn-logs
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r xml_file=/etc/hadoop/conf/yarn-site.xml
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r name=yarn.nodemanager.remote-app-log-dir
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r value=gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/yarn-logs
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.nodemanager.remote-app-log-dir --value gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/yarn-logs --create_if_absent --clobber
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + is_version_at_least 2.0 2.0
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r version1=2.0
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r version2=2.0
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + loginfo 'Comparing if version 2.0 is at least version 2.0 '
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + echo 'Comparing if version 2.0 is at least version 2.0 '
<13>Mar 21 02:43:10 google-dataproc-startup[948]: Comparing if version 2.0 is at least version 2.0 
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + compare_versions 2.0 2.0
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r version1=2.0
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r version2=2.0
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + validate_version 2.0
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r version=2.0
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + [[ 2.0 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + validate_version 2.0
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r version=2.0
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + [[ 2.0 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + [[ 2.0 == \2\.\0 ]]
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + case $? in
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + set_property_yarn_site yarn.log-aggregation.file-formats IFile,TFile
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r name=yarn.log-aggregation.file-formats
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r value=IFile,TFile
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + set_property_in_xml /etc/hadoop/conf/yarn-site.xml yarn.log-aggregation.file-formats IFile,TFile
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r xml_file=/etc/hadoop/conf/yarn-site.xml
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r name=yarn.log-aggregation.file-formats
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r value=IFile,TFile
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.log-aggregation.file-formats --value IFile,TFile --create_if_absent --clobber
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + set_property_yarn_site yarn.log-aggregation.file-controller.IFile.class org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r name=yarn.log-aggregation.file-controller.IFile.class
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r value=org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + set_property_in_xml /etc/hadoop/conf/yarn-site.xml yarn.log-aggregation.file-controller.IFile.class org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r xml_file=/etc/hadoop/conf/yarn-site.xml
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r name=yarn.log-aggregation.file-controller.IFile.class
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r value=org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.log-aggregation.file-controller.IFile.class --value org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController --create_if_absent --clobber
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ get_dataproc_property_or_default job.history.to-gcs.enabled false
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ local -r property_name=job.history.to-gcs.enabled
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ local -r default_value=false
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ local actual_value
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ get_dataproc_property job.history.to-gcs.enabled
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ local -r property_name=job.history.to-gcs.enabled
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ get_java_property /etc/google-dataproc/dataproc.properties job.history.to-gcs.enabled
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ local -r property_name=job.history.to-gcs.enabled
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ local property_value
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++++ grep '^job.history.to-gcs.enabled=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++++ cut -d = -f 2-
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++++ tail -n 1
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ property_value=true
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++++ echo true
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ property_value=true
<13>Mar 21 02:43:10 google-dataproc-startup[948]: +++ echo true
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ actual_value=true
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ [[ -n true ]]
<13>Mar 21 02:43:10 google-dataproc-startup[948]: ++ echo true
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + persist_history_to_gcs=true
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + [[ true == \t\r\u\e ]]
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + loginfo 'Enabling persisting MapReduce job history files to GCS.'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + echo 'Enabling persisting MapReduce job history files to GCS.'
<13>Mar 21 02:43:10 google-dataproc-startup[948]: Enabling persisting MapReduce job history files to GCS.
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + [[ -n dataproc-temp-us-central1-147286270203-nrwqwbuk ]]
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + set_property_mapred_site mapreduce.jobhistory.done-dir gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/mapreduce-job-history/done
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r name=mapreduce.jobhistory.done-dir
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r value=gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/mapreduce-job-history/done
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + set_property_in_xml /etc/hadoop/conf/mapred-site.xml mapreduce.jobhistory.done-dir gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/mapreduce-job-history/done
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r xml_file=/etc/hadoop/conf/mapred-site.xml
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r name=mapreduce.jobhistory.done-dir
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r value=gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/mapreduce-job-history/done
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/hadoop/conf/mapred-site.xml --name mapreduce.jobhistory.done-dir --value gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/mapreduce-job-history/done --create_if_absent --clobber
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + set_property_mapred_site mapreduce.jobhistory.intermediate-done-dir gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/mapreduce-job-history/done_intermediate
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r name=mapreduce.jobhistory.intermediate-done-dir
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r value=gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/mapreduce-job-history/done_intermediate
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + set_property_in_xml /etc/hadoop/conf/mapred-site.xml mapreduce.jobhistory.intermediate-done-dir gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/mapreduce-job-history/done_intermediate
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r xml_file=/etc/hadoop/conf/mapred-site.xml
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r name=mapreduce.jobhistory.intermediate-done-dir
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + local -r value=gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/mapreduce-job-history/done_intermediate
<13>Mar 21 02:43:10 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/hadoop/conf/mapred-site.xml --name mapreduce.jobhistory.intermediate-done-dir --value gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/mapreduce-job-history/done_intermediate --create_if_absent --clobber
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + set_property_mapred_site mapreduce.jobhistory.move.interval-ms 1000
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r name=mapreduce.jobhistory.move.interval-ms
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r value=1000
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + set_property_in_xml /etc/hadoop/conf/mapred-site.xml mapreduce.jobhistory.move.interval-ms 1000
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r xml_file=/etc/hadoop/conf/mapred-site.xml
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r name=mapreduce.jobhistory.move.interval-ms
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r value=1000
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + bdconfig set_property --configuration_file /etc/hadoop/conf/mapred-site.xml --name mapreduce.jobhistory.move.interval-ms --value 1000 --create_if_absent --clobber
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.users
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ local -r property_name=dataproc.users
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.users
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ local -r property_name=dataproc.users
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++++ grep '^dataproc.users=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ property_value=
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ echo ''
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ property_value=
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ echo ''
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + DATAPROC_USERS=
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ -n '' ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ false == \t\r\u\e ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + loginfo 'Merging user-specified cluster properties'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'Merging user-specified cluster properties'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: Merging user-specified cluster properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + merge_xml_properties /tmp/cluster/properties/capacity-scheduler.xml /etc/hadoop/conf/capacity-scheduler.xml
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local src=/tmp/cluster/properties/capacity-scheduler.xml
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local dest=/etc/hadoop/conf/capacity-scheduler.xml
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ ! -f /tmp/cluster/properties/capacity-scheduler.xml ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/capacity-scheduler.xml --source_configuration_file /tmp/cluster/properties/capacity-scheduler.xml --resolve_environment_variables --create_if_absent --clobber
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + loginfo 'Merged /tmp/cluster/properties/capacity-scheduler.xml.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'Merged /tmp/cluster/properties/capacity-scheduler.xml.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: Merged /tmp/cluster/properties/capacity-scheduler.xml.
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + merge_xml_properties /tmp/cluster/properties/core.xml /etc/hadoop/conf/core-site.xml
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local src=/tmp/cluster/properties/core.xml
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local dest=/etc/hadoop/conf/core-site.xml
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ ! -f /tmp/cluster/properties/core.xml ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file /tmp/cluster/properties/core.xml --resolve_environment_variables --create_if_absent --clobber
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + loginfo 'Merged /tmp/cluster/properties/core.xml.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'Merged /tmp/cluster/properties/core.xml.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: Merged /tmp/cluster/properties/core.xml.
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + merge_xml_properties /tmp/cluster/properties/distcp.xml /etc/hadoop/conf/distcp-default.xml
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local src=/tmp/cluster/properties/distcp.xml
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local dest=/etc/hadoop/conf/distcp-default.xml
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ ! -f /tmp/cluster/properties/distcp.xml ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/distcp-default.xml --source_configuration_file /tmp/cluster/properties/distcp.xml --resolve_environment_variables --create_if_absent --clobber
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + loginfo 'Merged /tmp/cluster/properties/distcp.xml.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'Merged /tmp/cluster/properties/distcp.xml.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: Merged /tmp/cluster/properties/distcp.xml.
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + merge_xml_properties /tmp/cluster/properties/mapred.xml /etc/hadoop/conf/mapred-site.xml
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local src=/tmp/cluster/properties/mapred.xml
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local dest=/etc/hadoop/conf/mapred-site.xml
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ ! -f /tmp/cluster/properties/mapred.xml ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file /tmp/cluster/properties/mapred.xml --resolve_environment_variables --create_if_absent --clobber
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + loginfo 'Merged /tmp/cluster/properties/mapred.xml.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'Merged /tmp/cluster/properties/mapred.xml.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: Merged /tmp/cluster/properties/mapred.xml.
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + merge_xml_properties /tmp/cluster/properties/yarn.xml /etc/hadoop/conf/yarn-site.xml
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local src=/tmp/cluster/properties/yarn.xml
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local dest=/etc/hadoop/conf/yarn-site.xml
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ ! -f /tmp/cluster/properties/yarn.xml ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/yarn-site.xml --source_configuration_file /tmp/cluster/properties/yarn.xml --resolve_environment_variables --create_if_absent --clobber
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + loginfo 'Merged /tmp/cluster/properties/yarn.xml.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'Merged /tmp/cluster/properties/yarn.xml.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: Merged /tmp/cluster/properties/yarn.xml.
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + merge_java_properties /tmp/cluster/properties/pig.properties /etc/pig/conf/pig.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r src=/tmp/cluster/properties/pig.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r dest=/etc/pig/conf/pig.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r 'header=\n# User-supplied properties.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ ! -f /tmp/cluster/properties/pig.properties ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo -e '\n# User-supplied properties.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + cat /tmp/cluster/properties/pig.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + loginfo 'Merged /tmp/cluster/properties/pig.properties.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'Merged /tmp/cluster/properties/pig.properties.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: Merged /tmp/cluster/properties/pig.properties.
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + merge_java_properties /tmp/cluster/properties/zookeeper.properties /etc/zookeeper/conf/zoo.cfg
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r src=/tmp/cluster/properties/zookeeper.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r dest=/etc/zookeeper/conf/zoo.cfg
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r 'header=\n# User-supplied properties.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ ! -f /tmp/cluster/properties/zookeeper.properties ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo -e '\n# User-supplied properties.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + cat /tmp/cluster/properties/zookeeper.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + loginfo 'Merged /tmp/cluster/properties/zookeeper.properties.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'Merged /tmp/cluster/properties/zookeeper.properties.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: Merged /tmp/cluster/properties/zookeeper.properties.
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + merge_sh_env_vars /tmp/cluster/properties/hadoop-env.sh /etc/hadoop/conf/hadoop-env.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local src=/tmp/cluster/properties/hadoop-env.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local dest=/etc/hadoop/conf/hadoop-env.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ ! -f /tmp/cluster/properties/hadoop-env.sh ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo -e '\n# User-supplied properties.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + cat /tmp/cluster/properties/hadoop-env.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + loginfo 'Merged /tmp/cluster/properties/hadoop-env.sh.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'Merged /tmp/cluster/properties/hadoop-env.sh.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: Merged /tmp/cluster/properties/hadoop-env.sh.
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + merge_sh_env_vars /tmp/cluster/properties/mapred-env.sh /etc/hadoop/conf/mapred-env.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local src=/tmp/cluster/properties/mapred-env.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local dest=/etc/hadoop/conf/mapred-env.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ ! -f /tmp/cluster/properties/mapred-env.sh ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo -e '\n# User-supplied properties.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + cat /tmp/cluster/properties/mapred-env.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + loginfo 'Merged /tmp/cluster/properties/mapred-env.sh.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'Merged /tmp/cluster/properties/mapred-env.sh.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: Merged /tmp/cluster/properties/mapred-env.sh.
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + merge_sh_env_vars /tmp/cluster/properties/yarn-env.sh /etc/hadoop/conf/yarn-env.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local src=/tmp/cluster/properties/yarn-env.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local dest=/etc/hadoop/conf/yarn-env.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ ! -f /tmp/cluster/properties/yarn-env.sh ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo -e '\n# User-supplied properties.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + cat /tmp/cluster/properties/yarn-env.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + loginfo 'Merged /tmp/cluster/properties/yarn-env.sh.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'Merged /tmp/cluster/properties/yarn-env.sh.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: Merged /tmp/cluster/properties/yarn-env.sh.
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + merge_java_properties /tmp/cluster/properties/hadoop-log4j.properties /etc/hadoop/conf/log4j.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r src=/tmp/cluster/properties/hadoop-log4j.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r dest=/etc/hadoop/conf/log4j.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r 'header=\n# User-supplied properties.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ ! -f /tmp/cluster/properties/hadoop-log4j.properties ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo -e '\n# User-supplied properties.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + cat /tmp/cluster/properties/hadoop-log4j.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + loginfo 'Merged /tmp/cluster/properties/hadoop-log4j.properties.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'Merged /tmp/cluster/properties/hadoop-log4j.properties.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: Merged /tmp/cluster/properties/hadoop-log4j.properties.
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + merge_java_properties /tmp/cluster/properties/zookeeper-log4j.properties /etc/zookeeper/conf/log4j.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r src=/tmp/cluster/properties/zookeeper-log4j.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r dest=/etc/zookeeper/conf/log4j.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r 'header=\n# User-supplied properties.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ ! -f /tmp/cluster/properties/zookeeper-log4j.properties ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo -e '\n# User-supplied properties.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + cat /tmp/cluster/properties/zookeeper-log4j.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + loginfo 'Merged /tmp/cluster/properties/zookeeper-log4j.properties.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'Merged /tmp/cluster/properties/zookeeper-log4j.properties.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: Merged /tmp/cluster/properties/zookeeper-log4j.properties.
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ get_java_property /etc/pig/conf/pig.properties log4jconf
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ local -r property_file=/etc/pig/conf/pig.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ local -r property_name=log4jconf
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ grep '^log4jconf=' /etc/pig/conf/pig.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ cut -d = -f 2-
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ tail -n 1
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ property_value=
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ echo ''
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + pig_log4j_location=
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ -n '' ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ -f /etc/hbase/conf/hbase-site.xml ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + merge_xml_properties /tmp/cluster/properties/hbase.xml /etc/hbase/conf/hbase-site.xml
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local src=/tmp/cluster/properties/hbase.xml
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local dest=/etc/hbase/conf/hbase-site.xml
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ ! -f /tmp/cluster/properties/hbase.xml ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + bdconfig merge_configurations --configuration_file /etc/hbase/conf/hbase-site.xml --source_configuration_file /tmp/cluster/properties/hbase.xml --resolve_environment_variables --create_if_absent --clobber
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + loginfo 'Merged /tmp/cluster/properties/hbase.xml.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'Merged /tmp/cluster/properties/hbase.xml.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: Merged /tmp/cluster/properties/hbase.xml.
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + merge_java_properties /tmp/cluster/properties/hbase-log4j.properties /etc/hbase/conf/log4j.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r src=/tmp/cluster/properties/hbase-log4j.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r dest=/etc/hbase/conf/log4j.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r 'header=\n# User-supplied properties.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ ! -f /tmp/cluster/properties/hbase-log4j.properties ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo -e '\n# User-supplied properties.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + cat /tmp/cluster/properties/hbase-log4j.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + loginfo 'Merged /tmp/cluster/properties/hbase-log4j.properties.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'Merged /tmp/cluster/properties/hbase-log4j.properties.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: Merged /tmp/cluster/properties/hbase-log4j.properties.
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ -f /etc/tez/conf/tez-site.xml ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + merge_xml_properties /tmp/cluster/properties/tez.xml /etc/tez/conf/tez-site.xml
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local src=/tmp/cluster/properties/tez.xml
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local dest=/etc/tez/conf/tez-site.xml
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ ! -f /tmp/cluster/properties/tez.xml ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + bdconfig merge_configurations --configuration_file /etc/tez/conf/tez-site.xml --source_configuration_file /tmp/cluster/properties/tez.xml --resolve_environment_variables --create_if_absent --clobber
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + loginfo 'Merged /tmp/cluster/properties/tez.xml.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'Merged /tmp/cluster/properties/tez.xml.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: Merged /tmp/cluster/properties/tez.xml.
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + DATAPROC_COMPONENTS=(${DATAPROC_OPTIONAL_COMPONENTS})
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'DATAPROC_COMPONENTS: docker-ce druid flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server jupyter kafka-server kerberos knox mapreduce miniconda3 mysql presto proxy-agent ranger solr-server spark yarn zeppelin zookeeper-server'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: DATAPROC_COMPONENTS: docker-ce druid flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server jupyter kafka-server kerberos knox mapreduce miniconda3 mysql presto proxy-agent ranger solr-server spark yarn zeppelin zookeeper-server
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + ARTIFACTS_TO_KEEP=("${SERVICES[@]}" ${DATAPROC_COMMON_PACKAGES})
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'ARTIFACTS_TO_KEEP: hadoop-hdfs-namenode hive-metastore hive-server2 solr-server hadoop-yarn-resourcemanager zookeeper-server hive-webhcat-server jupyter knox hadoop-mapreduce-historyserver mysql-server proxy-agent spark-history-server hadoop-yarn-timelineserver zeppelin hadoop-hdfs-secondarynamenode bigtop-utils hadoop-client hadoop-lzo pig tez autofs bash-completion bc git vim wget google-fluentd stackdriver-agent docker-ce druid flink hbase hdfs libhdfs0 hive-metastore hive-server2 hive-hcatalog texlive-xetex texlive-fonts-recommended texlive-plain-generic kafka-server kerberos mapreduce miniconda3 mysql mysql-connector-java presto ranger spark yarn libjansi-java python-numpy libsnappy-dev libzstd-dev libatlas3-base libopenblas-base libapr1 spark-extras python-setuptools linux-headers-4.19.0-14-cloud-amd64 uuid-runtime libssl-dev openssl python-pip python-requests linux-image-amd64 linux-headers-amd64 adoptopenjdk-8-hotspot'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ARTIFACTS_TO_KEEP: hadoop-hdfs-namenode hive-metastore hive-server2 solr-server hadoop-yarn-resourcemanager zookeeper-server hive-webhcat-server jupyter knox hadoop-mapreduce-historyserver mysql-server proxy-agent spark-history-server hadoop-yarn-timelineserver zeppelin hadoop-hdfs-secondarynamenode bigtop-utils hadoop-client hadoop-lzo pig tez autofs bash-completion bc git vim wget google-fluentd stackdriver-agent docker-ce druid flink hbase hdfs libhdfs0 hive-metastore hive-server2 hive-hcatalog texlive-xetex texlive-fonts-recommended texlive-plain-generic kafka-server kerberos mapreduce miniconda3 mysql mysql-connector-java presto ranger spark yarn libjansi-java python-numpy libsnappy-dev libzstd-dev libatlas3-base libopenblas-base libapr1 spark-extras python-setuptools linux-headers-4.19.0-14-cloud-amd64 uuid-runtime libssl-dev openssl python-pip python-requests linux-image-amd64 linux-headers-amd64 adoptopenjdk-8-hotspot
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + DATAPROC_START_AFTER_HDFS_SERVICES=(${DATAPROC_START_AFTER_HDFS_SERVICES})
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'DATAPROC_START_AFTER_HDFS_SERVICES: hadoop-mapreduce-historyserver'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: DATAPROC_START_AFTER_HDFS_SERVICES: hadoop-mapreduce-historyserver
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + COMPONENTS_TO_ACTIVATE=($(intersection COMPONENTS_TO_ACTIVATE ARTIFACTS_TO_KEEP))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ intersection COMPONENTS_TO_ACTIVATE ARTIFACTS_TO_KEEP
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ local -n values=COMPONENTS_TO_ACTIVATE
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ local -n filter=ARTIFACTS_TO_KEEP
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ comm -12 /dev/fd/63 /dev/fd/62
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ sort -u
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ printf '%s\n' hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ sort -u
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ printf '%s\n' hadoop-hdfs-namenode hive-metastore hive-server2 solr-server hadoop-yarn-resourcemanager zookeeper-server hive-webhcat-server jupyter knox hadoop-mapreduce-historyserver mysql-server proxy-agent spark-history-server hadoop-yarn-timelineserver zeppelin hadoop-hdfs-secondarynamenode bigtop-utils hadoop-client hadoop-lzo pig tez autofs bash-completion bc git vim wget google-fluentd stackdriver-agent docker-ce druid flink hbase hdfs libhdfs0 hive-metastore hive-server2 hive-hcatalog texlive-xetex texlive-fonts-recommended texlive-plain-generic kafka-server kerberos mapreduce miniconda3 mysql mysql-connector-java presto ranger spark yarn libjansi-java python-numpy libsnappy-dev libzstd-dev libatlas3-base libopenblas-base libapr1 spark-extras python-setuptools linux-headers-4.19.0-14-cloud-amd64 uuid-runtime libssl-dev openssl python-pip python-requests linux-image-amd64 linux-headers-amd64 adoptopenjdk-8-hotspot
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'COMPONENTS_TO_ACTIVATE: hdfs hive-metastore hive-server2 mapreduce miniconda3 mysql spark yarn'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: COMPONENTS_TO_ACTIVATE: hdfs hive-metastore hive-server2 mapreduce miniconda3 mysql spark yarn
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + loginfo 'Generating components_to_activate_env.sh'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'Generating components_to_activate_env.sh'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: Generating components_to_activate_env.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + cat
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + chmod +x /usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + NON_ACTIVATED_COMPONENTS=($(difference DATAPROC_COMPONENTS COMPONENTS_TO_ACTIVATE))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ difference DATAPROC_COMPONENTS COMPONENTS_TO_ACTIVATE
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ local -n values=DATAPROC_COMPONENTS
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ local -n filter=COMPONENTS_TO_ACTIVATE
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ sort -u
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ printf '%s\n' hdfs hive-metastore hive-server2 mapreduce miniconda3 mysql spark yarn
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ printf '%s\n' docker-ce druid flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server jupyter kafka-server kerberos knox mapreduce miniconda3 mysql presto proxy-agent ranger solr-server spark yarn zeppelin zookeeper-server
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ sort -u
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'NON_ACTIVATED_COMPONENTS: docker-ce druid flink hbase hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent ranger solr-server zeppelin zookeeper-server'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: NON_ACTIVATED_COMPONENTS: docker-ce druid flink hbase hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent ranger solr-server zeppelin zookeeper-server
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + ARTIFACTS_TO_UNINSTALL+=($(difference NON_ACTIVATED_COMPONENTS ARTIFACTS_TO_UNINSTALL))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ difference NON_ACTIVATED_COMPONENTS ARTIFACTS_TO_UNINSTALL
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ local -n values=NON_ACTIVATED_COMPONENTS
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ local -n filter=ARTIFACTS_TO_UNINSTALL
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ printf '%s\n' hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-hdfs-datanode hadoop-yarn-nodemanager
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ printf '%s\n' docker-ce druid flink hbase hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent ranger solr-server zeppelin zookeeper-server
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ sort -u
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ sort -u
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'ARTIFACTS_TO_UNINSTALL: hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-hdfs-datanode hadoop-yarn-nodemanager docker-ce druid flink hbase hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent ranger solr-server zeppelin zookeeper-server'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ARTIFACTS_TO_UNINSTALL: hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-hdfs-datanode hadoop-yarn-nodemanager docker-ce druid flink hbase hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent ranger solr-server zeppelin zookeeper-server
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + COMPONENTS_TO_UNINSTALL=($(intersection ARTIFACTS_TO_UNINSTALL DATAPROC_COMPONENTS))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ intersection ARTIFACTS_TO_UNINSTALL DATAPROC_COMPONENTS
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ local -n values=ARTIFACTS_TO_UNINSTALL
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ local -n filter=DATAPROC_COMPONENTS
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ comm -12 /dev/fd/63 /dev/fd/62
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ sort -u
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ printf '%s\n' hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-hdfs-datanode hadoop-yarn-nodemanager docker-ce druid flink hbase hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent ranger solr-server zeppelin zookeeper-server
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ printf '%s\n' docker-ce druid flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server jupyter kafka-server kerberos knox mapreduce miniconda3 mysql presto proxy-agent ranger solr-server spark yarn zeppelin zookeeper-server
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ sort -u
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'COMPONENTS_TO_UNINSTALL: docker-ce druid flink hbase hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent ranger solr-server zeppelin zookeeper-server'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: COMPONENTS_TO_UNINSTALL: docker-ce druid flink hbase hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent ranger solr-server zeppelin zookeeper-server
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + PACKAGES_TO_UNINSTALL=($(difference ARTIFACTS_TO_UNINSTALL DATAPROC_COMPONENTS))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ difference ARTIFACTS_TO_UNINSTALL DATAPROC_COMPONENTS
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ local -n values=ARTIFACTS_TO_UNINSTALL
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ local -n filter=DATAPROC_COMPONENTS
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ sort -u
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ sort -u
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ printf '%s\n' docker-ce druid flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server jupyter kafka-server kerberos knox mapreduce miniconda3 mysql presto proxy-agent ranger solr-server spark yarn zeppelin zookeeper-server
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ printf '%s\n' hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-hdfs-datanode hadoop-yarn-nodemanager docker-ce druid flink hbase hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent ranger solr-server zeppelin zookeeper-server
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'PACKAGES_TO_UNINSTALL: hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: PACKAGES_TO_UNINSTALL: hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.monitoring.stackdriver.enable
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ local -r property_name=dataproc.monitoring.stackdriver.enable
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.monitoring.stackdriver.enable
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ local -r property_name=dataproc.monitoring.stackdriver.enable
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++++ grep '^dataproc.monitoring.stackdriver.enable=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ property_value=false
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ echo false
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ property_value=false
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ echo false
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + STACKDRIVER_MONITORING_ENABLED=false
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ false == \t\r\u\e ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + loginfo 'Stackdriver monitoring disabled.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'Stackdriver monitoring disabled.'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: Stackdriver monitoring disabled.
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + chmod +x /usr/local/share/google/dataproc/bdutil/verify_setup.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + /usr/local/share/google/dataproc/bdutil/verify_setup.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + loginfo 'Running verify_setup.sh'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'Running verify_setup.sh'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: Running verify_setup.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ get_property_in_xml /etc/hive/conf/hive-site.xml hive.metastore.warehouse.dir
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ local -r path=/etc/hive/conf/hive-site.xml
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ local -r property=hive.metastore.warehouse.dir
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ local -r default_value=
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ local val
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ bdconfig get_property_value --configuration_file /etc/hive/conf/hive-site.xml --name hive.metastore.warehouse.dir
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ val=None
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ [[ None == \N\o\n\e ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ val=
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ echo ''
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + hive_warehouse_dir=
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ '' == \g\s\:\/\/* ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + loginfo 'Pre-activating components'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'Pre-activating components'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: Pre-activating components
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + pre_activate_components hdfs hive-metastore hive-server2 mapreduce miniconda3 mysql spark yarn
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + components=("$@")
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local components
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + run_components_scripts pre-activate hdfs hive-metastore hive-server2 mapreduce miniconda3 mysql spark yarn
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r script_type=pre-activate
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + all_components=("${@:2}")
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r all_components
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + components=()
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local components
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + for component in "${all_components[@]}"
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + components+=("${component}")
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + for component in "${all_components[@]}"
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + components+=("${component}")
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + for component in "${all_components[@]}"
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + components+=("${component}")
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + for component in "${all_components[@]}"
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/mapreduce.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/mapreduce.sh ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + for component in "${all_components[@]}"
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/miniconda3.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/miniconda3.sh ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + for component in "${all_components[@]}"
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + for component in "${all_components[@]}"
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + components+=("${component}")
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + for component in "${all_components[@]}"
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'Components with pre-activate script: hdfs hive-metastore hive-server2 spark'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: Components with pre-activate script: hdfs hive-metastore hive-server2 spark
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r sentinel_dir=/tmp/dataproc/sentinel
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + mkdir -p /tmp/dataproc/sentinel
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + names=()
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local names
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + scripts=()
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local scripts
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + deps=()
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local deps
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + for cmp in "${components[@]}"
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + names+=("${cmp}.${script_type}")
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + for cmp in "${components[@]}"
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + names+=("${cmp}.${script_type}")
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + for cmp in "${components[@]}"
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + names+=("${cmp}.${script_type}")
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + for cmp in "${components[@]}"
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + names+=("${cmp}.${script_type}")
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + execute_task_graph 'hdfs.pre-activate hive-metastore.pre-activate hive-server2.pre-activate spark.pre-activate' '/usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh' '/usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.deps' /tmp/dataproc/sentinel 2.0
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'Generating makefile for the task graph'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: Generating makefile for the task graph
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + mkdir -p /tmp/dataproc/make
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ mktemp /tmp/dataproc/make/makefile.XXXXXX
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + makefile=/tmp/dataproc/make/makefile.ZnHnNv
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + generate_task_graph_makefile 'hdfs.pre-activate hive-metastore.pre-activate hive-server2.pre-activate spark.pre-activate' '/usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh' '/usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.deps' /tmp/dataproc/sentinel 2.0
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + names=()
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local names
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + read -r -a names
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + scripts=()
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local scripts
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + read -r -a scripts
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + deps_manifests=()
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local deps_manifests
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + read -r -a deps_manifests
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r sentinel_dir=/tmp/dataproc/sentinel
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + task_args=("${@:5}")
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r task_args
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + targets=()
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local targets
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + (( i = 0 ))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + (( i < 4 ))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local name=hdfs.pre-activate
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.deps
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + deps=()
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local deps
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.deps ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local target=/tmp/dataproc/sentinel/hdfs.pre-activate
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + targets+=("${target}")
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo -e '/tmp/dataproc/sentinel/hdfs.pre-activate: '
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo -e '\t@echo '\''Running task: hdfs.pre-activate'\'''
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo -e '\tbash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh"; exit 1; }'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo -e '\ttouch /tmp/dataproc/sentinel/hdfs.pre-activate\n'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + (( i++ ))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + (( i < 4 ))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local name=hive-metastore.pre-activate
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.deps
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + deps=()
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local deps
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.deps ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local target=/tmp/dataproc/sentinel/hive-metastore.pre-activate
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + targets+=("${target}")
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo -e '/tmp/dataproc/sentinel/hive-metastore.pre-activate: '
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo -e '\t@echo '\''Running task: hive-metastore.pre-activate'\'''
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo -e '\tbash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh"; exit 1; }'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo -e '\ttouch /tmp/dataproc/sentinel/hive-metastore.pre-activate\n'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + (( i++ ))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + (( i < 4 ))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local name=hive-server2.pre-activate
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.deps
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + deps=()
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local deps
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.deps ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + dep_names=($(cat "${deps_manifest}"))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ cat /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.deps
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local dep_names
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + (( j = 0 ))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + (( j < 2 ))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local dep_name=hdfs.pre-activate
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[  hdfs.pre-activate hive-metastore.pre-activate hive-server2.pre-activate spark.pre-activate  =~  hdfs\.pre-activate  ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + deps+=("${sentinel_dir}/${dep_name}")
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + (( j++ ))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + (( j < 2 ))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local dep_name=hive-metastore.pre-activate
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[  hdfs.pre-activate hive-metastore.pre-activate hive-server2.pre-activate spark.pre-activate  =~  hive-metastore\.pre-activate  ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + deps+=("${sentinel_dir}/${dep_name}")
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + (( j++ ))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + (( j < 2 ))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local target=/tmp/dataproc/sentinel/hive-server2.pre-activate
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + targets+=("${target}")
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo -e '/tmp/dataproc/sentinel/hive-server2.pre-activate: /tmp/dataproc/sentinel/hdfs.pre-activate /tmp/dataproc/sentinel/hive-metastore.pre-activate'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo -e '\t@echo '\''Running task: hive-server2.pre-activate'\'''
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo -e '\tbash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh"; exit 1; }'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo -e '\ttouch /tmp/dataproc/sentinel/hive-server2.pre-activate\n'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + (( i++ ))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + (( i < 4 ))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local name=spark.pre-activate
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.deps
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + deps=()
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local deps
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.deps ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local target=/tmp/dataproc/sentinel/spark.pre-activate
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + targets+=("${target}")
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo -e '/tmp/dataproc/sentinel/spark.pre-activate: '
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo -e '\t@echo '\''Running task: spark.pre-activate'\'''
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo -e '\tbash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh"; exit 1; }'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo -e '\ttouch /tmp/dataproc/sentinel/spark.pre-activate\n'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + (( i++ ))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + (( i < 4 ))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo -e 'all: /tmp/dataproc/sentinel/hdfs.pre-activate /tmp/dataproc/sentinel/hive-metastore.pre-activate /tmp/dataproc/sentinel/hive-server2.pre-activate /tmp/dataproc/sentinel/spark.pre-activate'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'Generated makefile:'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: Generated makefile:
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + cat /tmp/dataproc/make/makefile.ZnHnNv
<13>Mar 21 02:43:11 google-dataproc-startup[948]: /tmp/dataproc/sentinel/hdfs.pre-activate: 
<13>Mar 21 02:43:11 google-dataproc-startup[948]: 	@echo 'Running task: hdfs.pre-activate'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: 	bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh"; exit 1; }
<13>Mar 21 02:43:11 google-dataproc-startup[948]: 	touch /tmp/dataproc/sentinel/hdfs.pre-activate
<13>Mar 21 02:43:11 google-dataproc-startup[948]: 
<13>Mar 21 02:43:11 google-dataproc-startup[948]: /tmp/dataproc/sentinel/hive-metastore.pre-activate: 
<13>Mar 21 02:43:11 google-dataproc-startup[948]: 	@echo 'Running task: hive-metastore.pre-activate'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: 	bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh"; exit 1; }
<13>Mar 21 02:43:11 google-dataproc-startup[948]: 	touch /tmp/dataproc/sentinel/hive-metastore.pre-activate
<13>Mar 21 02:43:11 google-dataproc-startup[948]: 
<13>Mar 21 02:43:11 google-dataproc-startup[948]: /tmp/dataproc/sentinel/hive-server2.pre-activate: /tmp/dataproc/sentinel/hdfs.pre-activate /tmp/dataproc/sentinel/hive-metastore.pre-activate
<13>Mar 21 02:43:11 google-dataproc-startup[948]: 	@echo 'Running task: hive-server2.pre-activate'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: 	bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh"; exit 1; }
<13>Mar 21 02:43:11 google-dataproc-startup[948]: 	touch /tmp/dataproc/sentinel/hive-server2.pre-activate
<13>Mar 21 02:43:11 google-dataproc-startup[948]: 
<13>Mar 21 02:43:11 google-dataproc-startup[948]: /tmp/dataproc/sentinel/spark.pre-activate: 
<13>Mar 21 02:43:11 google-dataproc-startup[948]: 	@echo 'Running task: spark.pre-activate'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: 	bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh"; exit 1; }
<13>Mar 21 02:43:11 google-dataproc-startup[948]: 	touch /tmp/dataproc/sentinel/spark.pre-activate
<13>Mar 21 02:43:11 google-dataproc-startup[948]: 
<13>Mar 21 02:43:11 google-dataproc-startup[948]: all: /tmp/dataproc/sentinel/hdfs.pre-activate /tmp/dataproc/sentinel/hive-metastore.pre-activate /tmp/dataproc/sentinel/hive-server2.pre-activate /tmp/dataproc/sentinel/spark.pre-activate
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + echo 'Running task graph:'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: Running task graph:
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + make -f /tmp/dataproc/make/makefile.ZnHnNv all
<13>Mar 21 02:43:11 google-dataproc-startup[948]: Running task: hdfs.pre-activate
<13>Mar 21 02:43:11 google-dataproc-startup[948]: bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh"; exit 1; }
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + set -euxo pipefail
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + set -a
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_env.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ ENABLE_HDFS=1
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ NUM_WORKERS=10
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ WORKERS=()
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + set +a
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_helpers.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ is_centos
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ . /etc/os-release
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++++ VERSION_ID=10
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++++ VERSION='10 (buster)'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++++ VERSION_CODENAME=buster
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++++ ID=debian
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ echo debian
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ return 1
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../cluster_properties.sh
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + set_log_tag pre-activate-component-hdfs
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + local -r tag=pre-activate-component-hdfs
<13>Mar 21 02:43:11 google-dataproc-startup[948]: + exec
<13>Mar 21 02:43:11 google-dataproc-startup[948]: ++ logger -s -t 'pre-activate-component-hdfs[1711]'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + readonly HDFS_ADMIN=hdfs
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + HDFS_ADMIN=hdfs
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + export HDFS_NAME_DIR=/hadoop/dfs/name
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + HDFS_NAME_DIR=/hadoop/dfs/name
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + export HDFS_SECONDARY_NAME_DIR=/hadoop/dfs/namesecondary
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + HDFS_SECONDARY_NAME_DIR=/hadoop/dfs/namesecondary
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ get_metadata_cluster_name
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ get_dataproc_metadata DATAPROC_METADATA_CLUSTER_NAME attributes/dataproc-cluster-name
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ local -r var_name=DATAPROC_METADATA_CLUSTER_NAME
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ local -r attr_name=attributes/dataproc-cluster-name
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ [[ -v DATAPROC_METADATA_CLUSTER_NAME ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ /usr/share/google/get_metadata_value attributes/dataproc-cluster-name
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + CLUSTER_NAME=student-01-71e17b39d43c-qwiklab
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + export CLUSTER_NAME
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ get_metadata_master
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ local -r var_name=DATAPROC_METADATA_MASTER
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ local -r attr_name=attributes/dataproc-master
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ [[ -v DATAPROC_METADATA_MASTER ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ /usr/share/google/get_metadata_value attributes/dataproc-master
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + DATAPROC_MASTER=student-01-71e17b39d43c-qwiklab-m
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ get_metadata_master_additional
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ local -r var_name=DATAPROC_METADATA_MASTER_ADDITIONAL
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ local -r attr_name=attributes/dataproc-master-additional
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ [[ -v DATAPROC_METADATA_MASTER_ADDITIONAL ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ /usr/share/google/get_metadata_value attributes/dataproc-master-additional
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + DATAPROC_MASTER_ADDITIONAL=
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + NUM_MASTERS=1
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ get_metadata_worker_count
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ get_dataproc_metadata DATAPROC_METADATA_WORKER_COUNT attributes/dataproc-worker-count
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ local -r var_name=DATAPROC_METADATA_WORKER_COUNT
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ local -r attr_name=attributes/dataproc-worker-count
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ [[ -v DATAPROC_METADATA_WORKER_COUNT ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ /usr/share/google/get_metadata_value attributes/dataproc-worker-count
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + NUM_WORKERS=2
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + DATANODE_PACKAGES=('hadoop-hdfs-datanode')
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + (( i = 0 ))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + (( i < 1 ))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + declare MASTER_HOSTNAME_0=student-01-71e17b39d43c-qwiklab-m
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + export MASTER_HOSTNAME_0
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + (( i++ ))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + (( i < 1 ))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + DATA_DIRS_ARRAY=($(get_data_dirs))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ get_data_dirs
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ local -a mount_points
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ mapfile -t mount_points
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: +++ find '/mnt/[0-9]*/' -maxdepth 0
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: find: ‘/mnt/[0-9]*/’: No such file or directory
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: +++ true
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ (( 0 ))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ echo /
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ return
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + HDFS_DIRS=/hadoop/dfs
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + HDFS_DIRS_ARRAY=(${HDFS_DIRS})
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + HDFS_DATA_DIRS_ARRAY=(${HDFS_DATA_DIRS})
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + mkdir -p /hadoop/dfs /hadoop/dfs/data
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + chown -L -R hdfs:hadoop /hadoop/dfs /hadoop/dfs
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + chmod -R 700 /hadoop/dfs
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ free -m
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ awk '/^Mem:/{print $2}'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + TOTAL_MEM=7454
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ python -c 'print int(7454 * 0.4 / 2)'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + NAMENODE_MEM_MB=1490
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + SECONDARYNAMENODE_MEM_MB=1490
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + cat
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + export HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + [[ 1 -gt 1 ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + readonly HDFS_TEMPLATE=hdfs-template.xml
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + HDFS_TEMPLATE=hdfs-template.xml
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/hdfs-site.xml --source_configuration_file hdfs-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + (( NUM_WORKERS == 0 ))
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ get_dataproc_property simplified.scaling.enable
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ local -r property_name=simplified.scaling.enable
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ local property_value
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: +++ get_java_property /etc/google-dataproc/dataproc.properties simplified.scaling.enable
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: +++ local -r property_name=simplified.scaling.enable
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: +++ local property_value
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++++ grep '^simplified.scaling.enable=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++++ tail -n 1
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: +++ property_value=true
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: +++ echo true
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ property_value=true
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: ++ echo true
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + SIMPLIFIED_SCALING_ENABLED=true
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + readonly SIMPLIFIED_SCALING_ENABLED
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + [[ true == \t\r\u\e ]]
<13>Mar 21 02:43:11 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + [[ 1 -gt 1 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:11 pre-activate-component-hdfs[1711]: + readonly HDFS_SIMPLIFICATION_MIXINS=hdfs-simplification-mixins.xml
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hdfs[1711]: + HDFS_SIMPLIFICATION_MIXINS=hdfs-simplification-mixins.xml
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hdfs[1711]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/hdfs-site.xml --source_configuration_file /usr/local/share/google/dataproc/bdutil/conf/hdfs-simplification-mixins.xml --resolve_environment_variables --create_if_absent --clobber
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hdfs[1711]: + merge_xml_properties /tmp/cluster/properties/hdfs.xml /etc/hadoop/conf/hdfs-site.xml
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hdfs[1711]: + local src=/tmp/cluster/properties/hdfs.xml
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hdfs[1711]: + local dest=/etc/hadoop/conf/hdfs-site.xml
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hdfs[1711]: + [[ ! -f /tmp/cluster/properties/hdfs.xml ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hdfs[1711]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/hdfs-site.xml --source_configuration_file /tmp/cluster/properties/hdfs.xml --resolve_environment_variables --create_if_absent --clobber
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hdfs[1711]: + loginfo 'Merged /tmp/cluster/properties/hdfs.xml.'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hdfs[1711]: + echo 'Merged /tmp/cluster/properties/hdfs.xml.'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hdfs[1711]: Merged /tmp/cluster/properties/hdfs.xml.
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hdfs[1711]: + [[ false == \t\r\u\e ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hdfs[1711]: ++ get_metadata_datanode_enabled
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hdfs[1711]: ++ get_dataproc_metadata DATAPROC_METADATA_DATANODE_ENABLED attributes/dataproc-datanode-enabled
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hdfs[1711]: ++ local -r var_name=DATAPROC_METADATA_DATANODE_ENABLED
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hdfs[1711]: ++ local -r attr_name=attributes/dataproc-datanode-enabled
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hdfs[1711]: ++ [[ -v DATAPROC_METADATA_DATANODE_ENABLED ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hdfs[1711]: ++ /usr/share/google/get_metadata_value attributes/dataproc-datanode-enabled
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hdfs[1711]: + [[ false != \t\r\u\e ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hdfs[1711]: + mark_packages_to_uninstall hadoop-hdfs-datanode
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hdfs[1711]: + for package in "$@"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hdfs[1711]: + touch /tmp/dataproc/uninstall/hadoop-hdfs-datanode
<13>Mar 21 02:43:12 google-dataproc-startup[948]: touch /tmp/dataproc/sentinel/hdfs.pre-activate
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Running task: hive-metastore.pre-activate
<13>Mar 21 02:43:12 google-dataproc-startup[948]: bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh"; exit 1; }
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + set -euxo pipefail
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NUM_WORKERS=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ WORKERS=()
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ is_centos
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ . /etc/os-release
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_ID=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION='10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_CODENAME=buster
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ ID=debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ echo debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ return 1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../cluster_properties.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + set_log_tag pre-activate-component-hive-metastore
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r tag=pre-activate-component-hive-metastore
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + exec
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ logger -s -t 'pre-activate-component-hive-metastore[1789]'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-metastore[1789]: + HIVE_CONF_DIR=/etc/hive/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-metastore[1789]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-metastore[1789]: ++ get_metadata_master
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-metastore[1789]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-metastore[1789]: ++ local -r var_name=DATAPROC_METADATA_MASTER
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-metastore[1789]: ++ local -r attr_name=attributes/dataproc-master
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-metastore[1789]: ++ [[ -v DATAPROC_METADATA_MASTER ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-metastore[1789]: ++ /usr/share/google/get_metadata_value attributes/dataproc-master
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-metastore[1789]: + DATAPROC_MASTER=student-01-71e17b39d43c-qwiklab-m
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-metastore[1789]: ++ get_metadata_master_additional
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-metastore[1789]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-metastore[1789]: ++ local -r var_name=DATAPROC_METADATA_MASTER_ADDITIONAL
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-metastore[1789]: ++ local -r attr_name=attributes/dataproc-master-additional
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-metastore[1789]: ++ [[ -v DATAPROC_METADATA_MASTER_ADDITIONAL ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-metastore[1789]: ++ /usr/share/google/get_metadata_value attributes/dataproc-master-additional
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-metastore[1789]: + DATAPROC_MASTER_ADDITIONAL=
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-metastore[1789]: + MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-metastore[1789]: + NUM_MASTERS=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-metastore[1789]: + [[ 1 -gt 1 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-metastore[1789]: + METASTORE_URIS=thrift://student-01-71e17b39d43c-qwiklab-m:9083
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-metastore[1789]: + bdconfig set_property --configuration_file /etc/hive/conf/hive-site.xml --name hive.metastore.uris --value thrift://student-01-71e17b39d43c-qwiklab-m:9083 --clobber
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-metastore[1789]: + [[ true == \f\a\l\s\e ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-metastore[1789]: + METADATASTORE_JDBC_URI=jdbc:mysql://student-01-71e17b39d43c-qwiklab-m/metastore
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-metastore[1789]: + bdconfig set_property --configuration_file /etc/hive/conf/hive-site.xml --name javax.jdo.option.ConnectionURL --value jdbc:mysql://student-01-71e17b39d43c-qwiklab-m/metastore --clobber
<13>Mar 21 02:43:12 google-dataproc-startup[948]: touch /tmp/dataproc/sentinel/hive-metastore.pre-activate
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Running task: hive-server2.pre-activate
<13>Mar 21 02:43:12 google-dataproc-startup[948]: bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh"; exit 1; }
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + set -euxo pipefail
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NUM_WORKERS=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ WORKERS=()
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ is_centos
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ . /etc/os-release
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_ID=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION='10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_CODENAME=buster
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ ID=debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ echo debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ return 1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../cluster_properties.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + set_log_tag pre-activate-component-hive-server2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r tag=pre-activate-component-hive-server2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + exec
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ logger -s -t 'pre-activate-component-hive-server2[1821]'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + HIVE_CONF_DIR=/etc/hive/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: ++ get_metadata_bucket
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: ++ get_dataproc_metadata DATAPROC_METADATA_BUCKET attributes/dataproc-bucket
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: ++ local -r var_name=DATAPROC_METADATA_BUCKET
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: ++ local -r attr_name=attributes/dataproc-bucket
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: ++ [[ -v DATAPROC_METADATA_BUCKET ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: ++ /usr/share/google/get_metadata_value attributes/dataproc-bucket
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + CONFIGBUCKET=student-01-71e17b39d43c-image-26164
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: ++ get_metadata_cluster_uuid
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: ++ get_dataproc_metadata DATAPROC_METADATA_CLUSTER_UUID attributes/dataproc-cluster-uuid
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: ++ local -r var_name=DATAPROC_METADATA_CLUSTER_UUID
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: ++ local -r attr_name=attributes/dataproc-cluster-uuid
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: ++ [[ -v DATAPROC_METADATA_CLUSTER_UUID ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: ++ /usr/share/google/get_metadata_value attributes/dataproc-cluster-uuid
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + CLUSTER_UUID=1508fba4-d944-4b05-b441-abe36766e1e0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: ++ get_metadata_master
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: ++ local -r var_name=DATAPROC_METADATA_MASTER
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: ++ local -r attr_name=attributes/dataproc-master
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: ++ [[ -v DATAPROC_METADATA_MASTER ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: ++ /usr/share/google/get_metadata_value attributes/dataproc-master
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + DATAPROC_MASTER=student-01-71e17b39d43c-qwiklab-m
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: ++ get_metadata_master_additional
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: ++ local -r var_name=DATAPROC_METADATA_MASTER_ADDITIONAL
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: ++ local -r attr_name=attributes/dataproc-master-additional
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: ++ [[ -v DATAPROC_METADATA_MASTER_ADDITIONAL ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: ++ /usr/share/google/get_metadata_value attributes/dataproc-master-additional
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + DATAPROC_MASTER_ADDITIONAL=
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + NUM_MASTERS=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + [[ 1 -gt 1 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + bdconfig set_property --configuration_file /etc/hive/conf/hive-site.xml --name hive.user.install.directory --value gs://student-01-71e17b39d43c-image-26164/google-cloud-dataproc-metainfo/1508fba4-d944-4b05-b441-abe36766e1e0/hive/user-install-dir --clobber
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + merge_xml_properties /tmp/cluster/properties/hive.xml /etc/hive/conf/hive-site.xml
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + local src=/tmp/cluster/properties/hive.xml
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + local dest=/etc/hive/conf/hive-site.xml
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + [[ ! -f /tmp/cluster/properties/hive.xml ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + bdconfig merge_configurations --configuration_file /etc/hive/conf/hive-site.xml --source_configuration_file /tmp/cluster/properties/hive.xml --resolve_environment_variables --create_if_absent --clobber
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + loginfo 'Merged /tmp/cluster/properties/hive.xml.'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + echo 'Merged /tmp/cluster/properties/hive.xml.'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: Merged /tmp/cluster/properties/hive.xml.
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + merge_java_properties /tmp/cluster/properties/hive-log4j2.properties /etc/hive/conf/hive-log4j2.properties
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + local -r src=/tmp/cluster/properties/hive-log4j2.properties
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + local -r dest=/etc/hive/conf/hive-log4j2.properties
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + local -r 'header=\n# User-supplied properties.'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + [[ ! -f /tmp/cluster/properties/hive-log4j2.properties ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + echo -e '\n# User-supplied properties.'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + cat /tmp/cluster/properties/hive-log4j2.properties
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + loginfo 'Merged /tmp/cluster/properties/hive-log4j2.properties.'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: + echo 'Merged /tmp/cluster/properties/hive-log4j2.properties.'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-hive-server2[1821]: Merged /tmp/cluster/properties/hive-log4j2.properties.
<13>Mar 21 02:43:12 google-dataproc-startup[948]: touch /tmp/dataproc/sentinel/hive-server2.pre-activate
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Running task: spark.pre-activate
<13>Mar 21 02:43:12 google-dataproc-startup[948]: bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh"; exit 1; }
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + set -euxo pipefail
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NUM_WORKERS=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ WORKERS=()
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ is_centos
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ . /etc/os-release
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_ID=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION='10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_CODENAME=buster
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ ID=debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ echo debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ return 1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../cluster_properties.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/dataproc_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CLUSTER_NAME=student-01-71e17b39d43c-qwiklab
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CLUSTER_UUID=1508fba4-d944-4b05-b441-abe36766e1e0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CONFIGBUCKET=student-01-71e17b39d43c-image-26164
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_ROOT_URI=hdfs://student-01-71e17b39d43c-qwiklab-m
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ MASTER_HOSTNAME_0=student-01-71e17b39d43c-qwiklab-m
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ MASTER_HOSTNAMES=(student-01-71e17b39d43c-qwiklab-m)
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NUM_MASTERS=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NUM_WORKERS=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ PREFIX=student-01-71e17b39d43c-qwiklab
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ PROJECT=qwiklabs-gcp-01-d65f02c71c6a
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ROLE=Master
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + set_log_tag pre-activate-component-spark
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r tag=pre-activate-component-spark
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + exec
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ logger -s -t 'pre-activate-component-spark[1870]'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + readonly HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + readonly SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + SPARK_EVENTLOG_DIR=hdfs://student-01-71e17b39d43c-qwiklab-m/user/spark/eventlog
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + readonly SPARK_EVENTLOG_DIR
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + readonly SPARK_TMPDIR=/hadoop/spark/tmp
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + SPARK_TMPDIR=/hadoop/spark/tmp
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + readonly SPARK_WORKDIR=/hadoop/spark/work
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + SPARK_WORKDIR=/hadoop/spark/work
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + readonly SPARK_LOG_DIR=/var/log/spark
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + SPARK_LOG_DIR=/var/log/spark
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + init_dirs
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r spark_hadoop_dir=/hadoop/spark
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + mkdir -p /hadoop/spark/tmp /hadoop/spark/work /var/log/spark
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + chown spark:spark -R /hadoop/spark /var/log/spark
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + chmod 1777 -R /hadoop/spark /var/log/spark
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + configure_env
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + cat
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + is_version_at_least 2.0 1.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version1=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version2=1.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + loginfo 'Comparing if version 2.0 is at least version 1.4 '
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + echo 'Comparing if version 2.0 is at least version 1.4 '
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: Comparing if version 2.0 is at least version 1.4 
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + compare_versions 2.0 1.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version1=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version2=1.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + validate_version 2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ 2.0 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + return 0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + validate_version 1.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version=1.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ 1.4 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + return 0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ 2.0 == \1\.\4 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local IFS=.
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + ver1=($version1)
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + ver2=($version2)
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local i ver1 ver2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + (( i=2 ))
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + (( i<2 ))
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + (( i=0 ))
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + (( i<2 ))
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ -z 1 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + (( 10#2 > 10#1 ))
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + return 1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + case $? in
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + return 0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + cat
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r spark_yarn_dir=/usr/lib/spark/yarn
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + is_version_at_least 2.0 1.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version1=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version2=1.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + loginfo 'Comparing if version 2.0 is at least version 1.4 '
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + echo 'Comparing if version 2.0 is at least version 1.4 '
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: Comparing if version 2.0 is at least version 1.4 
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + compare_versions 2.0 1.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version1=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version2=1.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + validate_version 2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ 2.0 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + return 0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + validate_version 1.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version=1.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ 1.4 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + return 0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ 2.0 == \1\.\4 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local IFS=.
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + ver1=($version1)
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + ver2=($version2)
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local i ver1 ver2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + (( i=2 ))
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + (( i<2 ))
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + (( i=0 ))
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + (( i<2 ))
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ -z 1 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + (( 10#2 > 10#1 ))
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + return 1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + case $? in
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + return 0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + cat
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + configure_arrow
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + is_version_at_least 2.0 1.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version1=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version2=1.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + loginfo 'Comparing if version 2.0 is at least version 1.4 '
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + echo 'Comparing if version 2.0 is at least version 1.4 '
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: Comparing if version 2.0 is at least version 1.4 
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + compare_versions 2.0 1.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version1=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version2=1.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + validate_version 2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ 2.0 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + return 0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + validate_version 1.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version=1.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ 1.4 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + return 0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ 2.0 == \1\.\4 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local IFS=.
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + ver1=($version1)
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + ver2=($version2)
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local i ver1 ver2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + (( i=2 ))
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + (( i<2 ))
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + (( i=0 ))
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + (( i<2 ))
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ -z 1 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + (( 10#2 > 10#1 ))
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + return 1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + case $? in
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + return 0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + is_version_at_least 2.0 2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version1=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version2=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + loginfo 'Comparing if version 2.0 is at least version 2.0 '
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + echo 'Comparing if version 2.0 is at least version 2.0 '
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: Comparing if version 2.0 is at least version 2.0 
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + compare_versions 2.0 2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version1=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version2=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + validate_version 2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ 2.0 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + return 0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + validate_version 2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ 2.0 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + return 0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ 2.0 == \2\.\0 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + return 0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + case $? in
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + return 0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + configure_defaults
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + cat
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + is_version_at_least 2.0 2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version1=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version2=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + loginfo 'Comparing if version 2.0 is at least version 2.0 '
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + echo 'Comparing if version 2.0 is at least version 2.0 '
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: Comparing if version 2.0 is at least version 2.0 
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + compare_versions 2.0 2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version1=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version2=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + validate_version 2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ 2.0 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + return 0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + validate_version 2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ 2.0 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + return 0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ 2.0 == \2\.\0 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + return 0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + case $? in
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + return 0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + is_component_selected hive-server2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r component=hive-server2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local activated_components
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ get_components_to_activate
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ tr '[:upper:]' '[:lower:]'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ get_dataproc_property dataproc.components.activate
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ local -r property_name=dataproc.components.activate
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ local property_value
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: +++ local -r property_name=dataproc.components.activate
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: +++ local property_value
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++++ tail -n 1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: +++ property_value='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: +++ echo 'hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ property_value='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ echo 'hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + activated_components='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3 == *hive-server2* ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + cat
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + configure_broadcast_join
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + is_version_at_least 2.0 2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version1=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version2=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + loginfo 'Comparing if version 2.0 is at least version 2.0 '
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + echo 'Comparing if version 2.0 is at least version 2.0 '
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: Comparing if version 2.0 is at least version 2.0 
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + compare_versions 2.0 2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version1=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version2=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + validate_version 2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ 2.0 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + return 0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + validate_version 2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r version=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ 2.0 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + return 0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ 2.0 == \2\.\0 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + return 0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + case $? in
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + return 0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local spark_executor_memory
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ get_java_property /tmp/cluster/properties/spark.properties spark.executor.memory
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ local -r property_file=/tmp/cluster/properties/spark.properties
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ local -r property_name=spark.executor.memory
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ local property_value
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: +++ tail -n 1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: +++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: +++ cut -d = -f 2-
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: +++ grep '^spark.executor.memory=' /tmp/cluster/properties/spark.properties
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ property_value=2688m
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ echo 2688m
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + spark_executor_memory=2688m
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local spark_executor_memory_bytes
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ get_size_in_bytes 2688m
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ local size=2688M
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ size=2688M
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ size=2688M
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ numfmt --from=iec 2688M
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + spark_executor_memory_bytes=2818572288
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local broadcast_join_threshold_mb
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ python -c 'print min(max(int(2818572288 * 0.0075 / 1024 / 1024), 10), 200)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + broadcast_join_threshold_mb=20
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + cat
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + configure_efm
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local spark_efm_property
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ get_dataproc_property efm.spark.shuffle
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ local -r property_name=efm.spark.shuffle
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ local property_value
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: +++ get_java_property /etc/google-dataproc/dataproc.properties efm.spark.shuffle
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: +++ local -r property_name=efm.spark.shuffle
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: +++ local property_value
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++++ grep '^efm.spark.shuffle=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++++ tail -n 1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: +++ property_value=
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: +++ echo ''
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ property_value=
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ echo ''
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + spark_efm_property=
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ '' == \p\r\i\m\a\r\y\-\w\o\r\k\e\r ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + configure_event_logs_dir
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local persist_history_to_gcs
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ get_dataproc_property_or_default job.history.to-gcs.enabled false
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ local -r property_name=job.history.to-gcs.enabled
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ local -r default_value=false
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ local actual_value
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: +++ get_dataproc_property job.history.to-gcs.enabled
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: +++ local -r property_name=job.history.to-gcs.enabled
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: +++ local property_value
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++++ get_java_property /etc/google-dataproc/dataproc.properties job.history.to-gcs.enabled
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++++ local -r property_name=job.history.to-gcs.enabled
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++++ local property_value
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: +++++ grep '^job.history.to-gcs.enabled=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: +++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: +++++ cut -d = -f 2-
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: +++++ tail -n 1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++++ property_value=true
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++++ echo true
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: +++ property_value=true
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: +++ echo true
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ actual_value=true
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ [[ -n true ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: ++ echo true
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + persist_history_to_gcs=true
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ true == \t\r\u\e ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + loginfo 'Enabling persisting Spark job history files to GCS.'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + echo 'Enabling persisting Spark job history files to GCS.'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: Enabling persisting Spark job history files to GCS.
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ -n dataproc-temp-us-central1-147286270203-nrwqwbuk ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + sed -i 's/spark.eventLog.dir.*/spark.eventLog.dir=gs:\/\/dataproc-temp-us-central1-147286270203-nrwqwbuk\/1508fba4-d944-4b05-b441-abe36766e1e0\/spark-job-history/' /etc/spark/conf/spark-defaults.conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + sed -i 's/spark.history.fs.logDirectory.*/spark.history.fs.logDirectory=gs:\/\/dataproc-temp-us-central1-147286270203-nrwqwbuk\/1508fba4-d944-4b05-b441-abe36766e1e0\/spark-job-history/' /etc/spark/conf/spark-defaults.conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + merge_user_properties
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + merge_java_properties /tmp/cluster/properties/spark.properties /etc/spark/conf/spark-defaults.conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r src=/tmp/cluster/properties/spark.properties
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r dest=/etc/spark/conf/spark-defaults.conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r 'header=\n# User-supplied properties.'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ ! -f /tmp/cluster/properties/spark.properties ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + echo -e '\n# User-supplied properties.'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + cat /tmp/cluster/properties/spark.properties
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + loginfo 'Merged /tmp/cluster/properties/spark.properties.'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + echo 'Merged /tmp/cluster/properties/spark.properties.'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: Merged /tmp/cluster/properties/spark.properties.
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + merge_sh_env_vars /tmp/cluster/properties/spark-env.sh /etc/spark/conf/spark-env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local src=/tmp/cluster/properties/spark-env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local dest=/etc/spark/conf/spark-env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ ! -f /tmp/cluster/properties/spark-env.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + echo -e '\n# User-supplied properties.'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + cat /tmp/cluster/properties/spark-env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + loginfo 'Merged /tmp/cluster/properties/spark-env.sh.'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + echo 'Merged /tmp/cluster/properties/spark-env.sh.'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: Merged /tmp/cluster/properties/spark-env.sh.
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + merge_java_properties /tmp/cluster/properties/spark-log4j.properties /etc/spark/conf/log4j.properties
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r src=/tmp/cluster/properties/spark-log4j.properties
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r dest=/etc/spark/conf/log4j.properties
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + local -r 'header=\n# User-supplied properties.'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + [[ ! -f /tmp/cluster/properties/spark-log4j.properties ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + echo -e '\n# User-supplied properties.'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + cat /tmp/cluster/properties/spark-log4j.properties
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + loginfo 'Merged /tmp/cluster/properties/spark-log4j.properties.'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: + echo 'Merged /tmp/cluster/properties/spark-log4j.properties.'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: <13>Mar 21 02:43:12 pre-activate-component-spark[1870]: Merged /tmp/cluster/properties/spark-log4j.properties.
<13>Mar 21 02:43:12 google-dataproc-startup[948]: touch /tmp/dataproc/sentinel/spark.pre-activate
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'All pre-activate scripts done'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: All pre-activate scripts done
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + loginfo 'Pre-uninstalling unselected components'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Pre-uninstalling unselected components'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Pre-uninstalling unselected components
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + pre_uninstall_components docker-ce druid flink hbase hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent ranger solr-server zeppelin zookeeper-server
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + components=("$@")
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local components
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + mkdir -p /tmp/dataproc/components/pre-uninstall
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for component in "${components[@]}"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + loginfo 'Pre-uninstalling component docker-ce'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Pre-uninstalling component docker-ce'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Pre-uninstalling component docker-ce
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + pre_uninstall_component docker-ce
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r component=docker-ce
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/docker-ce.running
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local exit_code=0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + set -euxo pipefail
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NUM_WORKERS=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ WORKERS=()
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ is_centos
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ . /etc/os-release
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_ID=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION='10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_CODENAME=buster
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ ID=debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ echo debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ return 1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + mark_packages_to_uninstall docker-ce
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for package in "$@"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/uninstall/docker-ce
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/docker-ce.done
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for component in "${components[@]}"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + loginfo 'Pre-uninstalling component druid'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Pre-uninstalling component druid'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Pre-uninstalling component druid
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + pre_uninstall_component druid
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r component=druid
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/druid.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/druid.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/druid.sh'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/druid.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/druid.running
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local exit_code=0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/druid.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + set -euxo pipefail
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NUM_WORKERS=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ WORKERS=()
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ is_centos
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ . /etc/os-release
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_ID=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION='10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_CODENAME=buster
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ ID=debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ echo debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ return 1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + mark_packages_to_uninstall druid
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for package in "$@"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/uninstall/druid
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/druid.done
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for component in "${components[@]}"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + loginfo 'Pre-uninstalling component flink'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Pre-uninstalling component flink'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Pre-uninstalling component flink
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + pre_uninstall_component flink
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r component=flink
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/flink.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/flink.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/flink.sh'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/flink.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/flink.running
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local exit_code=0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/flink.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + set -euxo pipefail
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NUM_WORKERS=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ WORKERS=()
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ is_centos
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ . /etc/os-release
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_ID=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION='10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_CODENAME=buster
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ ID=debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ echo debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ return 1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + mark_packages_to_uninstall flink
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for package in "$@"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/uninstall/flink
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/flink.done
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for component in "${components[@]}"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + loginfo 'Pre-uninstalling component hbase'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Pre-uninstalling component hbase'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Pre-uninstalling component hbase
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + pre_uninstall_component hbase
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r component=hbase
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hbase.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hbase.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hbase.sh'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hbase.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/hbase.running
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local exit_code=0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hbase.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + set -euxo pipefail
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NUM_WORKERS=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ WORKERS=()
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ is_centos
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ . /etc/os-release
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_ID=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION='10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_CODENAME=buster
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ ID=debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ echo debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ return 1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + mark_packages_to_uninstall_pre_activate hbase hive-hbase
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for package in "$@"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/uninstall-pre-activate/hbase
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for package in "$@"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/uninstall-pre-activate/hive-hbase
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/hbase.done
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for component in "${components[@]}"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + loginfo 'Pre-uninstalling component hive-webhcat-server'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Pre-uninstalling component hive-webhcat-server'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Pre-uninstalling component hive-webhcat-server
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + pre_uninstall_component hive-webhcat-server
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r component=hive-webhcat-server
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-webhcat-server.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-webhcat-server.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-webhcat-server.sh'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-webhcat-server.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/hive-webhcat-server.running
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local exit_code=0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-webhcat-server.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + set -euxo pipefail
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NUM_WORKERS=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ WORKERS=()
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ is_centos
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ . /etc/os-release
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_ID=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION='10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_CODENAME=buster
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ ID=debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ echo debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ return 1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + mark_packages_to_uninstall hive-webhcat-server
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for package in "$@"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/uninstall/hive-webhcat-server
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/hive-webhcat-server.done
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for component in "${components[@]}"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + loginfo 'Pre-uninstalling component jupyter'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Pre-uninstalling component jupyter'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Pre-uninstalling component jupyter
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + pre_uninstall_component jupyter
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r component=jupyter
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/jupyter.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/jupyter.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Component jupyter doesn'\''t have a pre-uninstall script'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Component jupyter doesn't have a pre-uninstall script
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for component in "${components[@]}"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + loginfo 'Pre-uninstalling component kafka-server'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Pre-uninstalling component kafka-server'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Pre-uninstalling component kafka-server
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + pre_uninstall_component kafka-server
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r component=kafka-server
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kafka-server.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kafka-server.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kafka-server.sh'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kafka-server.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/kafka-server.running
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local exit_code=0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kafka-server.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + set -euxo pipefail
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NUM_WORKERS=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ WORKERS=()
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ is_centos
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ . /etc/os-release
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_ID=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION='10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_CODENAME=buster
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ ID=debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ echo debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ return 1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + mark_packages_to_uninstall kafka-server
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for package in "$@"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/uninstall/kafka-server
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/kafka-server.done
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for component in "${components[@]}"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + loginfo 'Pre-uninstalling component kerberos'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Pre-uninstalling component kerberos'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Pre-uninstalling component kerberos
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + pre_uninstall_component kerberos
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r component=kerberos
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kerberos.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kerberos.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kerberos.sh'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kerberos.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/kerberos.running
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local exit_code=0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kerberos.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + set -euxo pipefail
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NUM_WORKERS=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ WORKERS=()
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ is_centos
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ . /etc/os-release
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_ID=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION='10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_CODENAME=buster
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ ID=debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ echo debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ return 1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kerberos.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/../shared/kerberos.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/components/shared/kerberos.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ KDC_SERVICE=krb5-kdc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ KADMIN_SERVICE=krb5-admin-server
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ KPROP_SERVICE=krb5-kpropd
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ KERBEROS_LIBRARIES=(krb5-user krb5-config krb5-kdc krb5-admin-server krb5-kpropd xinetd)
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ KERBEROS_SERVICES=(${KDC_SERVICE} ${KADMIN_SERVICE} ${KPROP_SERVICE})
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ KDC_CONF_DIR=/etc/krb5kdc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ KERBEROS_DATABASE_DIR=/var/lib/krb5kdc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ KERBEROS_USER_CONFIG=/etc/krb5.conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + is_centos
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ . /etc/os-release
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ VERSION_ID=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ VERSION='10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ VERSION_CODENAME=buster
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ ID=debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ echo debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + return 1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + mark_packages_to_uninstall krb5-user krb5-config krb5-kdc krb5-admin-server krb5-kpropd xinetd
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for package in "$@"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/uninstall/krb5-user
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for package in "$@"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/uninstall/krb5-config
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for package in "$@"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/uninstall/krb5-kdc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for package in "$@"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/uninstall/krb5-admin-server
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for package in "$@"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/uninstall/krb5-kpropd
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for package in "$@"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/uninstall/xinetd
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/kerberos.done
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for component in "${components[@]}"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + loginfo 'Pre-uninstalling component knox'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Pre-uninstalling component knox'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Pre-uninstalling component knox
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + pre_uninstall_component knox
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r component=knox
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/knox.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/knox.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/knox.sh'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/knox.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/knox.running
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local exit_code=0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/knox.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + set -euxo pipefail
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NUM_WORKERS=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ WORKERS=()
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ is_centos
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ . /etc/os-release
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_ID=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION='10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_CODENAME=buster
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ ID=debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ echo debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ return 1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + mark_packages_to_uninstall knox
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for package in "$@"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/uninstall/knox
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/knox.done
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for component in "${components[@]}"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + loginfo 'Pre-uninstalling component presto'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Pre-uninstalling component presto'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Pre-uninstalling component presto
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + pre_uninstall_component presto
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r component=presto
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/presto.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/presto.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/presto.sh'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/presto.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/presto.running
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local exit_code=0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/presto.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + set -euxo pipefail
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NUM_WORKERS=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ WORKERS=()
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ is_centos
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ . /etc/os-release
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_ID=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION='10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_CODENAME=buster
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ ID=debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ echo debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ return 1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + mark_packages_to_uninstall presto
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for package in "$@"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/uninstall/presto
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/presto.done
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for component in "${components[@]}"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + loginfo 'Pre-uninstalling component proxy-agent'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Pre-uninstalling component proxy-agent'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Pre-uninstalling component proxy-agent
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + pre_uninstall_component proxy-agent
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r component=proxy-agent
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/proxy-agent.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/proxy-agent.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Component proxy-agent doesn'\''t have a pre-uninstall script'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Component proxy-agent doesn't have a pre-uninstall script
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for component in "${components[@]}"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + loginfo 'Pre-uninstalling component ranger'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Pre-uninstalling component ranger'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Pre-uninstalling component ranger
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + pre_uninstall_component ranger
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r component=ranger
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/ranger.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/ranger.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/ranger.sh'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/ranger.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/ranger.running
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local exit_code=0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/ranger.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + set -euxo pipefail
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ NUM_WORKERS=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ WORKERS=()
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ is_centos
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ . /etc/os-release
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_ID=10
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION='10 (buster)'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ VERSION_CODENAME=buster
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ ID=debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ echo debian
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ return 1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + mark_packages_to_uninstall ranger
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for package in "$@"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/uninstall/ranger
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/ranger.done
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + for component in "${components[@]}"
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + loginfo 'Pre-uninstalling component solr-server'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Pre-uninstalling component solr-server'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Pre-uninstalling component solr-server
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + pre_uninstall_component solr-server
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r component=solr-server
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/solr-server.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/solr-server.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/solr-server.sh'
<13>Mar 21 02:43:12 google-dataproc-startup[948]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/solr-server.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/solr-server.running
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + local exit_code=0
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/solr-server.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + set -euxo pipefail
<13>Mar 21 02:43:12 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS=1
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:12 google-dataproc-startup[948]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ NUM_WORKERS=10
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ WORKERS=()
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ is_centos
<13>Mar 21 02:43:13 google-dataproc-startup[948]: +++ . /etc/os-release
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ VERSION_ID=10
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ VERSION='10 (buster)'
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ VERSION_CODENAME=buster
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ ID=debian
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:13 google-dataproc-startup[948]: +++ echo debian
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ return 1
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 21 02:43:13 google-dataproc-startup[948]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + mark_packages_to_uninstall solr-server
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + for package in "$@"
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + touch /tmp/dataproc/uninstall/solr-server
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/solr-server.done
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + for component in "${components[@]}"
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + loginfo 'Pre-uninstalling component zeppelin'
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + echo 'Pre-uninstalling component zeppelin'
<13>Mar 21 02:43:13 google-dataproc-startup[948]: Pre-uninstalling component zeppelin
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + pre_uninstall_component zeppelin
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + local -r component=zeppelin
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zeppelin.sh
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zeppelin.sh ]]
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zeppelin.sh'
<13>Mar 21 02:43:13 google-dataproc-startup[948]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zeppelin.sh
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/zeppelin.running
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + local exit_code=0
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zeppelin.sh
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + set -euxo pipefail
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ ENABLE_HDFS=1
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ NUM_WORKERS=10
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ WORKERS=()
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ is_centos
<13>Mar 21 02:43:13 google-dataproc-startup[948]: +++ . /etc/os-release
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ VERSION_ID=10
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ VERSION='10 (buster)'
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ VERSION_CODENAME=buster
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ ID=debian
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:13 google-dataproc-startup[948]: +++ echo debian
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ return 1
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 21 02:43:13 google-dataproc-startup[948]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + mark_packages_to_uninstall zeppelin
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + for package in "$@"
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + touch /tmp/dataproc/uninstall/zeppelin
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/zeppelin.done
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + for component in "${components[@]}"
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + loginfo 'Pre-uninstalling component zookeeper-server'
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + echo 'Pre-uninstalling component zookeeper-server'
<13>Mar 21 02:43:13 google-dataproc-startup[948]: Pre-uninstalling component zookeeper-server
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + pre_uninstall_component zookeeper-server
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + local -r component=zookeeper-server
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zookeeper-server.sh
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zookeeper-server.sh ]]
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zookeeper-server.sh'
<13>Mar 21 02:43:13 google-dataproc-startup[948]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zookeeper-server.sh
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/zookeeper-server.running
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + local exit_code=0
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zookeeper-server.sh
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + set -euxo pipefail
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ ENABLE_HDFS=1
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ NUM_WORKERS=10
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ WORKERS=()
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ is_centos
<13>Mar 21 02:43:13 google-dataproc-startup[948]: +++ . /etc/os-release
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ VERSION_ID=10
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ VERSION='10 (buster)'
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ VERSION_CODENAME=buster
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ ID=debian
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:13 google-dataproc-startup[948]: +++ echo debian
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ return 1
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 21 02:43:13 google-dataproc-startup[948]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + mark_packages_to_uninstall zookeeper-server
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + for package in "$@"
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + touch /tmp/dataproc/uninstall/zookeeper-server
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + touch /tmp/dataproc/components/pre-uninstall/zookeeper-server.done
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + loginfo 'Uninstalling packages which must be uninstalled before activing components'
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + echo 'Uninstalling packages which must be uninstalled before activing components'
<13>Mar 21 02:43:13 google-dataproc-startup[948]: Uninstalling packages which must be uninstalled before activing components
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + uninstall_packages_pre_activate
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + local packages_to_uninstall
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + packages_to_uninstall=($(list_packages_to_uninstall_pre_activate))
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ list_packages_to_uninstall_pre_activate
<13>Mar 21 02:43:13 google-dataproc-startup[948]: ++ find /tmp/dataproc/uninstall-pre-activate/ -type f -exec basename '{}' ';'
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + os_uninstall_packages hive-hbase hbase
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + packages_to_uninstall=("$@")
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + local -r packages_to_uninstall
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + loginfo 'Uninstalling packages: hive-hbase hbase'
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + echo 'Uninstalling packages: hive-hbase hbase'
<13>Mar 21 02:43:13 google-dataproc-startup[948]: Uninstalling packages: hive-hbase hbase
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + local -r 'uninstall_cmd=DEBIAN_FRONTEND=noninteractive     apt-get autoremove -y --purge hive-hbase hbase'
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + retry_constant bash -c 'DEBIAN_FRONTEND=noninteractive     apt-get autoremove -y --purge hive-hbase hbase'
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + retry_constant_custom 300 1 bash -c 'DEBIAN_FRONTEND=noninteractive     apt-get autoremove -y --purge hive-hbase hbase'
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + local -r max_retry_time=300
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + local -r retry_delay=1
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + cmd=("${@:3}")
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + local -r cmd
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + local -r max_retries=300
<13>Mar 21 02:43:13 google-dataproc-startup[948]: + set +x
<13>Mar 21 02:43:13 google-dataproc-startup[948]: About to run 'bash -c DEBIAN_FRONTEND=noninteractive     apt-get autoremove -y --purge hive-hbase hbase' with retries...
<13>Mar 21 02:43:14 google-dataproc-startup[948]: Reading package lists...
<13>Mar 21 02:43:14 google-dataproc-startup[948]: Building dependency tree...
<13>Mar 21 02:43:14 google-dataproc-startup[948]: Reading state information...
<13>Mar 21 02:43:14 google-dataproc-startup[948]: The following packages will be REMOVED:
<13>Mar 21 02:43:14 google-dataproc-startup[948]:   hbase* hive-hbase*
<13>Mar 21 02:43:14 google-dataproc-startup[948]: 0 upgraded, 0 newly installed, 2 to remove and 12 not upgraded.
<13>Mar 21 02:43:14 google-dataproc-startup[948]: After this operation, 282 MB disk space will be freed.
<13>Mar 21 02:43:15 google-dataproc-startup[948]: (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 167248 files and directories currently installed.)
<13>Mar 21 02:43:15 google-dataproc-startup[948]: Removing hive-hbase (3.1.2-1) ...
<13>Mar 21 02:43:15 google-dataproc-startup[948]: Removing hbase (2.2.6-1) ...
<13>Mar 21 02:43:15 google-dataproc-startup[948]: Processing triggers for man-db (2.8.5-2) ...
<13>Mar 21 02:43:16 google-dataproc-startup[948]: (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 166723 files and directories currently installed.)
<13>Mar 21 02:43:16 google-dataproc-startup[948]: Purging configuration files for hbase (2.2.6-1) ...
<13>Mar 21 02:43:16 google-dataproc-startup[948]: bash -c DEBIAN_FRONTEND=noninteractive     apt-get autoremove -y --purge hive-hbase hbase succeeded.
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + loginfo 'Starting to uninstall artifacts'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + echo 'Starting to uninstall artifacts'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: Starting to uninstall artifacts
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + start_uninstall_artifacts
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local blocking
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local init_action_count
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local blocking_default=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + is_version_at_least 2.0 2.0
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r version1=2.0
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r version2=2.0
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + loginfo 'Comparing if version 2.0 is at least version 2.0 '
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + echo 'Comparing if version 2.0 is at least version 2.0 '
<13>Mar 21 02:43:16 google-dataproc-startup[948]: Comparing if version 2.0 is at least version 2.0 
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + compare_versions 2.0 2.0
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r version1=2.0
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r version2=2.0
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + validate_version 2.0
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r version=2.0
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ 2.0 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + validate_version 2.0
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r version=2.0
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ 2.0 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ 2.0 == \2\.\0 ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + case $? in
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ get_dataproc_property_or_default dataproc.uninstall.packages.blocking ''
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ local -r property_name=dataproc.uninstall.packages.blocking
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ local -r default_value=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ local actual_value
<13>Mar 21 02:43:16 google-dataproc-startup[948]: +++ get_dataproc_property dataproc.uninstall.packages.blocking
<13>Mar 21 02:43:16 google-dataproc-startup[948]: +++ local -r property_name=dataproc.uninstall.packages.blocking
<13>Mar 21 02:43:16 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.uninstall.packages.blocking
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++++ local -r property_name=dataproc.uninstall.packages.blocking
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++++ local property_value
<13>Mar 21 02:43:16 google-dataproc-startup[948]: +++++ grep '^dataproc.uninstall.packages.blocking=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:16 google-dataproc-startup[948]: +++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: +++++ cut -d = -f 2-
<13>Mar 21 02:43:16 google-dataproc-startup[948]: +++++ tail -n 1
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++++ property_value=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++++ echo ''
<13>Mar 21 02:43:16 google-dataproc-startup[948]: +++ property_value=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: +++ echo ''
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ actual_value=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ [[ -n '' ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ echo ''
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + blocking=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ -z '' ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ /usr/share/google/get_metadata_value attributes/dataproc-initialization-script-count
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + init_action_count=1
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ 1 == \0 ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + blocking=true
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ true == \t\r\u\e ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + run_in_background --tag uninstall_artifacts uninstall_artifacts
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r pid=2147
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + run_with_logger --tag uninstall_artifacts uninstall_artifacts
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local tag=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local pid=2147
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + tag=uninstall_artifacts
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ 1 -eq 0 ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + uninstall_artifacts
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + echo 'Started background process [uninstall_artifacts] as pid 2147'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: Started background process [uninstall_artifacts] as pid 2147
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + cat
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ logger -s -t 'uninstall_artifacts[2147]'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ cat /usr/local/share/google/dataproc/bdutil/configure_multi_user_metadata_proxy.sh
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 uninstall_artifacts[2147]: + loginfo 'Uninstalling artifacts'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 uninstall_artifacts[2147]: + echo 'Uninstalling artifacts'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + bash configuration_multi_user_metadata_proxy.sh
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 uninstall_artifacts[2147]: Uninstalling artifacts
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 uninstall_artifacts[2147]: + uninstall_packages hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 uninstall_artifacts[2147]: + packages=("$@")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 uninstall_artifacts[2147]: + local packages
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 uninstall_artifacts[2147]: + mark_packages_to_uninstall hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 uninstall_artifacts[2147]: + for package in "$@"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 uninstall_artifacts[2147]: + touch /tmp/dataproc/uninstall/hadoop-hdfs-datanode
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 uninstall_artifacts[2147]: + for package in "$@"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 uninstall_artifacts[2147]: + touch /tmp/dataproc/uninstall/hadoop-hdfs-journalnode
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 uninstall_artifacts[2147]: + for package in "$@"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 uninstall_artifacts[2147]: + touch /tmp/dataproc/uninstall/hadoop-hdfs-zkfc
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + readonly MULTI_USER_METADATA_ADDRESS=169.254.169.254
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + MULTI_USER_METADATA_ADDRESS=169.254.169.254
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + readonly MULTI_USER_METADATA_PORT=80
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + MULTI_USER_METADATA_PORT=80
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + readonly MULTI_USER_METADATA_PASSTHROUGH_PORT=8081
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + MULTI_USER_METADATA_PASSTHROUGH_PORT=8081
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + readonly MULTI_USER_IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + MULTI_USER_IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + readonly METADATA_PROXY_PORT_BASE=801
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + METADATA_PROXY_PORT_BASE=801
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 uninstall_artifacts[2147]: + for package in "$@"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 uninstall_artifacts[2147]: + touch /tmp/dataproc/uninstall/hadoop-yarn-nodemanager
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 uninstall_artifacts[2147]: + local packages_to_uninstall
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 uninstall_artifacts[2147]: + packages_to_uninstall=($(list_packages_to_uninstall))
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 uninstall_artifacts[2147]: ++ list_packages_to_uninstall
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 uninstall_artifacts[2147]: ++ find /tmp/dataproc/uninstall/ -type f -exec basename '{}' ';'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.multi.user.metadata.proxy.enabled
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ local -r property_name=dataproc.multi.user.metadata.proxy.enabled
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:16 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.multi.user.metadata.proxy.enabled
<13>Mar 21 02:43:16 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:16 google-dataproc-startup[948]: +++ local -r property_name=dataproc.multi.user.metadata.proxy.enabled
<13>Mar 21 02:43:16 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++++ grep '^dataproc.multi.user.metadata.proxy.enabled=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:16 google-dataproc-startup[948]: +++ property_value=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: +++ echo ''
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ property_value=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ echo ''
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + readonly MULTI_USER_METADATA_PROXY_ENABLED=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + MULTI_USER_METADATA_PROXY_ENABLED=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.beta.secure.multi-tenancy.user.mapping
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ local -r property_name=dataproc.beta.secure.multi-tenancy.user.mapping
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:16 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.beta.secure.multi-tenancy.user.mapping
<13>Mar 21 02:43:16 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:16 google-dataproc-startup[948]: +++ local -r property_name=dataproc.beta.secure.multi-tenancy.user.mapping
<13>Mar 21 02:43:16 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++++ grep '^dataproc.beta.secure.multi-tenancy.user.mapping=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:16 google-dataproc-startup[948]: +++ property_value=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: +++ echo ''
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ property_value=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ echo ''
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + readonly DATAPROC_USER_MAPPINGS=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + DATAPROC_USER_MAPPINGS=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + multi_user_metadata_proxy_main
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ '' == \t\r\u\e ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + loginfo 'Starting services'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + echo 'Starting services'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: Starting services
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + start_services hadoop-hdfs-namenode hive-metastore hive-server2 solr-server hadoop-yarn-resourcemanager zookeeper-server hive-webhcat-server jupyter knox hadoop-mapreduce-historyserver mysql-server proxy-agent spark-history-server hadoop-yarn-timelineserver zeppelin hadoop-hdfs-secondarynamenode
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + services=("$@")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local services
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + for service in "${services[@]}"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + in_array hadoop-hdfs-namenode DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r value=hadoop-hdfs-namenode
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -n values=DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ !  docker-ce druid flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server jupyter kafka-server kerberos knox mapreduce miniconda3 mysql presto proxy-agent ranger solr-server spark yarn zeppelin zookeeper-server  == *\ \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e\ * ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + return 1
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + in_array hadoop-hdfs-namenode COMPONENT_SERVICES
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r value=hadoop-hdfs-namenode
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -n values=COMPONENT_SERVICES
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mariadb-server mysql-server spark-history-server  == *\ \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e\ * ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + continue
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + for service in "${services[@]}"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + in_array hive-metastore DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r value=hive-metastore
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -n values=DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ !  docker-ce druid flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server jupyter kafka-server kerberos knox mapreduce miniconda3 mysql presto proxy-agent ranger solr-server spark yarn zeppelin zookeeper-server  == *\ \h\i\v\e\-\m\e\t\a\s\t\o\r\e\ * ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + continue
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + for service in "${services[@]}"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + in_array hive-server2 DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r value=hive-server2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -n values=DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ !  docker-ce druid flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server jupyter kafka-server kerberos knox mapreduce miniconda3 mysql presto proxy-agent ranger solr-server spark yarn zeppelin zookeeper-server  == *\ \h\i\v\e\-\s\e\r\v\e\r\2\ * ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + continue
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + for service in "${services[@]}"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + in_array solr-server DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r value=solr-server
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -n values=DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ !  docker-ce druid flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server jupyter kafka-server kerberos knox mapreduce miniconda3 mysql presto proxy-agent ranger solr-server spark yarn zeppelin zookeeper-server  == *\ \s\o\l\r\-\s\e\r\v\e\r\ * ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + continue
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + for service in "${services[@]}"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + in_array hadoop-yarn-resourcemanager DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r value=hadoop-yarn-resourcemanager
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -n values=DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ !  docker-ce druid flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server jupyter kafka-server kerberos knox mapreduce miniconda3 mysql presto proxy-agent ranger solr-server spark yarn zeppelin zookeeper-server  == *\ \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r\ * ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + return 1
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + in_array hadoop-yarn-resourcemanager COMPONENT_SERVICES
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r value=hadoop-yarn-resourcemanager
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -n values=COMPONENT_SERVICES
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mariadb-server mysql-server spark-history-server  == *\ \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r\ * ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + return 1
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + run_in_background --tag setup-hadoop-yarn-resourcemanager setup_service hadoop-yarn-resourcemanager
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r pid=2183
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + echo 'Started background process [setup_service hadoop-yarn-resourcemanager] as pid 2183'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: Started background process [setup_service hadoop-yarn-resourcemanager] as pid 2183
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + for service in "${services[@]}"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + in_array zookeeper-server DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r value=zookeeper-server
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -n values=DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ !  docker-ce druid flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server jupyter kafka-server kerberos knox mapreduce miniconda3 mysql presto proxy-agent ranger solr-server spark yarn zeppelin zookeeper-server  == *\ \z\o\o\k\e\e\p\e\r\-\s\e\r\v\e\r\ * ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + continue
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + for service in "${services[@]}"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + in_array hive-webhcat-server DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r value=hive-webhcat-server
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -n values=DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ !  docker-ce druid flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server jupyter kafka-server kerberos knox mapreduce miniconda3 mysql presto proxy-agent ranger solr-server spark yarn zeppelin zookeeper-server  == *\ \h\i\v\e\-\w\e\b\h\c\a\t\-\s\e\r\v\e\r\ * ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + continue
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + for service in "${services[@]}"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + in_array jupyter DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r value=jupyter
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -n values=DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ !  docker-ce druid flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server jupyter kafka-server kerberos knox mapreduce miniconda3 mysql presto proxy-agent ranger solr-server spark yarn zeppelin zookeeper-server  == *\ \j\u\p\y\t\e\r\ * ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + continue
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + for service in "${services[@]}"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + in_array knox DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r value=knox
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -n values=DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ !  docker-ce druid flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server jupyter kafka-server kerberos knox mapreduce miniconda3 mysql presto proxy-agent ranger solr-server spark yarn zeppelin zookeeper-server  == *\ \k\n\o\x\ * ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + continue
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + for service in "${services[@]}"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + in_array hadoop-mapreduce-historyserver DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r value=hadoop-mapreduce-historyserver
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -n values=DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ !  docker-ce druid flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server jupyter kafka-server kerberos knox mapreduce miniconda3 mysql presto proxy-agent ranger solr-server spark yarn zeppelin zookeeper-server  == *\ \h\a\d\o\o\p\-\m\a\p\r\e\d\u\c\e\-\h\i\s\t\o\r\y\s\e\r\v\e\r\ * ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + return 1
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + in_array hadoop-mapreduce-historyserver COMPONENT_SERVICES
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r value=hadoop-mapreduce-historyserver
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -n values=COMPONENT_SERVICES
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mariadb-server mysql-server spark-history-server  == *\ \h\a\d\o\o\p\-\m\a\p\r\e\d\u\c\e\-\h\i\s\t\o\r\y\s\e\r\v\e\r\ * ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + return 1
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + run_in_background --tag setup-hadoop-mapreduce-historyserver setup_service hadoop-mapreduce-historyserver
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r pid=2184
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + echo 'Started background process [setup_service hadoop-mapreduce-historyserver] as pid 2184'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: Started background process [setup_service hadoop-mapreduce-historyserver] as pid 2184
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + for service in "${services[@]}"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + in_array mysql-server DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r value=mysql-server
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -n values=DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ !  docker-ce druid flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server jupyter kafka-server kerberos knox mapreduce miniconda3 mysql presto proxy-agent ranger solr-server spark yarn zeppelin zookeeper-server  == *\ \m\y\s\q\l\-\s\e\r\v\e\r\ * ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + return 1
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + in_array mysql-server COMPONENT_SERVICES
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r value=mysql-server
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -n values=COMPONENT_SERVICES
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mariadb-server mysql-server spark-history-server  == *\ \m\y\s\q\l\-\s\e\r\v\e\r\ * ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + continue
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + for service in "${services[@]}"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + in_array proxy-agent DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r value=proxy-agent
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -n values=DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ !  docker-ce druid flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server jupyter kafka-server kerberos knox mapreduce miniconda3 mysql presto proxy-agent ranger solr-server spark yarn zeppelin zookeeper-server  == *\ \p\r\o\x\y\-\a\g\e\n\t\ * ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + continue
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + for service in "${services[@]}"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + in_array spark-history-server DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r value=spark-history-server
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -n values=DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ !  docker-ce druid flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server jupyter kafka-server kerberos knox mapreduce miniconda3 mysql presto proxy-agent ranger solr-server spark yarn zeppelin zookeeper-server  == *\ \s\p\a\r\k\-\h\i\s\t\o\r\y\-\s\e\r\v\e\r\ * ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + return 1
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + in_array spark-history-server COMPONENT_SERVICES
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r value=spark-history-server
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -n values=COMPONENT_SERVICES
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mariadb-server mysql-server spark-history-server  == *\ \s\p\a\r\k\-\h\i\s\t\o\r\y\-\s\e\r\v\e\r\ * ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + continue
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + for service in "${services[@]}"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + in_array hadoop-yarn-timelineserver DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r value=hadoop-yarn-timelineserver
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -n values=DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ !  docker-ce druid flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server jupyter kafka-server kerberos knox mapreduce miniconda3 mysql presto proxy-agent ranger solr-server spark yarn zeppelin zookeeper-server  == *\ \h\a\d\o\o\p\-\y\a\r\n\-\t\i\m\e\l\i\n\e\s\e\r\v\e\r\ * ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + return 1
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + in_array hadoop-yarn-timelineserver COMPONENT_SERVICES
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r value=hadoop-yarn-timelineserver
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -n values=COMPONENT_SERVICES
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mariadb-server mysql-server spark-history-server  == *\ \h\a\d\o\o\p\-\y\a\r\n\-\t\i\m\e\l\i\n\e\s\e\r\v\e\r\ * ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + return 1
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + run_in_background --tag setup-hadoop-yarn-timelineserver setup_service hadoop-yarn-timelineserver
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r pid=2185
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + echo 'Started background process [setup_service hadoop-yarn-timelineserver] as pid 2185'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: Started background process [setup_service hadoop-yarn-timelineserver] as pid 2185
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + for service in "${services[@]}"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + in_array zeppelin DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r value=zeppelin
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -n values=DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ !  docker-ce druid flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server jupyter kafka-server kerberos knox mapreduce miniconda3 mysql presto proxy-agent ranger solr-server spark yarn zeppelin zookeeper-server  == *\ \z\e\p\p\e\l\i\n\ * ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + continue
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + for service in "${services[@]}"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + in_array hadoop-hdfs-secondarynamenode DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r value=hadoop-hdfs-secondarynamenode
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -n values=DATAPROC_COMPONENTS
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ !  docker-ce druid flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server jupyter kafka-server kerberos knox mapreduce miniconda3 mysql presto proxy-agent ranger solr-server spark yarn zeppelin zookeeper-server  == *\ \h\a\d\o\o\p\-\h\d\f\s\-\s\e\c\o\n\d\a\r\y\n\a\m\e\n\o\d\e\ * ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + return 1
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + in_array hadoop-hdfs-secondarynamenode COMPONENT_SERVICES
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r value=hadoop-hdfs-secondarynamenode
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -n values=COMPONENT_SERVICES
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mariadb-server mysql-server spark-history-server  == *\ \h\a\d\o\o\p\-\h\d\f\s\-\s\e\c\o\n\d\a\r\y\n\a\m\e\n\o\d\e\ * ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + continue
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + loginfo 'Activating components'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + echo 'Activating components'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: Activating components
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + activate_components hdfs hive-metastore hive-server2 mapreduce miniconda3 mysql spark yarn
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + components=("$@")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local components
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + mkdir -p /tmp/dataproc/components/activate
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + for component in "${components[@]}"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ hdfs != \k\e\r\b\e\r\o\s ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + loginfo 'Activating: hdfs'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + echo 'Activating: hdfs'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: Activating: hdfs
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + run_in_background --tag activate-component-hdfs activate_component hdfs
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r pid=2187
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + echo 'Started background process [activate_component hdfs] as pid 2187'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: Started background process [activate_component hdfs] as pid 2187
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + for component in "${components[@]}"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ hive-metastore != \k\e\r\b\e\r\o\s ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + loginfo 'Activating: hive-metastore'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + echo 'Activating: hive-metastore'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: Activating: hive-metastore
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + run_in_background --tag activate-component-hive-metastore activate_component hive-metastore
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r pid=2188
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + echo 'Started background process [activate_component hive-metastore] as pid 2188'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: Started background process [activate_component hive-metastore] as pid 2188
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + for component in "${components[@]}"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ hive-server2 != \k\e\r\b\e\r\o\s ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + loginfo 'Activating: hive-server2'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + echo 'Activating: hive-server2'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: Activating: hive-server2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + run_in_background --tag activate-component-hive-server2 activate_component hive-server2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r pid=2189
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + echo 'Started background process [activate_component hive-server2] as pid 2189'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: Started background process [activate_component hive-server2] as pid 2189
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + for component in "${components[@]}"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ mapreduce != \k\e\r\b\e\r\o\s ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + loginfo 'Activating: mapreduce'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + echo 'Activating: mapreduce'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: Activating: mapreduce
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + run_in_background --tag activate-component-mapreduce activate_component mapreduce
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r pid=2190
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + echo 'Started background process [activate_component mapreduce] as pid 2190'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: Started background process [activate_component mapreduce] as pid 2190
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + for component in "${components[@]}"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ miniconda3 != \k\e\r\b\e\r\o\s ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + loginfo 'Activating: miniconda3'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + echo 'Activating: miniconda3'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: Activating: miniconda3
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + run_in_background --tag activate-component-miniconda3 activate_component miniconda3
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r pid=2191
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + echo 'Started background process [activate_component miniconda3] as pid 2191'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: Started background process [activate_component miniconda3] as pid 2191
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + run_with_logger --tag setup-hadoop-yarn-timelineserver setup_service hadoop-yarn-timelineserver
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local tag=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local pid=2185
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + tag=setup-hadoop-yarn-timelineserver
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ 2 -eq 0 ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + setup_service hadoop-yarn-timelineserver
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + for component in "${components[@]}"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ mysql != \k\e\r\b\e\r\o\s ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + loginfo 'Activating: mysql'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + echo 'Activating: mysql'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: Activating: mysql
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + run_in_background --tag activate-component-mysql activate_component mysql
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r pid=2194
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + run_with_logger --tag setup-hadoop-mapreduce-historyserver setup_service hadoop-mapreduce-historyserver
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local tag=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local pid=2184
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + tag=setup-hadoop-mapreduce-historyserver
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ 2 -eq 0 ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + setup_service hadoop-mapreduce-historyserver
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + echo 'Started background process [activate_component mysql] as pid 2194'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: Started background process [activate_component mysql] as pid 2194
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + for component in "${components[@]}"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ spark != \k\e\r\b\e\r\o\s ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + loginfo 'Activating: spark'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + echo 'Activating: spark'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: Activating: spark
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + run_with_logger --tag activate-component-mapreduce activate_component mapreduce
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local tag=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local pid=2190
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + tag=activate-component-mapreduce
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ 2 -eq 0 ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ logger -s -t 'setup-hadoop-yarn-timelineserver[2185]'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + run_with_logger --tag activate-component-miniconda3 activate_component miniconda3
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local tag=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local pid=2191
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + tag=activate-component-miniconda3
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ 2 -eq 0 ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + activate_component miniconda3
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + run_with_logger --tag activate-component-hive-server2 activate_component hive-server2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local tag=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local pid=2189
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + tag=activate-component-hive-server2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ 2 -eq 0 ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + activate_component hive-server2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ logger -s -t 'activate-component-miniconda3[2191]'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + run_in_background --tag activate-component-spark activate_component spark
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r pid=2207
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + echo 'Started background process [activate_component spark] as pid 2207'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: Started background process [activate_component spark] as pid 2207
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + for component in "${components[@]}"
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ yarn != \k\e\r\b\e\r\o\s ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + loginfo 'Activating: yarn'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + echo 'Activating: yarn'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: Activating: yarn
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + run_in_background --tag activate-component-yarn activate_component yarn
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r pid=2208
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + echo 'Started background process [activate_component yarn] as pid 2208'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: Started background process [activate_component yarn] as pid 2208
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + is_service_installed google-osconfig-agent
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local -r service=google-osconfig-agent
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local output
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + activate_component mapreduce
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + run_with_logger --tag activate-component-hive-metastore activate_component hive-metastore
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local tag=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local pid=2188
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + tag=activate-component-hive-metastore
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ 2 -eq 0 ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + activate_component hive-metastore
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-miniconda3[2191]: + local -r component=miniconda3
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-miniconda3[2191]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-miniconda3[2191]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + run_with_logger --tag activate-component-mysql activate_component mysql
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + run_with_logger --tag activate-component-hdfs activate_component hdfs
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local tag=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ logger -s -t 'setup-hadoop-mapreduce-historyserver[2184]'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local tag=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local pid=2187
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local pid=2194
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + tag=activate-component-hdfs
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + tag=activate-component-mysql
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ 2 -eq 0 ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + run_with_logger --tag activate-component-spark activate_component spark
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local tag=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ 2 -eq 0 ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local pid=2207
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + tag=activate-component-spark
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ 2 -eq 0 ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + activate_component spark
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + activate_component mysql
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + activate_component hdfs
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ logger -s -t 'activate-component-mapreduce[2190]'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + run_with_logger --tag setup-hadoop-yarn-resourcemanager setup_service hadoop-yarn-resourcemanager
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ logger -s -t 'activate-component-hive-metastore[2188]'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-miniconda3[2191]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + run_with_logger --tag activate-component-yarn activate_component yarn
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-miniconda3[2191]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-miniconda3[2191]: + mkdir -p /tmp/dataproc/components/activate
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local tag=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local pid=2208
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local tag=
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + tag=activate-component-yarn
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ logger -s -t 'activate-component-hive-server2[2189]'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ 2 -eq 0 ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + activate_component yarn
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local status
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + local pid=2183
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + (( i = 0 ))
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + (( i < 10 ))
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + tag=setup-hadoop-yarn-resourcemanager
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + [[ 2 -eq 0 ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: + setup_service hadoop-yarn-resourcemanager
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-yarn-timelineserver[2185]: + export -f login_through_keytab_if_necessary
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ logger -s -t 'activate-component-mysql[2194]'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ logger -s -t 'activate-component-spark[2207]'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-miniconda3[2191]: + touch /tmp/dataproc/components/activate/miniconda3.running
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-mapreduce[2190]: + local -r component=mapreduce
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ logger -s -t 'activate-component-hdfs[2187]'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-hive-metastore[2188]: + local -r component=hive-metastore
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-yarn-timelineserver[2185]: + local -r service=hadoop-yarn-timelineserver
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-mapreduce-historyserver[2184]: + export -f login_through_keytab_if_necessary
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-yarn-timelineserver[2185]: + enable_service hadoop-yarn-timelineserver
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-yarn-timelineserver[2185]: + local -r service=hadoop-yarn-timelineserver
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-yarn-timelineserver[2185]: + local -r unit=hadoop-yarn-timelineserver.service
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-yarn-timelineserver[2185]: + retry_constant_short systemctl enable hadoop-yarn-timelineserver.service
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-yarn-timelineserver[2185]: + retry_constant_custom 30 1 systemctl enable hadoop-yarn-timelineserver.service
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-yarn-timelineserver[2185]: + local -r max_retry_time=30
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-yarn-timelineserver[2185]: + local -r retry_delay=1
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ logger -s -t 'activate-component-yarn[2208]'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-yarn-timelineserver[2185]: + cmd=("${@:3}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-yarn-timelineserver[2185]: + local -r cmd
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-yarn-timelineserver[2185]: + local -r max_retries=30
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-yarn-timelineserver[2185]: + set +x
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-yarn-timelineserver[2185]: About to run 'systemctl enable hadoop-yarn-timelineserver.service' with retries...
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-yarn-timelineserver[2185]: hadoop-yarn-timelineserver.service is not a native service, redirecting to systemd-sysv-install.
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-yarn-timelineserver[2185]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-yarn-timelineserver
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ systemctl cat google-osconfig-agent.service
<13>Mar 21 02:43:16 google-dataproc-startup[948]: ++ logger -s -t 'setup-hadoop-yarn-resourcemanager[2183]'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-miniconda3[2191]: + local exit_code=0
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-miniconda3[2191]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-yarn-resourcemanager[2183]: + export -f login_through_keytab_if_necessary
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-hive-server2[2189]: + local -r component=hive-server2
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-hive-server2[2189]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-hive-server2[2189]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-hive-server2[2189]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-hive-server2[2189]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-hive-server2[2189]: + mkdir -p /tmp/dataproc/components/activate
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-hive-server2[2189]: + touch /tmp/dataproc/components/activate/hive-server2.running
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-hive-server2[2189]: + local exit_code=0
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-hive-server2[2189]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-mapreduce-historyserver[2184]: + local -r service=hadoop-mapreduce-historyserver
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-mapreduce-historyserver[2184]: + enable_service hadoop-mapreduce-historyserver
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-mapreduce-historyserver[2184]: + local -r service=hadoop-mapreduce-historyserver
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-mapreduce-historyserver[2184]: + local -r unit=hadoop-mapreduce-historyserver.service
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-mapreduce-historyserver[2184]: + retry_constant_short systemctl enable hadoop-mapreduce-historyserver.service
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-mapreduce-historyserver[2184]: + retry_constant_custom 30 1 systemctl enable hadoop-mapreduce-historyserver.service
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-mapreduce-historyserver[2184]: + local -r max_retry_time=30
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-mapreduce-historyserver[2184]: + local -r retry_delay=1
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-mapreduce-historyserver[2184]: + cmd=("${@:3}")
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-mapreduce-historyserver[2184]: + local -r cmd
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-mapreduce[2190]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-spark[2207]: + local -r component=spark
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-mapreduce[2190]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-mapreduce-historyserver[2184]: + local -r max_retries=30
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-mapreduce-historyserver[2184]: + set +x
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-hive-metastore[2188]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-mapreduce[2190]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-mapreduce-historyserver[2184]: About to run 'systemctl enable hadoop-mapreduce-historyserver.service' with retries...
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-hive-metastore[2188]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh ]]
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-mapreduce[2190]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-mapreduce-historyserver[2184]: hadoop-mapreduce-historyserver.service is not a native service, redirecting to systemd-sysv-install.
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-hive-metastore[2188]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh'
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-mapreduce[2190]: + mkdir -p /tmp/dataproc/components/activate
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 setup-hadoop-mapreduce-historyserver[2184]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-mapreduce-historyserver
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-hive-metastore[2188]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-mapreduce[2190]: + touch /tmp/dataproc/components/activate/mapreduce.running
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-hive-metastore[2188]: + mkdir -p /tmp/dataproc/components/activate
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-mapreduce[2190]: + local exit_code=0
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-hive-metastore[2188]: + touch /tmp/dataproc/components/activate/hive-metastore.running
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-mapreduce[2190]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh
<13>Mar 21 02:43:16 google-dataproc-startup[948]: <13>Mar 21 02:43:16 activate-component-yarn[2208]: + local -r component=yarn
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + output='# /lib/systemd/system/google-osconfig-agent.service
<13>Mar 21 02:43:17 google-dataproc-startup[948]: [Unit]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: Description=Google OSConfig Agent
<13>Mar 21 02:43:17 google-dataproc-startup[948]: After=local-fs.target network-online.target
<13>Mar 21 02:43:17 google-dataproc-startup[948]: Wants=local-fs.target network-online.target
<13>Mar 21 02:43:17 google-dataproc-startup[948]: 
<13>Mar 21 02:43:17 google-dataproc-startup[948]: [Service]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ExecStart=/usr/bin/google_osconfig_agent
<13>Mar 21 02:43:17 google-dataproc-startup[948]: Restart=always
<13>Mar 21 02:43:17 google-dataproc-startup[948]: RestartSec=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: StartLimitInterval=120
<13>Mar 21 02:43:17 google-dataproc-startup[948]: StartLimitBurst=3
<13>Mar 21 02:43:17 google-dataproc-startup[948]: KillMode=mixed
<13>Mar 21 02:43:17 google-dataproc-startup[948]: KillSignal=SIGTERM
<13>Mar 21 02:43:17 google-dataproc-startup[948]: 
<13>Mar 21 02:43:17 google-dataproc-startup[948]: [Install]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: WantedBy=multi-user.target
<13>Mar 21 02:43:17 google-dataproc-startup[948]: 
<13>Mar 21 02:43:17 google-dataproc-startup[948]: # /etc/systemd/system/google-osconfig-agent.service.d/dataproc.conf
<13>Mar 21 02:43:17 google-dataproc-startup[948]: [Unit]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ConditionPathExists=!/etc/google-dataproc/hermetic_vm'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + status=0
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + [[ 0 -eq 0 ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + local exit_code=0
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + return 0
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-hadoop-yarn-resourcemanager[2183]: + local -r service=hadoop-yarn-resourcemanager
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + is_hermetic_vm
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-hadoop-yarn-resourcemanager[2183]: + enable_service hadoop-yarn-resourcemanager
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-hadoop-yarn-resourcemanager[2183]: + local -r service=hadoop-yarn-resourcemanager
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-hadoop-yarn-resourcemanager[2183]: + local -r unit=hadoop-yarn-resourcemanager.service
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-hadoop-yarn-resourcemanager[2183]: + retry_constant_short systemctl enable hadoop-yarn-resourcemanager.service
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-hadoop-yarn-resourcemanager[2183]: + retry_constant_custom 30 1 systemctl enable hadoop-yarn-resourcemanager.service
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-hadoop-yarn-resourcemanager[2183]: + local -r max_retry_time=30
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-hadoop-yarn-resourcemanager[2183]: + local -r retry_delay=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-hadoop-yarn-resourcemanager[2183]: + cmd=("${@:3}")
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-hadoop-yarn-resourcemanager[2183]: + local -r cmd
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-hadoop-yarn-resourcemanager[2183]: + local -r max_retries=30
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-hadoop-yarn-resourcemanager[2183]: + set +x
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-hadoop-yarn-resourcemanager[2183]: About to run 'systemctl enable hadoop-yarn-resourcemanager.service' with retries...
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + local -r sentinel_file=/etc/google-dataproc/hermetic_vm
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + [[ -f /etc/google-dataproc/hermetic_vm ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + local hermetic_vm
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + local -r component=mysql
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-hadoop-yarn-resourcemanager[2183]: hadoop-yarn-resourcemanager.service is not a native service, redirecting to systemd-sysv-install.
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mapreduce[2190]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mapreduce[2190]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mapreduce[2190]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mapreduce[2190]: + touch /tmp/dataproc/components/activate/mapreduce.done
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + mkdir -p /tmp/dataproc/components/activate
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + touch /tmp/dataproc/components/activate/mysql.running
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + local exit_code=0
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + set -euxo pipefail
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: + mkdir -p /tmp/dataproc/components/activate
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: + touch /tmp/dataproc/components/activate/spark.running
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: + local exit_code=0
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ /usr/share/google/get_metadata_value attributes/hermetic-vm
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-yarn[2208]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-yarn[2208]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-yarn[2208]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-yarn[2208]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-yarn[2208]: + mkdir -p /tmp/dataproc/components/activate
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-yarn[2208]: + touch /tmp/dataproc/components/activate/yarn.running
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-yarn[2208]: + local exit_code=0
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-yarn[2208]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-yarn[2208]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-yarn[2208]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: + set -euxo pipefail
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-yarn[2208]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-yarn[2208]: + touch /tmp/dataproc/components/activate/yarn.done
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + local -r component=hdfs
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + set -euxo pipefail
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + mkdir -p /tmp/dataproc/components/activate
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + touch /tmp/dataproc/components/activate/hdfs.running
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + local exit_code=0
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + set -euxo pipefail
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + source /etc/environment
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-hadoop-yarn-resourcemanager[2183]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-yarn-resourcemanager
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ JAVA_CONFIG_DIR=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/jre/lib
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ BDUTIL_DIR=/usr/local/share/google/dataproc/bdutil
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ OS_BDUTIL_DIR=/usr/local/share/google/dataproc/bdutil/os/debian
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_env.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ ENABLE_HDFS=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ NUM_WORKERS=10
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ WORKERS=()
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ DATAPROC_AGENT_JAR=/usr/local/share/google/dataproc/dataproc-agent.jar
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ DATAPROC_STARTUP_SCRIPT=/usr/local/share/google/dataproc/startup-script.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ DATAPROC_POST_HDFS_STARTUP_SCRIPT=/usr/local/share/google/dataproc/post-hdfs-startup-script.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ DATAPROC_COMMON_PACKAGES='bigtop-utils hadoop-client hadoop-lzo pig tez autofs bash-completion bc git vim wget google-fluentd stackdriver-agent docker-ce druid flink hbase hdfs libhdfs0 hive-metastore hive-server2 hive-hcatalog texlive-xetex texlive-fonts-recommended texlive-plain-generic kafka-server kerberos mapreduce miniconda3 mysql mysql-connector-java presto ranger spark yarn libjansi-java python-numpy libsnappy-dev libzstd-dev libatlas3-base libopenblas-base libapr1 spark-extras python-setuptools linux-headers-4.19.0-14-cloud-amd64 uuid-runtime libssl-dev openssl python-pip python-requests linux-image-amd64 linux-headers-amd64 adoptopenjdk-8-hotspot'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ DATAPROC_MASTER_SERVICES='hadoop-hdfs-namenode hive-metastore hive-server2 solr-server hadoop-yarn-resourcemanager zookeeper-server'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ DATAPROC_MASTER_EXCLUSIVE_SERVICES='hive-webhcat-server jupyter knox hadoop-mapreduce-historyserver mysql-server proxy-agent spark-history-server hadoop-yarn-timelineserver zeppelin'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ DATAPROC_MASTER_STANDALONE_SERVICES=hadoop-hdfs-secondarynamenode
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ DATAPROC_MASTER_HA_SERVICES='hadoop-hdfs-journalnode hadoop-hdfs-zkfc'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ TWO_MASTER_HA_SERVICES='hadoop-hdfs-namenode hadoop-hdfs-zkfc'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ DATAPROC_WORKER_SERVICES='hadoop-hdfs-datanode hadoop-yarn-nodemanager'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ DATAPROC_IMAGE_VERSION=20210311-093551-RC01-2_0_deb10_20210311_015200-RC01
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ DATAPROC_VERSION=2.0
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ DATAPROC_OPTIONAL_COMPONENTS='docker-ce druid flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server jupyter kafka-server kerberos knox mapreduce miniconda3 mysql presto proxy-agent ranger solr-server spark yarn zeppelin zookeeper-server'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ DATAPROC_NON_DEBIAN_COMPONENTS='docker-ce druid flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server jupyter kafka-server kerberos knox mapreduce miniconda3 mysql presto proxy-agent ranger solr-server spark yarn zeppelin zookeeper-server'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ DATAPROC_START_AFTER_HDFS_SERVICES=hadoop-mapreduce-historyserver
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ DATAPROC_GENERATED_KEYS_DIR=/mnt/cluster-keys-ram-disk
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ SPARK_HOME=/usr/lib/spark
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: + set -euxo pipefail
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + set -euxo pipefail
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_env.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ ENABLE_HDFS=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ NUM_WORKERS=10
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ WORKERS=()
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_helpers.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ is_centos
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_env.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ ENABLE_HDFS=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ NUM_WORKERS=10
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ WORKERS=()
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../shared/miniconda3.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ export MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ export MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ export PATH=/opt/conda/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ PATH=/opt/conda/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_env.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ ENABLE_HDFS=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ NUM_WORKERS=10
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ WORKERS=()
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_helpers.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ is_centos
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_env.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ ENABLE_HDFS=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ NUM_WORKERS=10
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ WORKERS=()
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: +++ . /etc/os-release
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++++ VERSION_ID=10
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++++ VERSION='10 (buster)'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++++ VERSION_CODENAME=buster
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++++ ID=debian
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: +++ echo debian
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_helpers.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ is_centos
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ return 1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: +++ . /etc/os-release
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++++ VERSION_ID=10
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++++ VERSION='10 (buster)'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++++ VERSION_CODENAME=buster
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++++ ID=debian
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: +++ echo debian
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ return 1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_helpers.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ is_centos
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: +++ . /etc/os-release
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++++ VERSION_ID=10
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++++ VERSION='10 (buster)'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++++ VERSION_CODENAME=buster
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++++ ID=debian
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: +++ echo debian
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ return 1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: +++ . /etc/os-release
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++++ VERSION_ID=10
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++++ VERSION='10 (buster)'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++++ VERSION_CODENAME=buster
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++++ ID=debian
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: +++ echo debian
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ return 1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_helpers.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ is_centos
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_helpers.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ is_centos
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + activate_mysql
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: +++ . /etc/os-release
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++++ VERSION_ID=10
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++++ VERSION='10 (buster)'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++++ VERSION_CODENAME=buster
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++++ ID=debian
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: +++ . /etc/os-release
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++++ VERSION_ID=10
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++++ VERSION='10 (buster)'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++++ VERSION_CODENAME=buster
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++++ ID=debian
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: +++ echo debian
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ return 1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../cluster_properties.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + [[ student-01-71e17b39d43c-qwiklab-m != \s\t\u\d\e\n\t\-\0\1\-\7\1\e\1\7\b\3\9\d\4\3\c\-\q\w\i\k\l\a\b\-\m ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + local service_name
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + is_centos
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: +++ echo debian
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ return 1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../cluster_properties.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ . /etc/os-release
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: +++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: +++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: +++ VERSION_ID=10
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: +++ VERSION='10 (buster)'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: +++ VERSION_CODENAME=buster
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: +++ ID=debian
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: +++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: +++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: +++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ echo debian
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + return 1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + is_ubuntu
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../shared/hive.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ . /etc/os-release
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ HIVE_CONF_DIR=/etc/hive/conf
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: + activate_hive_server2
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: + [[ Master == \M\a\s\t\e\r ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: + configure_hive_hbase
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: + is_component_selected hbase
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: + local -r component=hbase
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: + local activated_components
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../shared/effective-python.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ export EFFECTIVE_PYTHON_PROFILE=/etc/profile.d/effective-python.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ EFFECTIVE_PYTHON_PROFILE=/etc/profile.d/effective-python.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ export DEFAULT_CONDA_INSTALL_PATH=/opt/conda/default
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ DEFAULT_CONDA_INSTALL_PATH=/opt/conda/default
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + install_effective_python_profile
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + [[ ! -f /etc/profile.d/effective-python.sh ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + generate_effective_python_profile
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + local activated_components
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + activated_components=($(get_dataproc_property dataproc.components.activate))
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: +++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: +++ NAME='Debian GNU/Linux'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: +++ VERSION_ID=10
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: +++ VERSION='10 (buster)'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: +++ VERSION_CODENAME=buster
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: +++ ID=debian
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: +++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: +++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: +++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: ++ echo debian
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + [[ debian == \u\b\u\n\t\u ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + return 1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + service_name=mysql
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + enable_service mysql
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + local -r service=mysql
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + local -r unit=mysql.service
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + retry_constant_short systemctl enable mysql.service
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + retry_constant_custom 30 1 systemctl enable mysql.service
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + local -r max_retry_time=30
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + local -r retry_delay=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + cmd=("${@:3}")
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + local -r cmd
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + local -r max_retries=30
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: + set +x
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: About to run 'systemctl enable mysql.service' with retries...
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ get_components_to_activate
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ get_dataproc_property dataproc.components.activate
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: Synchronizing state of mysql.service with SysV service script with /lib/systemd/systemd-sysv-install.
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ tr '[:upper:]' '[:lower:]'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ get_dataproc_property dataproc.components.activate
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ local -r property_name=dataproc.components.activate
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ local property_value
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-mysql[2194]: Executing: /lib/systemd/systemd-sysv-install enable mysql
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ local -r property_name=dataproc.components.activate
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ local property_value
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: +++ local -r property_name=dataproc.components.activate
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: +++ local property_value
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++++ tail -n 1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../shared/hive.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ HIVE_CONF_DIR=/etc/hive/conf
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + HIVE_CONF_DIR=/etc/hive/conf
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ get_metadata_role
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ local -r var_name=DATAPROC_METADATA_ROLE
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ local -r attr_name=attributes/dataproc-role
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ [[ -v DATAPROC_METADATA_ROLE ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ /usr/share/google/get_metadata_value attributes/dataproc-role
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: +++ property_value='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: +++ echo 'hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ property_value='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: ++ echo 'hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: + activated_components='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: + [[ hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3 == *hbase* ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: + [[ true == \t\r\u\e ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: + echo 'Delaying starting Hive server until HDFS is ready'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: Delaying starting Hive server until HDFS is ready
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-server2[2189]: + touch /tmp/dataproc/components/activate/hive-server2.done
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 uninstall_artifacts[2147]: + os_uninstall_packages hadoop-hdfs-journalnode docker-ce ranger hadoop-yarn-nodemanager krb5-config hadoop-hdfs-zkfc druid zeppelin flink presto zookeeper-server krb5-kdc knox hive-webhcat-server krb5-kpropd kafka-server solr-server krb5-admin-server xinetd hadoop-hdfs-datanode krb5-user
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 uninstall_artifacts[2147]: + packages_to_uninstall=("$@")
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 uninstall_artifacts[2147]: + local -r packages_to_uninstall
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 uninstall_artifacts[2147]: + loginfo 'Uninstalling packages: hadoop-hdfs-journalnode docker-ce ranger hadoop-yarn-nodemanager krb5-config hadoop-hdfs-zkfc druid zeppelin flink presto zookeeper-server krb5-kdc knox hive-webhcat-server krb5-kpropd kafka-server solr-server krb5-admin-server xinetd hadoop-hdfs-datanode krb5-user'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 uninstall_artifacts[2147]: + echo 'Uninstalling packages: hadoop-hdfs-journalnode docker-ce ranger hadoop-yarn-nodemanager krb5-config hadoop-hdfs-zkfc druid zeppelin flink presto zookeeper-server krb5-kdc knox hive-webhcat-server krb5-kpropd kafka-server solr-server krb5-admin-server xinetd hadoop-hdfs-datanode krb5-user'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 uninstall_artifacts[2147]: Uninstalling packages: hadoop-hdfs-journalnode docker-ce ranger hadoop-yarn-nodemanager krb5-config hadoop-hdfs-zkfc druid zeppelin flink presto zookeeper-server krb5-kdc knox hive-webhcat-server krb5-kpropd kafka-server solr-server krb5-admin-server xinetd hadoop-hdfs-datanode krb5-user
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 uninstall_artifacts[2147]: + local -r 'uninstall_cmd=DEBIAN_FRONTEND=noninteractive     apt-get autoremove -y --purge hadoop-hdfs-journalnode docker-ce ranger hadoop-yarn-nodemanager krb5-config hadoop-hdfs-zkfc druid zeppelin flink presto zookeeper-server krb5-kdc knox hive-webhcat-server krb5-kpropd kafka-server solr-server krb5-admin-server xinetd hadoop-hdfs-datanode krb5-user'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 uninstall_artifacts[2147]: + retry_constant bash -c 'DEBIAN_FRONTEND=noninteractive     apt-get autoremove -y --purge hadoop-hdfs-journalnode docker-ce ranger hadoop-yarn-nodemanager krb5-config hadoop-hdfs-zkfc druid zeppelin flink presto zookeeper-server krb5-kdc knox hive-webhcat-server krb5-kpropd kafka-server solr-server krb5-admin-server xinetd hadoop-hdfs-datanode krb5-user'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 uninstall_artifacts[2147]: + retry_constant_custom 300 1 bash -c 'DEBIAN_FRONTEND=noninteractive     apt-get autoremove -y --purge hadoop-hdfs-journalnode docker-ce ranger hadoop-yarn-nodemanager krb5-config hadoop-hdfs-zkfc druid zeppelin flink presto zookeeper-server krb5-kdc knox hive-webhcat-server krb5-kpropd kafka-server solr-server krb5-admin-server xinetd hadoop-hdfs-datanode krb5-user'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 uninstall_artifacts[2147]: + local -r max_retry_time=300
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 uninstall_artifacts[2147]: + local -r retry_delay=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 uninstall_artifacts[2147]: + cmd=("${@:3}")
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 uninstall_artifacts[2147]: + local -r cmd
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 uninstall_artifacts[2147]: + local -r max_retries=300
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 uninstall_artifacts[2147]: + set +x
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 uninstall_artifacts[2147]: About to run 'bash -c DEBIAN_FRONTEND=noninteractive     apt-get autoremove -y --purge hadoop-hdfs-journalnode docker-ce ranger hadoop-yarn-nodemanager krb5-config hadoop-hdfs-zkfc druid zeppelin flink presto zookeeper-server krb5-kdc knox hive-webhcat-server krb5-kpropd kafka-server solr-server krb5-admin-server xinetd hadoop-hdfs-datanode krb5-user' with retries...
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: +++ local -r property_name=dataproc.components.activate
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: +++ local property_value
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../shared/spark.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../shared/hdfs.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + MASTER_HOSTNAMES=(${DATAPROC_MASTER} ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + export -f login_through_keytab_if_necessary
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + activate_hdfs
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + mkdir -p /var/run/hadoop-hdfs
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: + activate_spark
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: + [[ Master == \M\a\s\t\e\r ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: + [[ true == \t\r\u\e ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: + echo 'Delaying starting Spark history server until HDFS is ready'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: Delaying starting Spark history server until HDFS is ready
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-spark[2207]: + touch /tmp/dataproc/components/activate/spark.done
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + sudo -u hdfs hdfs namenode -genclusterid
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++++ tail -n 1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + chown root:hdfs /var/run/hadoop-hdfs
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: +++ property_value='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: +++ echo 'hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ property_value='hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ echo 'hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + in_array anaconda activated_components
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + local -r value=anaconda
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + local -n values=activated_components
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + [[ !  hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3  == *\ \a\n\a\c\o\n\d\a\ * ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + return 1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + in_array miniconda3 activated_components
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + local -r value=miniconda3
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + local -n values=activated_components
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + [[ !  hdfs yarn mapreduce mysql hive-metastore hive-server2 spark miniconda3  == *\ \m\i\n\i\c\o\n\d\a\3\ * ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + chmod 775 /var/run/hadoop-hdfs
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + [[ Master == \M\a\s\t\e\r ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + start_master_services
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + [[ 1 == \1 ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + start_hdfs_namenode
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + case "${MASTER_INDEX?}" in
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + loginfo 'Formatting NameNode'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + echo 'Formatting NameNode'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: Formatting NameNode
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + retry_constant_short su -s /bin/bash hdfs -c 'source /etc/default/hadoop-hdfs-namenode &&           login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/student-01-71e17b39d43c-qwiklab-m.us-central1-f.c.qwiklabs-gcp-01-d65f02c71c6a.internal &&           hdfs namenode -format -nonInteractive'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + retry_constant_custom 30 1 su -s /bin/bash hdfs -c 'source /etc/default/hadoop-hdfs-namenode &&           login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/student-01-71e17b39d43c-qwiklab-m.us-central1-f.c.qwiklabs-gcp-01-d65f02c71c6a.internal &&           hdfs namenode -format -nonInteractive'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + local -r max_retry_time=30
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + local -r retry_delay=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + cmd=("${@:3}")
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + local -r cmd
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + local -r max_retries=30
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: + set +x
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hdfs[2187]: About to run 'su -s /bin/bash hdfs -c source /etc/default/hadoop-hdfs-namenode &&           login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/student-01-71e17b39d43c-qwiklab-m.us-central1-f.c.qwiklabs-gcp-01-d65f02c71c6a.internal &&           hdfs namenode -format -nonInteractive' with retries...
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../shared/miniconda3.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ export MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ export MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ export PATH=/opt/conda/miniconda3/bin:/opt/conda/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ PATH=/opt/conda/miniconda3/bin:/opt/conda/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + ln -f -s /opt/conda/miniconda3 /opt/conda/default
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ echo ''
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + hermetic_vm=
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + [[ '' == \t\r\u\e ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + return 1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + loginfo 'Starting service google-osconfig-agent'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + echo 'Starting service google-osconfig-agent'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: Starting service google-osconfig-agent
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + run_in_background --tag start-osconfig-service enable_and_start_service google-osconfig-agent
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + local -r pid=2363
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + echo 'Started background process [enable_and_start_service google-osconfig-agent] as pid 2363'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: Started background process [enable_and_start_service google-osconfig-agent] as pid 2363
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.conscrypt.provider.enable
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + run_with_logger --tag start-osconfig-service enable_and_start_service google-osconfig-agent
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + local tag=
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + local pid=2363
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + tag=start-osconfig-service
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + [[ 2 -eq 0 ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + enable_and_start_service google-osconfig-agent
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ local -r property_name=dataproc.conscrypt.provider.enable
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + emit_conda_profile /opt/conda/default/bin
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + local -r python_bin=/opt/conda/default/bin
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + local temp
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: ++ mktemp
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ logger -s -t 'start-osconfig-service[2363]'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + temp=/tmp/tmp.46BRRqCrro
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + ln -f -s /opt/conda/default/etc/profile.d/conda.sh /etc/profile.d/conda.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.conscrypt.provider.enable
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++ local -r property_name=dataproc.conscrypt.provider.enable
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + chmod +x /opt/conda/default/etc/profile.d/conda.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 start-osconfig-service[2363]: + local -r service=google-osconfig-agent
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 start-osconfig-service[2363]: + enable_service google-osconfig-agent
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 start-osconfig-service[2363]: + local -r service=google-osconfig-agent
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 start-osconfig-service[2363]: + local -r unit=google-osconfig-agent.service
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 start-osconfig-service[2363]: + retry_constant_short systemctl enable google-osconfig-agent.service
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 start-osconfig-service[2363]: + retry_constant_custom 30 1 systemctl enable google-osconfig-agent.service
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 start-osconfig-service[2363]: + local -r max_retry_time=30
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 start-osconfig-service[2363]: + local -r retry_delay=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 start-osconfig-service[2363]: + cmd=("${@:3}")
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 start-osconfig-service[2363]: + local -r cmd
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 start-osconfig-service[2363]: + local -r max_retries=30
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 start-osconfig-service[2363]: + set +x
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 start-osconfig-service[2363]: About to run 'systemctl enable google-osconfig-agent.service' with retries...
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++++ grep '^dataproc.conscrypt.provider.enable=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + cat
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++ property_value=true
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++ echo true
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ property_value=true
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ echo true
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + CONSCRYPT_ENABLED=true
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + [[ true == \t\r\u\e ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + ROLE=Master
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ compgen -G '/usr/local/share/google/dataproc/conscrypt/conscrypt-openjdk-*.jar'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ get_metadata_master
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ local -r var_name=DATAPROC_METADATA_MASTER
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + CONSCRYPT_JAR=/usr/local/share/google/dataproc/conscrypt/conscrypt-openjdk-2.5.1-linux-x86_64.jar
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + ln -s /usr/local/share/google/dataproc/conscrypt/conscrypt-openjdk-2.5.1-linux-x86_64.jar /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/jre/lib/ext/conscrypt-openjdk.jar
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ local -r attr_name=attributes/dataproc-master
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ [[ -v DATAPROC_METADATA_MASTER ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: ++ /usr/share/google/get_metadata_value attributes/dataproc-master
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + mv -n -v /tmp/tmp.46BRRqCrro /etc/profile.d/effective-python.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: renamed '/tmp/tmp.46BRRqCrro' -> '/etc/profile.d/effective-python.sh'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + chmod a+r /etc/profile.d/effective-python.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + ln -s /usr/local/share/google/dataproc/conscrypt/libconscrypt_openjdk_jni-linux-x86_64.so /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/jre/lib/amd64/libconscrypt_openjdk_jni.so
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + cp /usr/local/share/google/dataproc/java.security.conscrypt /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/jre/lib/security/java.security
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + rm -Rf /tmp/tmp.46BRRqCrro
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-miniconda3[2191]: + touch /tmp/dataproc/components/activate/miniconda3.done
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ get_dataproc_property dataproc.logging.stackdriver.enable
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ local -r property_name=dataproc.logging.stackdriver.enable
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ local property_value
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.logging.stackdriver.enable
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++ local -r property_name=dataproc.logging.stackdriver.enable
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++++ tail -n 1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++++ grep '^dataproc.logging.stackdriver.enable=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++ property_value=
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++ echo ''
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ property_value=
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ echo ''
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + STACKDRIVER_LOGGING_ENABLED=
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + [[ '' == \f\a\l\s\e ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + loginfo 'Stackdriver enabled; enabling google-fluentd.'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + echo 'Stackdriver enabled; enabling google-fluentd.'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: Stackdriver enabled; enabling google-fluentd.
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 uninstall_artifacts[2147]: Reading package lists...
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + source /usr/local/share/google/dataproc/bdutil/configure_fluentd.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ set -e
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ set -u
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ loginfo 'Running configure_fluentd.sh'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ echo 'Running configure_fluentd.sh'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: Running configure_fluentd.sh
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ DATAPROC_ETC_DIR=/etc/google-dataproc
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ FLUENTD_BASE_DIR=/etc/google-fluentd
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ FLUENTD_CONF_DIR=/etc/google-fluentd/config.d
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ FLUENTD_PLUGIN_DIR=/etc/google-fluentd/plugin
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + DATAPROC_MASTER=student-01-71e17b39d43c-qwiklab-m
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + activate_hive_metastore
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + [[ Master == \M\a\s\t\e\r ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + wait_for_mysql
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + wait_for_port mysql student-01-71e17b39d43c-qwiklab-m 3306
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + local -r name=mysql
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + local -r host=student-01-71e17b39d43c-qwiklab-m
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + local -r port=3306
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + local -r timeout=300
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + local -r capped_timeout=300
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + loginfo 'Waiting 300 seconds for service to come up on host=student-01-71e17b39d43c-qwiklab-m port=3306 name=mysql.'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + echo 'Waiting 300 seconds for service to come up on host=student-01-71e17b39d43c-qwiklab-m port=3306 name=mysql.'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: Waiting 300 seconds for service to come up on host=student-01-71e17b39d43c-qwiklab-m port=3306 name=mysql.
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + retry_constant_custom 300 1 nc -v -z -w 1 student-01-71e17b39d43c-qwiklab-m 3306
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + local -r max_retry_time=300
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + local -r retry_delay=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + cmd=("${@:3}")
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + local -r cmd
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + local -r max_retries=300
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: + set +x
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: About to run 'nc -v -z -w 1 student-01-71e17b39d43c-qwiklab-m 3306' with retries...
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ mv /usr/local/share/google/dataproc/bdutil/fluentd/job_logging/plugin/filter_add_insert_ids.rb /usr/local/share/google/dataproc/bdutil/fluentd/job_logging/plugin/in_object_space_dump.rb /usr/local/share/google/dataproc/bdutil/fluentd/job_logging/plugin/monitoring.rb /usr/local/share/google/dataproc/bdutil/fluentd/job_logging/plugin/out_google_cloud.rb /usr/local/share/google/dataproc/bdutil/fluentd/job_logging/plugin/statusz.rb /etc/google-fluentd/plugin
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++ get_dataproc_property dataproc.logging.stackdriver.job.driver.enable
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++ local -r property_name=dataproc.logging.stackdriver.job.driver.enable
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 3306 (tcp) failed: Connection refused
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.logging.stackdriver.job.driver.enable
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++++ local -r property_name=dataproc.logging.stackdriver.job.driver.enable
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++++ local property_value
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 activate-component-hive-metastore[2188]: 'nc -v -z -w 1 student-01-71e17b39d43c-qwiklab-m 3306' attempt 1 failed! Sleeping 1s.
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++++ cut -d = -f 2-
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++++ grep '^dataproc.logging.stackdriver.job.driver.enable=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++++ tail -n 1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++++ property_value=
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++++ echo ''
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++ property_value=
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++ echo ''
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ JOB_DRIVER_LOGGING_ENABLED=
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ [[ '' == \t\r\u\e ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++ get_dataproc_property dataproc.logging.stackdriver.job.yarn.container.enable
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++ local -r property_name=dataproc.logging.stackdriver.job.yarn.container.enable
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++ local property_value
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.logging.stackdriver.job.yarn.container.enable
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++++ local -r property_name=dataproc.logging.stackdriver.job.yarn.container.enable
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++++ local property_value
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++++ grep '^dataproc.logging.stackdriver.job.yarn.container.enable=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++++ tail -n 1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++++ cut -d = -f 2-
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++++ property_value=
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++++ echo ''
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++ property_value=
<13>Mar 21 02:43:17 google-dataproc-startup[948]: +++ echo ''
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ CONTAINER_LOGGING_ENABLED=
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ [[ '' == \t\r\u\e ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + run_in_background --tag setup-google-fluentd setup_service google-fluentd
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + local -r pid=2449
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + echo 'Started background process [setup_service google-fluentd] as pid 2449'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: Started background process [setup_service google-fluentd] as pid 2449
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + [[ us-central1 != \g\l\o\b\a\l ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + add_regional_bigtop_repo us-central1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + local -r region=us-central1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + local regional_bigtop_repo_uri
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + run_with_logger --tag setup-google-fluentd setup_service google-fluentd
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ cat /etc/apt/sources.list.d/dataproc.list
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + local tag=
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + local pid=2449
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + tag=setup-google-fluentd
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + shift 2
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + [[ 2 -eq 0 ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + setup_service google-fluentd
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ grep 'deb .*goog-dataproc-bigtop-repo-us-central1.* dataproc contrib'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ logger -s -t 'setup-google-fluentd[2449]'
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ sed s#dataproc-bigtop-repo#goog-dataproc-bigtop-repo-us-central1#
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ cut -d ' ' -f 2
<13>Mar 21 02:43:17 google-dataproc-startup[948]: ++ head -1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-google-fluentd[2449]: + export -f login_through_keytab_if_necessary
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-google-fluentd[2449]: + local -r service=google-fluentd
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-google-fluentd[2449]: + enable_service google-fluentd
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-google-fluentd[2449]: + local -r service=google-fluentd
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-google-fluentd[2449]: + local -r unit=google-fluentd.service
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-google-fluentd[2449]: + retry_constant_short systemctl enable google-fluentd.service
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-google-fluentd[2449]: + retry_constant_custom 30 1 systemctl enable google-fluentd.service
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-google-fluentd[2449]: + local -r max_retry_time=30
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-google-fluentd[2449]: + local -r retry_delay=1
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-google-fluentd[2449]: + cmd=("${@:3}")
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-google-fluentd[2449]: + local -r cmd
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-google-fluentd[2449]: + local -r max_retries=30
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-google-fluentd[2449]: + set +x
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-google-fluentd[2449]: About to run 'systemctl enable google-fluentd.service' with retries...
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-google-fluentd[2449]: google-fluentd.service is not a native service, redirecting to systemd-sysv-install.
<13>Mar 21 02:43:17 google-dataproc-startup[948]: <13>Mar 21 02:43:17 setup-google-fluentd[2449]: Executing: /lib/systemd/systemd-sysv-install enable google-fluentd
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + regional_bigtop_repo_uri=https://storage.googleapis.com/goog-dataproc-bigtop-repo-us-central1/2_0_deb10_20210311_015200-RC01
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + [[ -z https://storage.googleapis.com/goog-dataproc-bigtop-repo-us-central1/2_0_deb10_20210311_015200-RC01 ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + [[ https://storage.googleapis.com/goog-dataproc-bigtop-repo-us-central1/2_0_deb10_20210311_015200-RC01 == */ ]]
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + local -r bigtop_key_uri=https://storage.googleapis.com/goog-dataproc-bigtop-repo-us-central1/2_0_deb10_20210311_015200-RC01/archive.key
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + apt-key add -
<13>Mar 21 02:43:17 google-dataproc-startup[948]: + curl -fsS --retry-connrefused --retry 3 --retry-delay 5 https://storage.googleapis.com/goog-dataproc-bigtop-repo-us-central1/2_0_deb10_20210311_015200-RC01/archive.key
<13>Mar 21 02:43:18 google-dataproc-startup[948]: Warning: apt-key output should not be parsed (stdout is not a terminal)
<13>Mar 21 02:43:18 google-dataproc-startup[948]: <13>Mar 21 02:43:18 uninstall_artifacts[2147]: Building dependency tree...
<13>Mar 21 02:43:18 google-dataproc-startup[948]: <13>Mar 21 02:43:18 uninstall_artifacts[2147]: Reading state information...
<13>Mar 21 02:43:18 google-dataproc-startup[948]: <13>Mar 21 02:43:18 start-osconfig-service[2363]: Created symlink /etc/systemd/system/multi-user.target.wants/google-osconfig-agent.service → /lib/systemd/system/google-osconfig-agent.service.
<13>Mar 21 02:43:18 google-dataproc-startup[948]: <13>Mar 21 02:43:18 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 3306 (tcp) failed: Connection refused
<13>Mar 21 02:43:18 google-dataproc-startup[948]: <13>Mar 21 02:43:18 start-osconfig-service[2363]: systemctl enable google-osconfig-agent.service succeeded.
<13>Mar 21 02:43:18 google-dataproc-startup[948]: <13>Mar 21 02:43:18 start-osconfig-service[2363]: + return 0
<13>Mar 21 02:43:18 google-dataproc-startup[948]: <13>Mar 21 02:43:18 start-osconfig-service[2363]: + local -r common_restart_drop_in=/etc/systemd/system/common/restart.conf
<13>Mar 21 02:43:18 google-dataproc-startup[948]: <13>Mar 21 02:43:18 start-osconfig-service[2363]: + [[ ! -f /etc/systemd/system/common/restart.conf ]]
<13>Mar 21 02:43:18 google-dataproc-startup[948]: <13>Mar 21 02:43:18 start-osconfig-service[2363]: ++ dirname /etc/systemd/system/common/restart.conf
<13>Mar 21 02:43:18 google-dataproc-startup[948]: <13>Mar 21 02:43:18 start-osconfig-service[2363]: + mkdir -p /etc/systemd/system/common
<13>Mar 21 02:43:18 google-dataproc-startup[948]: <13>Mar 21 02:43:18 start-osconfig-service[2363]: + cat
<13>Mar 21 02:43:18 google-dataproc-startup[948]: <13>Mar 21 02:43:18 start-osconfig-service[2363]: + local -r worker_restart_drop_in=/etc/systemd/system/common/worker-restart.conf
<13>Mar 21 02:43:18 google-dataproc-startup[948]: <13>Mar 21 02:43:18 start-osconfig-service[2363]: + [[ ! -f /etc/systemd/system/common/worker-restart.conf ]]
<13>Mar 21 02:43:18 google-dataproc-startup[948]: <13>Mar 21 02:43:18 start-osconfig-service[2363]: ++ dirname /etc/systemd/system/common/worker-restart.conf
<13>Mar 21 02:43:18 google-dataproc-startup[948]: <13>Mar 21 02:43:18 start-osconfig-service[2363]: + mkdir -p /etc/systemd/system/common
<13>Mar 21 02:43:18 google-dataproc-startup[948]: <13>Mar 21 02:43:18 start-osconfig-service[2363]: + cat
<13>Mar 21 02:43:19 google-dataproc-startup[948]: <13>Mar 21 02:43:19 start-osconfig-service[2363]: + local -r drop_in_dir=/etc/systemd/system/google-osconfig-agent.service.d
<13>Mar 21 02:43:19 google-dataproc-startup[948]: <13>Mar 21 02:43:19 start-osconfig-service[2363]: + mkdir -p /etc/systemd/system/google-osconfig-agent.service.d
<13>Mar 21 02:43:19 google-dataproc-startup[948]: <13>Mar 21 02:43:19 start-osconfig-service[2363]: + local props
<13>Mar 21 02:43:19 google-dataproc-startup[948]: <13>Mar 21 02:43:19 start-osconfig-service[2363]: ++ systemctl show google-osconfig-agent.service -p Restart,RemainAfterExit
<13>Mar 21 02:43:19 google-dataproc-startup[948]: <13>Mar 21 02:43:19 uninstall_artifacts[2147]: The following packages will be REMOVED:
<13>Mar 21 02:43:19 google-dataproc-startup[948]: <13>Mar 21 02:43:19 uninstall_artifacts[2147]:   aufs-dkms* aufs-tools* bind9-host* cgroupfs-mount* containerd.io* dkms*
<13>Mar 21 02:43:19 google-dataproc-startup[948]: <13>Mar 21 02:43:19 uninstall_artifacts[2147]:   docker-ce* docker-ce-cli* druid* flink* geoip-database*
<13>Mar 21 02:43:19 google-dataproc-startup[948]: <13>Mar 21 02:43:19 uninstall_artifacts[2147]:   hadoop-hdfs-datanode* hadoop-hdfs-journalnode* hadoop-hdfs-zkfc*
<13>Mar 21 02:43:19 google-dataproc-startup[948]: <13>Mar 21 02:43:19 uninstall_artifacts[2147]:   hadoop-yarn-nodemanager* hive-webhcat* hive-webhcat-server* kafka*
<13>Mar 21 02:43:19 google-dataproc-startup[948]: <13>Mar 21 02:43:19 uninstall_artifacts[2147]:   kafka-server* knox* krb5-admin-server* krb5-config* krb5-kdc* krb5-kpropd*
<13>Mar 21 02:43:19 google-dataproc-startup[948]: <13>Mar 21 02:43:19 uninstall_artifacts[2147]:   krb5-user* libbind9-161* libdns1104* libev4* libfstrm0* libgeoip1*
<13>Mar 21 02:43:19 google-dataproc-startup[948]: <13>Mar 21 02:43:19 uninstall_artifacts[2147]:   libgssrpc4* libisc1100* libisccc161* libisccfg163* libkadm5clnt-mit11*
<13>Mar 21 02:43:19 google-dataproc-startup[948]: <13>Mar 21 02:43:19 uninstall_artifacts[2147]:   libkadm5srv-mit11* libkdb5-9* liblmdb0* liblwres161* libprotobuf-c1*
<13>Mar 21 02:43:19 google-dataproc-startup[948]: <13>Mar 21 02:43:19 uninstall_artifacts[2147]:   libverto-libev1* libverto1* presto* ranger* solr* solr-server* update-inetd*
<13>Mar 21 02:43:19 google-dataproc-startup[948]: <13>Mar 21 02:43:19 uninstall_artifacts[2147]:   xinetd* zeppelin* zookeeper-server*
<13>Mar 21 02:43:19 google-dataproc-startup[948]: <13>Mar 21 02:43:19 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 3306 (tcp) failed: Connection refused
<13>Mar 21 02:43:19 google-dataproc-startup[948]: OK
<13>Mar 21 02:43:20 google-dataproc-startup[948]: + echo 'Adding regional Bigtop repo for us-central1 in APT sources.'
<13>Mar 21 02:43:20 google-dataproc-startup[948]: Adding regional Bigtop repo for us-central1 in APT sources.
<13>Mar 21 02:43:20 google-dataproc-startup[948]: + cat
<13>Mar 21 02:43:20 google-dataproc-startup[948]: + cat /etc/apt/sources.list.d/dataproc.list
<13>Mar 21 02:43:20 google-dataproc-startup[948]: + mv -f /tmp/dataproc.list /etc/apt/sources.list.d/dataproc.list
<13>Mar 21 02:43:20 google-dataproc-startup[948]: + wait_on_async_processes
<13>Mar 21 02:43:20 google-dataproc-startup[948]: + loginfo 'Waiting on async processes'
<13>Mar 21 02:43:20 google-dataproc-startup[948]: + echo 'Waiting on async processes'
<13>Mar 21 02:43:20 google-dataproc-startup[948]: Waiting on async processes
<13>Mar 21 02:43:20 google-dataproc-startup[948]: + (( i = 0 ))
<13>Mar 21 02:43:20 google-dataproc-startup[948]: + (( i < 14 ))
<13>Mar 21 02:43:20 google-dataproc-startup[948]: + local pid=2449
<13>Mar 21 02:43:20 google-dataproc-startup[948]: + local 'cmd=setup_service google-fluentd'
<13>Mar 21 02:43:20 google-dataproc-startup[948]: + loginfo 'Waiting on pid=2449 cmd=[setup_service google-fluentd]'
<13>Mar 21 02:43:20 google-dataproc-startup[948]: + echo 'Waiting on pid=2449 cmd=[setup_service google-fluentd]'
<13>Mar 21 02:43:20 google-dataproc-startup[948]: Waiting on pid=2449 cmd=[setup_service google-fluentd]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: + echo 'setup_service google-fluentd'
<13>Mar 21 02:43:20 google-dataproc-startup[948]: + local status=0
<13>Mar 21 02:43:20 google-dataproc-startup[948]: + wait 2449
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: systemctl enable hadoop-yarn-resourcemanager.service succeeded.
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + return 0
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + local -r common_restart_drop_in=/etc/systemd/system/common/restart.conf
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + [[ ! -f /etc/systemd/system/common/restart.conf ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + local -r worker_restart_drop_in=/etc/systemd/system/common/worker-restart.conf
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + [[ ! -f /etc/systemd/system/common/worker-restart.conf ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + local -r drop_in_dir=/etc/systemd/system/hadoop-yarn-resourcemanager.service.d
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + mkdir -p /etc/systemd/system/hadoop-yarn-resourcemanager.service.d
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + local props
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 start-osconfig-service[2363]: + props='Restart=always
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 start-osconfig-service[2363]: RemainAfterExit=no'
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 start-osconfig-service[2363]: + [[ google-osconfig-agent != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 start-osconfig-service[2363]: + [[ google-osconfig-agent != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 start-osconfig-service[2363]: + [[ Restart=always
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 start-osconfig-service[2363]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 start-osconfig-service[2363]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 start-osconfig-service[2363]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: ++ systemctl show hadoop-yarn-resourcemanager.service -p Restart,RemainAfterExit
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 start-osconfig-service[2363]: ++ dirname /etc/systemd/system/common/agent-gate.conf
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 start-osconfig-service[2363]: + mkdir -p /etc/systemd/system/common
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 start-osconfig-service[2363]: + cat
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 start-osconfig-service[2363]: + [[ google-osconfig-agent == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 start-osconfig-service[2363]: + [[ google-osconfig-agent == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 start-osconfig-service[2363]: + start_service google-osconfig-agent
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 start-osconfig-service[2363]: + local -r service=google-osconfig-agent
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 start-osconfig-service[2363]: + local -r unit=google-osconfig-agent.service
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 start-osconfig-service[2363]: + retry_constant_short systemctl start google-osconfig-agent.service
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 start-osconfig-service[2363]: + retry_constant_custom 30 1 systemctl start google-osconfig-agent.service
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 start-osconfig-service[2363]: + local -r max_retry_time=30
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 start-osconfig-service[2363]: + local -r retry_delay=1
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 start-osconfig-service[2363]: + cmd=("${@:3}")
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 start-osconfig-service[2363]: + local -r cmd
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 start-osconfig-service[2363]: + local -r max_retries=30
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 start-osconfig-service[2363]: + set +x
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 start-osconfig-service[2363]: About to run 'systemctl start google-osconfig-agent.service' with retries...
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: systemctl enable hadoop-mapreduce-historyserver.service succeeded.
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: + return 0
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: + local -r common_restart_drop_in=/etc/systemd/system/common/restart.conf
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: + [[ ! -f /etc/systemd/system/common/restart.conf ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: + local -r worker_restart_drop_in=/etc/systemd/system/common/worker-restart.conf
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: + [[ ! -f /etc/systemd/system/common/worker-restart.conf ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: + local -r drop_in_dir=/etc/systemd/system/hadoop-mapreduce-historyserver.service.d
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: + mkdir -p /etc/systemd/system/hadoop-mapreduce-historyserver.service.d
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: + local props
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: ++ systemctl show hadoop-mapreduce-historyserver.service -p Restart,RemainAfterExit
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-timelineserver[2185]: systemctl enable hadoop-yarn-timelineserver.service succeeded.
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-timelineserver[2185]: + return 0
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-timelineserver[2185]: + local -r common_restart_drop_in=/etc/systemd/system/common/restart.conf
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-timelineserver[2185]: + [[ ! -f /etc/systemd/system/common/restart.conf ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-timelineserver[2185]: + local -r worker_restart_drop_in=/etc/systemd/system/common/worker-restart.conf
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-timelineserver[2185]: + [[ ! -f /etc/systemd/system/common/worker-restart.conf ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-timelineserver[2185]: + local -r drop_in_dir=/etc/systemd/system/hadoop-yarn-timelineserver.service.d
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-timelineserver[2185]: + mkdir -p /etc/systemd/system/hadoop-yarn-timelineserver.service.d
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-timelineserver[2185]: + local props
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-timelineserver[2185]: ++ systemctl show hadoop-yarn-timelineserver.service -p Restart,RemainAfterExit
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: Created symlink /etc/systemd/system/multi-user.target.wants/mysql.service → /lib/systemd/system/mysql.service.
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 3306 (tcp) failed: Connection refused
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + props='Restart=no
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: systemctl enable mysql.service succeeded.
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: RemainAfterExit=no'
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: + return 0
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + [[ hadoop-yarn-resourcemanager != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: + local -r common_restart_drop_in=/etc/systemd/system/common/restart.conf
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: + [[ ! -f /etc/systemd/system/common/restart.conf ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + [[ hadoop-yarn-resourcemanager != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + [[ Restart=no
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + [[ Restart=no
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + ln -s -f /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-yarn-resourcemanager.service.d
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + in_array hadoop-yarn-resourcemanager DATAPROC_START_AFTER_HDFS_SERVICES
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + local -r value=hadoop-yarn-resourcemanager
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + [[ !  hadoop-mapreduce-historyserver  == *\ \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r\ * ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + return 1
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + local am_on_primary_worker_enabled
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: ++ get_dataproc_property am.primary_only
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: ++ local -r property_name=am.primary_only
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: ++ local property_value
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: +++ local -r property_name=am.primary_only
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: +++ local property_value
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: + local -r worker_restart_drop_in=/etc/systemd/system/common/worker-restart.conf
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: + [[ ! -f /etc/systemd/system/common/worker-restart.conf ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: + local -r drop_in_dir=/etc/systemd/system/mysql.service.d
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: + mkdir -p /etc/systemd/system/mysql.service.d
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: + local props
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: ++ systemctl show mysql.service -p Restart,RemainAfterExit
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: ++++ tail -n 1
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: +++ property_value=false
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: +++ echo false
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: ++ property_value=false
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: ++ echo false
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + am_on_primary_worker_enabled=false
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + [[ false == \t\r\u\e ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + retry_constant systemctl start hadoop-yarn-resourcemanager
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + retry_constant_custom 300 1 systemctl start hadoop-yarn-resourcemanager
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + local -r max_retry_time=300
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + local -r retry_delay=1
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + cmd=("${@:3}")
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + local -r cmd
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + local -r max_retries=300
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: + set +x
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-resourcemanager[2183]: About to run 'systemctl start hadoop-yarn-resourcemanager' with retries...
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: + props='Restart=no
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: RemainAfterExit=no'
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: + [[ hadoop-mapreduce-historyserver != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: + [[ hadoop-mapreduce-historyserver != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: + [[ Restart=no
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: + [[ Restart=no
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: + ln -s -f /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-mapreduce-historyserver.service.d
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: + [[ hadoop-mapreduce-historyserver == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: + [[ hadoop-mapreduce-historyserver == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: + in_array hadoop-mapreduce-historyserver DATAPROC_START_AFTER_HDFS_SERVICES
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: + local -r value=hadoop-mapreduce-historyserver
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: + [[ !  hadoop-mapreduce-historyserver  == *\ \h\a\d\o\o\p\-\m\a\p\r\e\d\u\c\e\-\h\i\s\t\o\r\y\s\e\r\v\e\r\ * ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-mapreduce-historyserver[2184]: + return
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-google-fluentd[2449]: systemctl enable google-fluentd.service succeeded.
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-google-fluentd[2449]: + return 0
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-google-fluentd[2449]: + local -r common_restart_drop_in=/etc/systemd/system/common/restart.conf
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-google-fluentd[2449]: + [[ ! -f /etc/systemd/system/common/restart.conf ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-google-fluentd[2449]: + local -r worker_restart_drop_in=/etc/systemd/system/common/worker-restart.conf
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: + props='Restart=on-failure
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: RemainAfterExit=no'
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: + [[ mysql != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: + [[ mysql != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: + [[ Restart=on-failure
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-google-fluentd[2449]: + [[ ! -f /etc/systemd/system/common/worker-restart.conf ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: + [[ mysql == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-timelineserver[2185]: + props='Restart=no
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-timelineserver[2185]: RemainAfterExit=no'
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-timelineserver[2185]: + [[ hadoop-yarn-timelineserver != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-timelineserver[2185]: + [[ hadoop-yarn-timelineserver != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-timelineserver[2185]: + [[ Restart=no
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-timelineserver[2185]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-timelineserver[2185]: + [[ Restart=no
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-google-fluentd[2449]: + local -r drop_in_dir=/etc/systemd/system/google-fluentd.service.d
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: + [[ mysql == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 21 02:43:20 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-timelineserver[2185]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-google-fluentd[2449]: + mkdir -p /etc/systemd/system/google-fluentd.service.d
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-google-fluentd[2449]: + local props
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:20 setup-hadoop-yarn-timelineserver[2185]: + ln -s -f /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-yarn-timelineserver.service.d
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: + retry_constant systemctl start mysql
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: + retry_constant_custom 300 1 systemctl start mysql
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: + local -r max_retry_time=300
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: + local -r retry_delay=1
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: + cmd=("${@:3}")
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: + local -r cmd
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: + local -r max_retries=300
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: + set +x
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:20 activate-component-mysql[2194]: About to run 'systemctl start mysql' with retries...
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: ++ systemctl show google-fluentd.service -p Restart,RemainAfterExit
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 start-osconfig-service[2363]: systemctl start google-osconfig-agent.service succeeded.
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 start-osconfig-service[2363]: + return 0
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: + in_array hadoop-yarn-timelineserver DATAPROC_START_AFTER_HDFS_SERVICES
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: + local -r value=hadoop-yarn-timelineserver
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: + [[ !  hadoop-mapreduce-historyserver  == *\ \h\a\d\o\o\p\-\y\a\r\n\-\t\i\m\e\l\i\n\e\s\e\r\v\e\r\ * ]]
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: + return 1
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: + local am_on_primary_worker_enabled
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: ++ get_dataproc_property am.primary_only
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: ++ local -r property_name=am.primary_only
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: ++ local property_value
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: +++ local -r property_name=am.primary_only
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: +++ local property_value
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: ++++ tail -n 1
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: + props='Restart=no
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: RemainAfterExit=yes'
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: + [[ google-fluentd != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: + [[ google-fluentd != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: + [[ Restart=no
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: RemainAfterExit=yes == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: + [[ Restart=no
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: RemainAfterExit=yes == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: + [[ google-fluentd == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: + [[ google-fluentd == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: + in_array google-fluentd DATAPROC_START_AFTER_HDFS_SERVICES
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: + local -r value=google-fluentd
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: + [[ !  hadoop-mapreduce-historyserver  == *\ \g\o\o\g\l\e\-\f\l\u\e\n\t\d\ * ]]
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: + return 1
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: + local am_on_primary_worker_enabled
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: ++ get_dataproc_property am.primary_only
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: ++ local -r property_name=am.primary_only
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: ++ local property_value
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: +++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: +++ local -r property_name=am.primary_only
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: +++ local property_value
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: +++ property_value=false
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: +++ echo false
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: ++ property_value=false
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: ++ echo false
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: ++++ cut -d = -f 2-
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: ++++ tail -n 1
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: + am_on_primary_worker_enabled=false
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: + retry_constant systemctl start hadoop-yarn-timelineserver
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: + retry_constant_custom 300 1 systemctl start hadoop-yarn-timelineserver
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: + local -r max_retry_time=300
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: + local -r retry_delay=1
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: + cmd=("${@:3}")
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: + local -r cmd
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: + local -r max_retries=300
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: + set +x
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: About to run 'systemctl start hadoop-yarn-timelineserver' with retries...
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-hadoop-yarn-timelineserver[2185]: Warning: The unit file, source configuration file or drop-ins of hadoop-yarn-timelineserver.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: +++ property_value=false
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: +++ echo false
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: ++ property_value=false
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: ++ echo false
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: + am_on_primary_worker_enabled=false
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: + [[ google-fluentd == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: + retry_constant systemctl start google-fluentd
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: + retry_constant_custom 300 1 systemctl start google-fluentd
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: + local -r max_retry_time=300
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: + local -r retry_delay=1
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: + cmd=("${@:3}")
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: + local -r cmd
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: + local -r max_retries=300
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: + set +x
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 setup-google-fluentd[2449]: About to run 'systemctl start google-fluentd' with retries...
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 activate-component-hdfs[2187]: WARNING: HADOOP_NAMENODE_OPTS has been replaced by HDFS_NAMENODE_OPTS. Using value of HADOOP_NAMENODE_OPTS.
<13>Mar 21 02:43:21 google-dataproc-startup[948]: <13>Mar 21 02:43:21 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 3306 (tcp) failed: Connection refused
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 uninstall_artifacts[2147]: 0 upgraded, 0 newly installed, 50 to remove and 12 not upgraded.
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 uninstall_artifacts[2147]: After this operation, 4713 MB disk space will be freed.
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 uninstall_artifacts[2147]: (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 166712 files and directories currently installed.)
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 uninstall_artifacts[2147]: Removing aufs-dkms (4.19+20190211-1) ...
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-mysql[2194]: systemctl start mysql succeeded.
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-mysql[2194]: + return 0
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-mysql[2194]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-mysql[2194]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh'
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-mysql[2194]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-mysql[2194]: + touch /tmp/dataproc/components/activate/mysql.done
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-hive-metastore[2188]: Connection to student-01-71e17b39d43c-qwiklab-m 3306 port [tcp/mysql] succeeded!
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-hive-metastore[2188]: nc -v -z -w 1 student-01-71e17b39d43c-qwiklab-m 3306 succeeded.
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-hive-metastore[2188]: + return 0
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-hive-metastore[2188]: + loginfo 'Service up on host=student-01-71e17b39d43c-qwiklab-m port=3306 name=mysql.'
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-hive-metastore[2188]: + echo 'Service up on host=student-01-71e17b39d43c-qwiklab-m port=3306 name=mysql.'
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-hive-metastore[2188]: Service up on host=student-01-71e17b39d43c-qwiklab-m port=3306 name=mysql.
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-hive-metastore[2188]: + start_hive_metastore
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-hive-metastore[2188]: + enable_service hive-metastore
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-hive-metastore[2188]: + local -r service=hive-metastore
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-hive-metastore[2188]: + local -r unit=hive-metastore.service
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-hive-metastore[2188]: + retry_constant_short systemctl enable hive-metastore.service
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-hive-metastore[2188]: + retry_constant_custom 30 1 systemctl enable hive-metastore.service
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-hive-metastore[2188]: + local -r max_retry_time=30
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-hive-metastore[2188]: + local -r retry_delay=1
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-hive-metastore[2188]: + cmd=("${@:3}")
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-hive-metastore[2188]: + local -r cmd
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-hive-metastore[2188]: + local -r max_retries=30
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-hive-metastore[2188]: + set +x
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-hive-metastore[2188]: About to run 'systemctl enable hive-metastore.service' with retries...
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-hive-metastore[2188]: hive-metastore.service is not a native service, redirecting to systemd-sysv-install.
<13>Mar 21 02:43:22 google-dataproc-startup[948]: <13>Mar 21 02:43:22 activate-component-hive-metastore[2188]: Executing: /lib/systemd/systemd-sysv-install enable hive-metastore
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: systemctl enable hive-metastore.service succeeded.
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + return 0
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + local -r common_restart_drop_in=/etc/systemd/system/common/restart.conf
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + [[ ! -f /etc/systemd/system/common/restart.conf ]]
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + local -r worker_restart_drop_in=/etc/systemd/system/common/worker-restart.conf
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + [[ ! -f /etc/systemd/system/common/worker-restart.conf ]]
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + local -r drop_in_dir=/etc/systemd/system/hive-metastore.service.d
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + mkdir -p /etc/systemd/system/hive-metastore.service.d
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + local props
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: ++ systemctl show hive-metastore.service -p Restart,RemainAfterExit
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + props='Restart=no
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: RemainAfterExit=no'
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + [[ hive-metastore != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + [[ hive-metastore != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + [[ Restart=no
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + [[ Restart=no
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + ln -s -f /etc/systemd/system/common/restart.conf /etc/systemd/system/hive-metastore.service.d
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + [[ hive-metastore == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + [[ hive-metastore == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + retry_constant systemctl restart hive-metastore
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + retry_constant_custom 300 1 systemctl restart hive-metastore
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + local -r max_retry_time=300
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + local -r retry_delay=1
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + cmd=("${@:3}")
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + local -r cmd
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + local -r max_retries=300
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: + set +x
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: About to run 'systemctl restart hive-metastore' with retries...
<13>Mar 21 02:43:24 google-dataproc-startup[948]: <13>Mar 21 02:43:24 activate-component-hive-metastore[2188]: Warning: The unit file, source configuration file or drop-ins of hive-metastore.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 uninstall_artifacts[2147]: 
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 uninstall_artifacts[2147]: -------- Uninstall Beginning --------
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 uninstall_artifacts[2147]: Module:  aufs
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 uninstall_artifacts[2147]: Version: 4.19+20190211
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 uninstall_artifacts[2147]: Kernel:  4.19.0-14-cloud-amd64 (x86_64)
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 uninstall_artifacts[2147]: -------------------------------------
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 uninstall_artifacts[2147]: 
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 uninstall_artifacts[2147]: Status: Before uninstall, this module version was ACTIVE on this kernel.
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 uninstall_artifacts[2147]: 
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 uninstall_artifacts[2147]: aufs.ko:
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 uninstall_artifacts[2147]:  - Uninstallation
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 uninstall_artifacts[2147]:    - Deleting from: /lib/modules/4.19.0-14-cloud-amd64/updates/dkms/
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 uninstall_artifacts[2147]:  - Original module
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 uninstall_artifacts[2147]:    - No original module was found for this module on this kernel.
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 uninstall_artifacts[2147]:    - Use the dkms install command to reinstall any previous module version.
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 uninstall_artifacts[2147]: 
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: 2021-03-21 02:43:25,656 INFO namenode.NameNode: STARTUP_MSG: 
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: /************************************************************
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: STARTUP_MSG: Starting NameNode
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: STARTUP_MSG:   host = student-01-71e17b39d43c-qwiklab-m/10.128.0.5
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: STARTUP_MSG:   args = [-format, -nonInteractive]
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: STARTUP_MSG:   version = 3.2.2
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: STARTUP_MSG:   classpath = /etc/hadoop/conf:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.36.v20210114.jar:/usr/lib/hadoop/lib/lz4-1.2.0.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/error_prone_annotations-2.3.4.jar:/usr/lib/hadoop/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop/lib/hadoop-lzo-0.4.20.jar:/usr/lib/hadoop/lib/httpcore-4.4.13.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hado
<13>Mar 21 02:43:25 google-dataproc-startup[948]: op/lib/aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hado
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: op/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-7.9.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jsch-0.1.55.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop/lib/j2objc-annotations-1.3.jar:/usr/lib/hadoop/lib/wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/azure-storage-7.0.0.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/failureaccess-1.0.1.jar:/usr/lib/hadoop/lib/htrace-core4-4.
<13>Mar 21 02:43:25 google-dataproc-startup[948]: 1.0-incubating.jar:/usr/lib/hadoop/lib/kafka-clien
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: ts-0.8.2.1.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/ojalgo-43.0.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/commons-lang3-3.10.jar:/usr/lib/hadoop/lib/jetty-util-9.4.36.v20210114.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/httpclient-4.5.13.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop/lib/javax.activation-api-1.2.0.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/jsr305-3.0.2.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/l
<13>Mar 21 02:43:25 google-dataproc-startup[948]: ib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: /jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop/lib/checker-qual-2.10.0.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/jetty-util-ajax-9.4.36.v20210114.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop/lib/aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop/lib/jetty-security-9.4.36.v20210114.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jetty-io-9.4.36.v20210114.jar:/usr/lib/hadoop/lib/kerb-util-
<13>Mar 21 02:43:25 google-dataproc-startup[948]: 1.0.1.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.36.
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: v20210114.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/aws-java-sdk-bundle-1.11.563.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/jdom-1.1.jar:/usr/lib/hadoop/lib/avro-1.9.2.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.36.v20210114.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/guava-28.2-jre.jar:/usr/lib/hadoop/lib/jetty-http-9.4.36.v20210114.jar:/usr/lib/hadoop/lib/aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop/lib/jetty-server-9.4.36.v20210114.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.2.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib
<13>Mar 21 02:43:25 google-dataproc-startup[948]: /hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/./
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: /hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.2.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.2.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.2.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.2.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.2.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.2.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.2.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.2.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.2.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.2.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.2.jar:/usr/lib/hadoop/.//hadoop-openstack.jar
<13>Mar 21 02:43:25 google-dataproc-startup[948]: :/usr/lib/hadoop/.//hadoop-common-3.2.2-tests.jar:
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: /usr/lib/hadoop/.//hadoop-kafka-3.2.2.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.2.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-common-3.2.2.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.2.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.2.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.2.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.2.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.2.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.2.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.2.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/kerby-co
<13>Mar 21 02:43:25 google-dataproc-startup[948]: nfig-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-te
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: xt-1.4.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.36.v20210114.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/error_prone_annotations-2.3.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/nimb
<13>Mar 21 02:43:25 google-dataproc-startup[948]: us-jose-jwt-7.9.jar:/usr/lib/hadoop-hdfs/lib/commo
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: ns-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.55.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/j2objc-annotations-1.3.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/failureaccess-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/kerb-
<13>Mar 21 02:43:25 google-dataproc-startup[948]: common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: lang3-3.10.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.36.v20210114.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.13.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/javax.activation-api-1.2.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr
<13>Mar 21 02:43:25 google-dataproc-startup[948]: /lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/checker-qual-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.1.48.Final.jar:/usr/lib/hadoop-hdfs/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.36.v20210114.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.36.v20210114.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.36.v20210114.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.36.v20210114.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/avro-1.9.2.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.36.v20210114.jar:/usr/lib/hadoop-hd
<13>Mar 21 02:43:25 google-dataproc-startup[948]: fs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/l
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: ib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/guava-28.2-jre.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.36.v20210114.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.36.v20210114.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.2.jar:/usr/lib/hadoop-hdfs/.//hadoop
<13>Mar 21 02:43:25 google-dataproc-startup[948]: -hdfs-client-3.2.2-tests.jar:/usr/lib/hadoop-hdfs/
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: .//hadoop-hdfs-native-client-3.2.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.2-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.2.jar:/usr/lib/hadoop-mapred
<13>Mar 21 02:43:25 google-dataproc-startup[948]: uce/.//hadoop-mapreduce-client-hs-plugins-3.2.2.ja
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: r:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.2.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/jackson-modu
<13>Mar 21 02:43:25 google-dataproc-startup[948]: le-jaxb-annotations-2.10.5.jar:/usr/lib/hadoop-yar
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: n/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.2.jar:/usr/lib/hadoop-yarn/.//hadoop-
<13>Mar 21 02:43:25 google-dataproc-startup[948]: yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: hadoop-yarn-server-common-3.2.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3
<13>Mar 21 02:43:25 google-dataproc-startup[948]: .2.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-appli
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: cations-unmanaged-am-launcher-3.2.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-servic
<13>Mar 21 02:43:25 google-dataproc-startup[948]: es-api-3.2.2.jar:/usr/lib/hadoop-yarn/.//hadoop-ya
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: rn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/local/share/google/dataproc/lib/gcs-connector-hadoop3-2.2.0.jar:/usr/local/share/google/dataproc/lib/gcs-connector.jar
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: STARTUP_MSG:   build = https://bigdataoss-internal.googlesource.com/third_party/apache/hadoop -r 5dee0867618113edafb97f68d8aae96ff9e89bd1; compiled by 'bigtop' on 2021-03-11T10:15Z
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: STARTUP_MSG:   java = 1.8.0_282
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: ************************************************************/
<13>Mar 21 02:43:25 google-dataproc-startup[948]: <13>Mar 21 02:43:25 activate-component-hdfs[2187]: 2021-03-21 02:43:25,744 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
<13>Mar 21 02:43:26 google-dataproc-startup[948]: <13>Mar 21 02:43:26 activate-component-hdfs[2187]: 2021-03-21T02:43:26.046+0000: 4.321: [GC (Allocation Failure) 2021-03-21T02:43:26.046+0000: 4.322: [ParNew: 32320K->3968K(36288K), 0.0277733 secs] 32320K->4439K(116864K), 0.0278667 secs] [Times: user=0.00 sys=0.01, real=0.03 secs] 
<13>Mar 21 02:43:26 google-dataproc-startup[948]: <13>Mar 21 02:43:26 activate-component-hdfs[2187]: 2021-03-21 02:43:26,585 INFO namenode.NameNode: createNameNode [-format, -nonInteractive]
<13>Mar 21 02:43:27 google-dataproc-startup[948]: <13>Mar 21 02:43:27 uninstall_artifacts[2147]: depmod...
<13>Mar 21 02:43:27 google-dataproc-startup[948]: <13>Mar 21 02:43:27 uninstall_artifacts[2147]: 
<13>Mar 21 02:43:27 google-dataproc-startup[948]: <13>Mar 21 02:43:27 uninstall_artifacts[2147]: DKMS: uninstall completed.
<13>Mar 21 02:43:27 google-dataproc-startup[948]: <13>Mar 21 02:43:27 uninstall_artifacts[2147]: 
<13>Mar 21 02:43:27 google-dataproc-startup[948]: <13>Mar 21 02:43:27 uninstall_artifacts[2147]: ------------------------------
<13>Mar 21 02:43:27 google-dataproc-startup[948]: <13>Mar 21 02:43:27 uninstall_artifacts[2147]: Deleting module version: 4.19+20190211
<13>Mar 21 02:43:27 google-dataproc-startup[948]: <13>Mar 21 02:43:27 uninstall_artifacts[2147]: completely from the DKMS tree.
<13>Mar 21 02:43:27 google-dataproc-startup[948]: <13>Mar 21 02:43:27 uninstall_artifacts[2147]: ------------------------------
<13>Mar 21 02:43:27 google-dataproc-startup[948]: <13>Mar 21 02:43:27 uninstall_artifacts[2147]: Done.
<13>Mar 21 02:43:27 google-dataproc-startup[948]: <13>Mar 21 02:43:27 uninstall_artifacts[2147]: Removing aufs-tools (1:4.14+20190211-1) ...
<13>Mar 21 02:43:27 google-dataproc-startup[948]: <13>Mar 21 02:43:27 uninstall_artifacts[2147]: Removing krb5-admin-server (1.17-3+deb10u1) ...
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: systemctl restart hive-metastore succeeded.
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + return 0
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + wait_for_hive_metastore
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + local timeout
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: ++ get_dataproc_property_or_default startup.component.service-binding-timeout.hive-metastore 300
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: ++ local -r property_name=startup.component.service-binding-timeout.hive-metastore
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: ++ local -r default_value=300
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: ++ local actual_value
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: +++ get_dataproc_property startup.component.service-binding-timeout.hive-metastore
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: +++ local -r property_name=startup.component.service-binding-timeout.hive-metastore
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: +++ local property_value
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: ++++ get_java_property /etc/google-dataproc/dataproc.properties startup.component.service-binding-timeout.hive-metastore
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: ++++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: ++++ local -r property_name=startup.component.service-binding-timeout.hive-metastore
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: ++++ local property_value
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: +++++ cut -d = -f 2-
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: +++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: +++++ tail -n 1
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: +++++ grep '^startup.component.service-binding-timeout.hive-metastore=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: ++++ property_value=
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: ++++ echo ''
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: +++ property_value=
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: +++ echo ''
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: ++ actual_value=
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: ++ [[ -n '' ]]
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: ++ echo 300
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + timeout=300
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + local metastore_uris
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: ++ get_property_in_xml /etc/hive/conf/hive-site.xml hive.metastore.uris
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: ++ local -r path=/etc/hive/conf/hive-site.xml
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: ++ local -r property=hive.metastore.uris
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: ++ local -r default_value=
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: ++ local val
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: +++ bdconfig get_property_value --configuration_file /etc/hive/conf/hive-site.xml --name hive.metastore.uris
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: ++ val=thrift://student-01-71e17b39d43c-qwiklab-m:9083
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: ++ [[ thrift://student-01-71e17b39d43c-qwiklab-m:9083 == \N\o\n\e ]]
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: ++ echo thrift://student-01-71e17b39d43c-qwiklab-m:9083
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + metastore_uris=thrift://student-01-71e17b39d43c-qwiklab-m:9083
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + local host
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: ++ sed -n 's#.*://\(.*\):.*#\1#p'
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: ++ echo thrift://student-01-71e17b39d43c-qwiklab-m:9083
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + host=student-01-71e17b39d43c-qwiklab-m
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + [[ -z student-01-71e17b39d43c-qwiklab-m ]]
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + local port
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: ++ sed -n 's#.*://.*:\(.*\)#\1#p'
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: ++ echo thrift://student-01-71e17b39d43c-qwiklab-m:9083
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + port=9083
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + [[ -z 9083 ]]
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + wait_for_port hive-metastore student-01-71e17b39d43c-qwiklab-m 9083 300
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + local -r name=hive-metastore
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + local -r host=student-01-71e17b39d43c-qwiklab-m
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + local -r port=9083
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + local -r timeout=300
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + local -r capped_timeout=300
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + loginfo 'Waiting 300 seconds for service to come up on host=student-01-71e17b39d43c-qwiklab-m port=9083 name=hive-metastore.'
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + echo 'Waiting 300 seconds for service to come up on host=student-01-71e17b39d43c-qwiklab-m port=9083 name=hive-metastore.'
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: Waiting 300 seconds for service to come up on host=student-01-71e17b39d43c-qwiklab-m port=9083 name=hive-metastore.
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + retry_constant_custom 300 1 nc -v -z -w 1 student-01-71e17b39d43c-qwiklab-m 9083
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + local -r max_retry_time=300
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + local -r retry_delay=1
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + cmd=("${@:3}")
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + local -r cmd
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + local -r max_retries=300
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: + set +x
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: About to run 'nc -v -z -w 1 student-01-71e17b39d43c-qwiklab-m 9083' with retries...
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 activate-component-hive-metastore[2188]: 'nc -v -z -w 1 student-01-71e17b39d43c-qwiklab-m 9083' attempt 1 failed! Sleeping 1s.
<13>Mar 21 02:43:28 google-dataproc-startup[948]: <13>Mar 21 02:43:28 uninstall_artifacts[2147]: Removing krb5-kpropd (1.17-3+deb10u1) ...
<13>Mar 21 02:43:29 google-dataproc-startup[948]: <13>Mar 21 02:43:29 setup-hadoop-yarn-timelineserver[2185]: systemctl start hadoop-yarn-timelineserver succeeded.
<13>Mar 21 02:43:29 google-dataproc-startup[948]: <13>Mar 21 02:43:29 setup-hadoop-yarn-timelineserver[2185]: + return 0
<13>Mar 21 02:43:29 google-dataproc-startup[948]: <13>Mar 21 02:43:29 activate-component-hdfs[2187]: 2021-03-21T02:43:29.267+0000: 7.542: [GC (Allocation Failure) 2021-03-21T02:43:29.267+0000: 7.542: [ParNew: 36288K->3539K(36288K), 0.1169241 secs] 36759K->6155K(116864K), 0.1169927 secs] [Times: user=0.03 sys=0.01, real=0.11 secs] 
<13>Mar 21 02:43:29 google-dataproc-startup[948]: <13>Mar 21 02:43:29 setup-hadoop-yarn-resourcemanager[2183]: systemctl start hadoop-yarn-resourcemanager succeeded.
<13>Mar 21 02:43:29 google-dataproc-startup[948]: <13>Mar 21 02:43:29 setup-hadoop-yarn-resourcemanager[2183]: + return 0
<13>Mar 21 02:43:29 google-dataproc-startup[948]: <13>Mar 21 02:43:29 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:30 google-dataproc-startup[948]: <13>Mar 21 02:43:30 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:30 google-dataproc-startup[948]: <13>Mar 21 02:43:30 uninstall_artifacts[2147]: Removing krb5-kdc (1.17-3+deb10u1) ...
<13>Mar 21 02:43:31 google-dataproc-startup[948]: <13>Mar 21 02:43:31 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:32 google-dataproc-startup[948]: <13>Mar 21 02:43:32 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:32 google-dataproc-startup[948]: <13>Mar 21 02:43:32 uninstall_artifacts[2147]: Removing krb5-user (1.17-3+deb10u1) ...
<13>Mar 21 02:43:32 google-dataproc-startup[948]: <13>Mar 21 02:43:32 activate-component-hdfs[2187]: 2021-03-21 02:43:32,915 INFO common.Util: Assuming 'file' scheme for path /hadoop/dfs/name in configuration.
<13>Mar 21 02:43:32 google-dataproc-startup[948]: <13>Mar 21 02:43:32 activate-component-hdfs[2187]: 2021-03-21 02:43:32,942 INFO common.Util: Assuming 'file' scheme for path /hadoop/dfs/name in configuration.
<13>Mar 21 02:43:32 google-dataproc-startup[948]: <13>Mar 21 02:43:32 uninstall_artifacts[2147]: Removing krb5-config (2.6) ...
<13>Mar 21 02:43:33 google-dataproc-startup[948]: <13>Mar 21 02:43:33 activate-component-hdfs[2187]: Formatting using clusterid: CID-ea1ca698-7323-40a4-82da-8bac9f940dc5
<13>Mar 21 02:43:33 google-dataproc-startup[948]: <13>Mar 21 02:43:33 uninstall_artifacts[2147]: Removing bind9-host (1:9.11.5.P4+dfsg-5.1+deb10u3) ...
<13>Mar 21 02:43:33 google-dataproc-startup[948]: <13>Mar 21 02:43:33 setup-google-fluentd[2449]: systemctl start google-fluentd succeeded.
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + (( status != 0 ))
<13>Mar 21 02:43:33 google-dataproc-startup[948]: <13>Mar 21 02:43:33 setup-google-fluentd[2449]: + return 0
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + tee /tmp/dataproc/commands/2449.done
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + echo 'Command cmd=[setup_service google-fluentd] pid=2449 exited with 0'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: Command cmd=[setup_service google-fluentd] pid=2449 exited with 0
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + (( ++i ))
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + (( i < 14 ))
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + local pid=2363
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + local 'cmd=enable_and_start_service google-osconfig-agent'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + loginfo 'Waiting on pid=2363 cmd=[enable_and_start_service google-osconfig-agent]'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + echo 'Waiting on pid=2363 cmd=[enable_and_start_service google-osconfig-agent]'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: Waiting on pid=2363 cmd=[enable_and_start_service google-osconfig-agent]
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + echo 'enable_and_start_service google-osconfig-agent'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + local status=0
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + wait 2363
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + (( status != 0 ))
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + echo 'Command cmd=[enable_and_start_service google-osconfig-agent] pid=2363 exited with 0'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + tee /tmp/dataproc/commands/2363.done
<13>Mar 21 02:43:33 google-dataproc-startup[948]: Command cmd=[enable_and_start_service google-osconfig-agent] pid=2363 exited with 0
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + (( ++i ))
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + (( i < 14 ))
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + local pid=2208
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + local 'cmd=activate_component yarn'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + loginfo 'Waiting on pid=2208 cmd=[activate_component yarn]'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + echo 'Waiting on pid=2208 cmd=[activate_component yarn]'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: Waiting on pid=2208 cmd=[activate_component yarn]
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + echo 'activate_component yarn'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + local status=0
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + wait 2208
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + (( status != 0 ))
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + tee /tmp/dataproc/commands/2208.done
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + echo 'Command cmd=[activate_component yarn] pid=2208 exited with 0'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: Command cmd=[activate_component yarn] pid=2208 exited with 0
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + (( ++i ))
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + (( i < 14 ))
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + local pid=2207
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + local 'cmd=activate_component spark'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + loginfo 'Waiting on pid=2207 cmd=[activate_component spark]'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + echo 'Waiting on pid=2207 cmd=[activate_component spark]'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: Waiting on pid=2207 cmd=[activate_component spark]
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + echo 'activate_component spark'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + local status=0
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + wait 2207
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + (( status != 0 ))
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + echo 'Command cmd=[activate_component spark] pid=2207 exited with 0'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: <13>Mar 21 02:43:33 uninstall_artifacts[2147]: Removing cgroupfs-mount (1.4) ...
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + tee /tmp/dataproc/commands/2207.done
<13>Mar 21 02:43:33 google-dataproc-startup[948]: Command cmd=[activate_component spark] pid=2207 exited with 0
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + (( ++i ))
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + (( i < 14 ))
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + local pid=2194
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + local 'cmd=activate_component mysql'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + loginfo 'Waiting on pid=2194 cmd=[activate_component mysql]'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + echo 'Waiting on pid=2194 cmd=[activate_component mysql]'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: Waiting on pid=2194 cmd=[activate_component mysql]
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + echo 'activate_component mysql'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + local status=0
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + wait 2194
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + (( status != 0 ))
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + tee /tmp/dataproc/commands/2194.done
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + echo 'Command cmd=[activate_component mysql] pid=2194 exited with 0'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: Command cmd=[activate_component mysql] pid=2194 exited with 0
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + (( ++i ))
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + (( i < 14 ))
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + local pid=2191
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + local 'cmd=activate_component miniconda3'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + loginfo 'Waiting on pid=2191 cmd=[activate_component miniconda3]'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + echo 'Waiting on pid=2191 cmd=[activate_component miniconda3]'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: Waiting on pid=2191 cmd=[activate_component miniconda3]
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + echo 'activate_component miniconda3'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + local status=0
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + wait 2191
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + (( status != 0 ))
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + tee /tmp/dataproc/commands/2191.done
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + echo 'Command cmd=[activate_component miniconda3] pid=2191 exited with 0'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: Command cmd=[activate_component miniconda3] pid=2191 exited with 0
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + (( ++i ))
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + (( i < 14 ))
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + local pid=2190
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + local 'cmd=activate_component mapreduce'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + loginfo 'Waiting on pid=2190 cmd=[activate_component mapreduce]'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + echo 'Waiting on pid=2190 cmd=[activate_component mapreduce]'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: Waiting on pid=2190 cmd=[activate_component mapreduce]
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + echo 'activate_component mapreduce'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + local status=0
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + wait 2190
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + (( status != 0 ))
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + tee /tmp/dataproc/commands/2190.done
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + echo 'Command cmd=[activate_component mapreduce] pid=2190 exited with 0'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: Command cmd=[activate_component mapreduce] pid=2190 exited with 0
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + (( ++i ))
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + (( i < 14 ))
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + local pid=2189
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + local 'cmd=activate_component hive-server2'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + loginfo 'Waiting on pid=2189 cmd=[activate_component hive-server2]'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + echo 'Waiting on pid=2189 cmd=[activate_component hive-server2]'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: Waiting on pid=2189 cmd=[activate_component hive-server2]
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + echo 'activate_component hive-server2'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + local status=0
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + wait 2189
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + (( status != 0 ))
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + tee /tmp/dataproc/commands/2189.done
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + echo 'Command cmd=[activate_component hive-server2] pid=2189 exited with 0'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: Command cmd=[activate_component hive-server2] pid=2189 exited with 0
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + (( ++i ))
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + (( i < 14 ))
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + local pid=2188
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + local 'cmd=activate_component hive-metastore'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + loginfo 'Waiting on pid=2188 cmd=[activate_component hive-metastore]'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + echo 'Waiting on pid=2188 cmd=[activate_component hive-metastore]'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: Waiting on pid=2188 cmd=[activate_component hive-metastore]
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + echo 'activate_component hive-metastore'
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + local status=0
<13>Mar 21 02:43:33 google-dataproc-startup[948]: + wait 2188
<13>Mar 21 02:43:33 google-dataproc-startup[948]: <13>Mar 21 02:43:33 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:33 google-dataproc-startup[948]: <13>Mar 21 02:43:33 activate-component-hdfs[2187]: 2021-03-21 02:43:33,792 INFO namenode.FSEditLog: Edit logging is async:true
<13>Mar 21 02:43:34 google-dataproc-startup[948]: <13>Mar 21 02:43:34 activate-component-hdfs[2187]: 2021-03-21 02:43:34,238 INFO namenode.FSNamesystem: KeyProvider: null
<13>Mar 21 02:43:34 google-dataproc-startup[948]: <13>Mar 21 02:43:34 activate-component-hdfs[2187]: 2021-03-21 02:43:34,241 INFO namenode.FSNamesystem: fsLock is fair: true
<13>Mar 21 02:43:34 google-dataproc-startup[948]: <13>Mar 21 02:43:34 activate-component-hdfs[2187]: 2021-03-21 02:43:34,252 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
<13>Mar 21 02:43:34 google-dataproc-startup[948]: <13>Mar 21 02:43:34 activate-component-hdfs[2187]: 2021-03-21 02:43:34,289 INFO namenode.FSNamesystem: fsOwner             = hdfs (auth:SIMPLE)
<13>Mar 21 02:43:34 google-dataproc-startup[948]: <13>Mar 21 02:43:34 activate-component-hdfs[2187]: 2021-03-21 02:43:34,289 INFO namenode.FSNamesystem: supergroup          = hadoop
<13>Mar 21 02:43:34 google-dataproc-startup[948]: <13>Mar 21 02:43:34 activate-component-hdfs[2187]: 2021-03-21 02:43:34,289 INFO namenode.FSNamesystem: isPermissionEnabled = false
<13>Mar 21 02:43:34 google-dataproc-startup[948]: <13>Mar 21 02:43:34 activate-component-hdfs[2187]: 2021-03-21 02:43:34,290 INFO namenode.FSNamesystem: HA Enabled: false
<13>Mar 21 02:43:34 google-dataproc-startup[948]: <13>Mar 21 02:43:34 uninstall_artifacts[2147]: Removing docker-ce (5:19.03.15~3-0~debian-buster) ...
<13>Mar 21 02:43:34 google-dataproc-startup[948]: <13>Mar 21 02:43:34 activate-component-hdfs[2187]: 2021-03-21T02:43:34.475+0000: 12.750: [GC (Allocation Failure) 2021-03-21T02:43:34.475+0000: 12.750: [ParNew: 35859K->3967K(36288K), 0.0728890 secs] 38475K->7599K(116864K), 0.0729720 secs] [Times: user=0.02 sys=0.00, real=0.08 secs] 
<13>Mar 21 02:43:34 google-dataproc-startup[948]: <13>Mar 21 02:43:34 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:34 google-dataproc-startup[948]: <13>Mar 21 02:43:34 activate-component-hdfs[2187]: 2021-03-21 02:43:34,893 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
<13>Mar 21 02:43:35 google-dataproc-startup[948]: <13>Mar 21 02:43:35 uninstall_artifacts[2147]: Removing containerd.io (1.4.4-1) ...
<13>Mar 21 02:43:35 google-dataproc-startup[948]: <13>Mar 21 02:43:35 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:36 google-dataproc-startup[948]: <13>Mar 21 02:43:36 activate-component-hdfs[2187]: 2021-03-21T02:43:36.003+0000: 14.278: [GC (Allocation Failure) 2021-03-21T02:43:36.003+0000: 14.278: [ParNew: 36287K->3082K(36288K), 0.0332933 secs] 39919K->8630K(116864K), 0.0334257 secs] [Times: user=0.01 sys=0.00, real=0.03 secs] 
<13>Mar 21 02:43:36 google-dataproc-startup[948]: <13>Mar 21 02:43:36 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21T02:43:37.049+0000: 15.324: [GC (Allocation Failure) 2021-03-21T02:43:37.049+0000: 15.324: [ParNew: 35402K->1520K(36288K), 0.0043132 secs] 40950K->7068K(116864K), 0.0043940 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 uninstall_artifacts[2147]: Removing dkms (2.6.1-4) ...
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 uninstall_artifacts[2147]: Removing docker-ce-cli (5:19.03.15~3-0~debian-buster) ...
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,242 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,242 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=false
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,242 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.retry-hostname-dns-lookup=true
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 uninstall_artifacts[2147]: Removing druid (0.20.1-1) ...
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,288 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,288 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Mar 21 02:43:37
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,301 INFO util.GSet: Computing capacity for map BlocksMap
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,301 INFO util.GSet: VM type       = 64-bit
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,304 INFO util.GSet: 2.0% max memory 1.4 GB = 29.5 MB
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,304 INFO util.GSet: capacity      = 2^22 = 4194304 entries
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,363 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,364 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,394 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,394 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,394 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,395 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,408 INFO blockmanagement.BlockManager: defaultReplication         = 2
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,408 INFO blockmanagement.BlockManager: maxReplication             = 512
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,408 INFO blockmanagement.BlockManager: minReplication             = 1
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,408 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,408 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,408 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,408 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 uninstall_artifacts[2147]: Removing flink (1.12.2-1) ...
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,666 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,666 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,666 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,666 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 uninstall_artifacts[2147]: Removing geoip-database (20181108-1) ...
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 uninstall_artifacts[2147]: Removing hadoop-hdfs-datanode (3.2.2-1) ...
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21T02:43:37.747+0000: 16.022: [GC (Allocation Failure) 2021-03-21T02:43:37.747+0000: 16.022: [ParNew: 33840K->1450K(36288K), 0.1482684 secs] 39388K->23381K(116864K), 0.1483395 secs] [Times: user=0.07 sys=0.00, real=0.14 secs] 
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,914 INFO util.GSet: Computing capacity for map INodeMap
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,919 INFO util.GSet: VM type       = 64-bit
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,919 INFO util.GSet: 1.0% max memory 1.4 GB = 14.7 MB
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,922 INFO util.GSet: capacity      = 2^21 = 2097152 entries
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,923 INFO namenode.FSDirectory: ACLs enabled? false
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,933 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,933 INFO namenode.FSDirectory: XAttrs enabled? true
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,934 INFO namenode.NameNode: Caching file names occurring more than 10 times
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,980 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
<13>Mar 21 02:43:37 google-dataproc-startup[948]: <13>Mar 21 02:43:37 activate-component-hdfs[2187]: 2021-03-21 02:43:37,983 INFO snapshot.SnapshotManager: SkipList is disabled
<13>Mar 21 02:43:38 google-dataproc-startup[948]: <13>Mar 21 02:43:38 activate-component-hdfs[2187]: 2021-03-21 02:43:38,010 INFO util.GSet: Computing capacity for map cachedBlocks
<13>Mar 21 02:43:38 google-dataproc-startup[948]: <13>Mar 21 02:43:38 activate-component-hdfs[2187]: 2021-03-21 02:43:38,040 INFO util.GSet: VM type       = 64-bit
<13>Mar 21 02:43:38 google-dataproc-startup[948]: <13>Mar 21 02:43:38 activate-component-hdfs[2187]: 2021-03-21 02:43:38,041 INFO util.GSet: 0.25% max memory 1.4 GB = 3.7 MB
<13>Mar 21 02:43:38 google-dataproc-startup[948]: <13>Mar 21 02:43:38 activate-component-hdfs[2187]: 2021-03-21 02:43:38,041 INFO util.GSet: capacity      = 2^19 = 524288 entries
<13>Mar 21 02:43:38 google-dataproc-startup[948]: <13>Mar 21 02:43:38 activate-component-hdfs[2187]: 2021-03-21 02:43:38,129 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
<13>Mar 21 02:43:38 google-dataproc-startup[948]: <13>Mar 21 02:43:38 activate-component-hdfs[2187]: 2021-03-21 02:43:38,138 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
<13>Mar 21 02:43:38 google-dataproc-startup[948]: <13>Mar 21 02:43:38 activate-component-hdfs[2187]: 2021-03-21 02:43:38,141 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
<13>Mar 21 02:43:38 google-dataproc-startup[948]: <13>Mar 21 02:43:38 activate-component-hdfs[2187]: 2021-03-21 02:43:38,180 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
<13>Mar 21 02:43:38 google-dataproc-startup[948]: <13>Mar 21 02:43:38 activate-component-hdfs[2187]: 2021-03-21 02:43:38,181 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
<13>Mar 21 02:43:38 google-dataproc-startup[948]: <13>Mar 21 02:43:38 activate-component-hdfs[2187]: 2021-03-21 02:43:38,190 INFO util.GSet: Computing capacity for map NameNodeRetryCache
<13>Mar 21 02:43:38 google-dataproc-startup[948]: <13>Mar 21 02:43:38 activate-component-hdfs[2187]: 2021-03-21 02:43:38,203 INFO util.GSet: VM type       = 64-bit
<13>Mar 21 02:43:38 google-dataproc-startup[948]: <13>Mar 21 02:43:38 activate-component-hdfs[2187]: 2021-03-21 02:43:38,204 INFO util.GSet: 0.029999999329447746% max memory 1.4 GB = 452.6 KB
<13>Mar 21 02:43:38 google-dataproc-startup[948]: <13>Mar 21 02:43:38 activate-component-hdfs[2187]: 2021-03-21 02:43:38,205 INFO util.GSet: capacity      = 2^16 = 65536 entries
<13>Mar 21 02:43:38 google-dataproc-startup[948]: <13>Mar 21 02:43:38 activate-component-hdfs[2187]: 2021-03-21 02:43:38,389 INFO namenode.FSImage: Allocated new BlockPoolId: BP-2147142081-10.128.0.5-1616294618333
<13>Mar 21 02:43:38 google-dataproc-startup[948]: <13>Mar 21 02:43:38 activate-component-hdfs[2187]: 2021-03-21 02:43:38,447 INFO common.Storage: Storage directory /hadoop/dfs/name has been successfully formatted.
<13>Mar 21 02:43:38 google-dataproc-startup[948]: <13>Mar 21 02:43:38 activate-component-hdfs[2187]: 2021-03-21 02:43:38,598 INFO namenode.FSImageFormatProtobuf: Saving image file /hadoop/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression
<13>Mar 21 02:43:38 google-dataproc-startup[948]: <13>Mar 21 02:43:38 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:38 google-dataproc-startup[948]: <13>Mar 21 02:43:38 uninstall_artifacts[2147]: Removing hadoop-hdfs-journalnode (3.2.2-1) ...
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: 2021-03-21T02:43:38.979+0000: 17.254: [GC (Allocation Failure) 2021-03-21T02:43:38.979+0000: 17.254: [ParNew: 33770K->3968K(36288K), 0.0758135 secs] 55701K->35182K(116864K), 0.0758783 secs] [Times: user=0.03 sys=0.00, real=0.07 secs] 
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: 2021-03-21 02:43:39,410 INFO namenode.FSImageFormatProtobuf: Image file /hadoop/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 395 bytes saved in 0 seconds .
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: 2021-03-21 02:43:39,471 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: 2021-03-21 02:43:39,568 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: 2021-03-21 02:43:39,569 INFO namenode.NameNode: SHUTDOWN_MSG: 
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: /************************************************************
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: SHUTDOWN_MSG: Shutting down NameNode at student-01-71e17b39d43c-qwiklab-m/10.128.0.5
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: ************************************************************/
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: Heap
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]:  par new generation   total 36288K, used 17782K [0x00000000a2e00000, 0x00000000a5550000, 0x00000000ad460000)
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]:   eden space 32320K,  42% used [0x00000000a2e00000, 0x00000000a3b7d910, 0x00000000a4d90000)
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]:   from space 3968K, 100% used [0x00000000a5170000, 0x00000000a5550000, 0x00000000a5550000)
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]:   to   space 3968K,   0% used [0x00000000a4d90000, 0x00000000a4d90000, 0x00000000a5170000)
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]:  concurrent mark-sweep generation total 80576K, used 31214K [0x00000000ad460000, 0x00000000b2310000, 0x0000000100000000)
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]:  Metaspace       used 21404K, capacity 21656K, committed 21960K, reserved 1069056K
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]:   class space    used 2364K, capacity 2446K, committed 2508K, reserved 1048576K
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: su -s /bin/bash hdfs -c source /etc/default/hadoop-hdfs-namenode &&           login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/student-01-71e17b39d43c-qwiklab-m.us-central1-f.c.qwiklabs-gcp-01-d65f02c71c6a.internal &&           hdfs namenode -format -nonInteractive succeeded.
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: + return 0
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: + enable_and_start_service hadoop-hdfs-namenode
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: + local -r service=hadoop-hdfs-namenode
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: + enable_service hadoop-hdfs-namenode
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: + local -r service=hadoop-hdfs-namenode
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: + local -r unit=hadoop-hdfs-namenode.service
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: + retry_constant_short systemctl enable hadoop-hdfs-namenode.service
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: + retry_constant_custom 30 1 systemctl enable hadoop-hdfs-namenode.service
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: + local -r max_retry_time=30
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: + local -r retry_delay=1
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: + cmd=("${@:3}")
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: + local -r cmd
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: + local -r max_retries=30
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: + set +x
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: About to run 'systemctl enable hadoop-hdfs-namenode.service' with retries...
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: hadoop-hdfs-namenode.service is not a native service, redirecting to systemd-sysv-install.
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hdfs[2187]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-hdfs-namenode
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:39 google-dataproc-startup[948]: <13>Mar 21 02:43:39 activate-component-hive-metastore[2188]: 'nc -v -z -w 1 student-01-71e17b39d43c-qwiklab-m 9083' attempt 12 failed! Sleeping 1s.
<13>Mar 21 02:43:40 google-dataproc-startup[948]: <13>Mar 21 02:43:40 uninstall_artifacts[2147]: Removing hadoop-hdfs-zkfc (3.2.2-1) ...
<13>Mar 21 02:43:40 google-dataproc-startup[948]: <13>Mar 21 02:43:40 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:42 google-dataproc-startup[948]: <13>Mar 21 02:43:42 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: systemctl enable hadoop-hdfs-namenode.service succeeded.
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + return 0
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + local -r common_restart_drop_in=/etc/systemd/system/common/restart.conf
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + [[ ! -f /etc/systemd/system/common/restart.conf ]]
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + local -r worker_restart_drop_in=/etc/systemd/system/common/worker-restart.conf
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + [[ ! -f /etc/systemd/system/common/worker-restart.conf ]]
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + local -r drop_in_dir=/etc/systemd/system/hadoop-hdfs-namenode.service.d
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + mkdir -p /etc/systemd/system/hadoop-hdfs-namenode.service.d
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + local props
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: ++ systemctl show hadoop-hdfs-namenode.service -p Restart,RemainAfterExit
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + props='Restart=no
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: RemainAfterExit=no'
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + [[ hadoop-hdfs-namenode != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + [[ hadoop-hdfs-namenode != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + [[ Restart=no
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + [[ Restart=no
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + ln -s -f /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-hdfs-namenode.service.d
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + start_service hadoop-hdfs-namenode
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + local -r service=hadoop-hdfs-namenode
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + local -r unit=hadoop-hdfs-namenode.service
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + retry_constant_short systemctl start hadoop-hdfs-namenode.service
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + retry_constant_custom 30 1 systemctl start hadoop-hdfs-namenode.service
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + local -r max_retry_time=30
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + local -r retry_delay=1
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + cmd=("${@:3}")
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + local -r cmd
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + local -r max_retries=30
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: + set +x
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: About to run 'systemctl start hadoop-hdfs-namenode.service' with retries...
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 activate-component-hdfs[2187]: Warning: The unit file, source configuration file or drop-ins of hadoop-hdfs-namenode.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Mar 21 02:43:43 google-dataproc-startup[948]: <13>Mar 21 02:43:43 uninstall_artifacts[2147]: Removing hadoop-yarn-nodemanager (3.2.2-1) ...
<13>Mar 21 02:43:44 google-dataproc-startup[948]: <13>Mar 21 02:43:44 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:44 google-dataproc-startup[948]: <13>Mar 21 02:43:44 uninstall_artifacts[2147]: Removing hive-webhcat-server (3.1.2-1) ...
<13>Mar 21 02:43:45 google-dataproc-startup[948]: <13>Mar 21 02:43:45 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:45 google-dataproc-startup[948]: <13>Mar 21 02:43:45 uninstall_artifacts[2147]: Removing hive-webhcat (3.1.2-1) ...
<13>Mar 21 02:43:45 google-dataproc-startup[948]: <13>Mar 21 02:43:45 uninstall_artifacts[2147]: Removing kafka-server (2.3.1-1) ...
<13>Mar 21 02:43:45 google-dataproc-startup[948]: <13>Mar 21 02:43:45 uninstall_artifacts[2147]: Removing kafka (2.3.1-1) ...
<13>Mar 21 02:43:45 google-dataproc-startup[948]: <13>Mar 21 02:43:45 uninstall_artifacts[2147]: Removing knox (1.3.0-1) ...
<13>Mar 21 02:43:46 google-dataproc-startup[948]: <13>Mar 21 02:43:46 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:47 google-dataproc-startup[948]: <13>Mar 21 02:43:47 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:48 google-dataproc-startup[948]: <13>Mar 21 02:43:48 uninstall_artifacts[2147]: Removing libbind9-161:amd64 (1:9.11.5.P4+dfsg-5.1+deb10u3) ...
<13>Mar 21 02:43:48 google-dataproc-startup[948]: <13>Mar 21 02:43:48 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:48 google-dataproc-startup[948]: <13>Mar 21 02:43:48 uninstall_artifacts[2147]: Removing libisccfg163:amd64 (1:9.11.5.P4+dfsg-5.1+deb10u3) ...
<13>Mar 21 02:43:48 google-dataproc-startup[948]: <13>Mar 21 02:43:48 uninstall_artifacts[2147]: Removing libdns1104:amd64 (1:9.11.5.P4+dfsg-5.1+deb10u3) ...
<13>Mar 21 02:43:48 google-dataproc-startup[948]: <13>Mar 21 02:43:48 uninstall_artifacts[2147]: Removing liblwres161:amd64 (1:9.11.5.P4+dfsg-5.1+deb10u3) ...
<13>Mar 21 02:43:48 google-dataproc-startup[948]: <13>Mar 21 02:43:48 uninstall_artifacts[2147]: Removing libisccc161:amd64 (1:9.11.5.P4+dfsg-5.1+deb10u3) ...
<13>Mar 21 02:43:48 google-dataproc-startup[948]: <13>Mar 21 02:43:48 uninstall_artifacts[2147]: Removing libisc1100:amd64 (1:9.11.5.P4+dfsg-5.1+deb10u3) ...
<13>Mar 21 02:43:48 google-dataproc-startup[948]: <13>Mar 21 02:43:48 uninstall_artifacts[2147]: Removing libgeoip1:amd64 (1.6.12-1) ...
<13>Mar 21 02:43:49 google-dataproc-startup[948]: <13>Mar 21 02:43:49 uninstall_artifacts[2147]: Removing libkadm5srv-mit11:amd64 (1.17-3+deb10u1) ...
<13>Mar 21 02:43:49 google-dataproc-startup[948]: <13>Mar 21 02:43:49 uninstall_artifacts[2147]: Removing libkdb5-9:amd64 (1.17-3+deb10u1) ...
<13>Mar 21 02:43:49 google-dataproc-startup[948]: <13>Mar 21 02:43:49 uninstall_artifacts[2147]: Removing libkadm5clnt-mit11:amd64 (1.17-3+deb10u1) ...
<13>Mar 21 02:43:49 google-dataproc-startup[948]: <13>Mar 21 02:43:49 uninstall_artifacts[2147]: Removing libgssrpc4:amd64 (1.17-3+deb10u1) ...
<13>Mar 21 02:43:49 google-dataproc-startup[948]: <13>Mar 21 02:43:49 uninstall_artifacts[2147]: Removing liblmdb0:amd64 (0.9.22-1) ...
<13>Mar 21 02:43:49 google-dataproc-startup[948]: <13>Mar 21 02:43:49 uninstall_artifacts[2147]: Removing libprotobuf-c1:amd64 (1.3.1-1+b1) ...
<13>Mar 21 02:43:49 google-dataproc-startup[948]: <13>Mar 21 02:43:49 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:49 google-dataproc-startup[948]: <13>Mar 21 02:43:49 uninstall_artifacts[2147]: Removing presto (340-1) ...
<13>Mar 21 02:43:49 google-dataproc-startup[948]: <13>Mar 21 02:43:49 uninstall_artifacts[2147]: Removing ranger (2.0.0-1) ...
<13>Mar 21 02:43:50 google-dataproc-startup[948]: <13>Mar 21 02:43:50 uninstall_artifacts[2147]: Removing solr-server (8.1.1-1) ...
<13>Mar 21 02:43:50 google-dataproc-startup[948]: <13>Mar 21 02:43:50 uninstall_artifacts[2147]: Removing solr (8.1.1-1) ...
<13>Mar 21 02:43:50 google-dataproc-startup[948]: <13>Mar 21 02:43:50 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:50 google-dataproc-startup[948]: <13>Mar 21 02:43:50 activate-component-hive-metastore[2188]: 'nc -v -z -w 1 student-01-71e17b39d43c-qwiklab-m 9083' attempt 22 failed! Sleeping 1s.
<13>Mar 21 02:43:50 google-dataproc-startup[948]: <13>Mar 21 02:43:50 uninstall_artifacts[2147]: Removing update-inetd (4.49) ...
<13>Mar 21 02:43:51 google-dataproc-startup[948]: <13>Mar 21 02:43:51 uninstall_artifacts[2147]: Removing xinetd (1:2.3.15.3-1) ...
<13>Mar 21 02:43:51 google-dataproc-startup[948]: <13>Mar 21 02:43:51 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:52 google-dataproc-startup[948]: <13>Mar 21 02:43:52 activate-component-hdfs[2187]: systemctl start hadoop-hdfs-namenode.service succeeded.
<13>Mar 21 02:43:52 google-dataproc-startup[948]: <13>Mar 21 02:43:52 activate-component-hdfs[2187]: + return 0
<13>Mar 21 02:43:52 google-dataproc-startup[948]: <13>Mar 21 02:43:52 activate-component-hdfs[2187]: + start_hdfs_secondarynamenode
<13>Mar 21 02:43:52 google-dataproc-startup[948]: <13>Mar 21 02:43:52 activate-component-hdfs[2187]: + enable_and_start_service hadoop-hdfs-secondarynamenode
<13>Mar 21 02:43:52 google-dataproc-startup[948]: <13>Mar 21 02:43:52 activate-component-hdfs[2187]: + local -r service=hadoop-hdfs-secondarynamenode
<13>Mar 21 02:43:52 google-dataproc-startup[948]: <13>Mar 21 02:43:52 activate-component-hdfs[2187]: + enable_service hadoop-hdfs-secondarynamenode
<13>Mar 21 02:43:52 google-dataproc-startup[948]: <13>Mar 21 02:43:52 activate-component-hdfs[2187]: + local -r service=hadoop-hdfs-secondarynamenode
<13>Mar 21 02:43:52 google-dataproc-startup[948]: <13>Mar 21 02:43:52 activate-component-hdfs[2187]: + local -r unit=hadoop-hdfs-secondarynamenode.service
<13>Mar 21 02:43:52 google-dataproc-startup[948]: <13>Mar 21 02:43:52 activate-component-hdfs[2187]: + retry_constant_short systemctl enable hadoop-hdfs-secondarynamenode.service
<13>Mar 21 02:43:52 google-dataproc-startup[948]: <13>Mar 21 02:43:52 activate-component-hdfs[2187]: + retry_constant_custom 30 1 systemctl enable hadoop-hdfs-secondarynamenode.service
<13>Mar 21 02:43:52 google-dataproc-startup[948]: <13>Mar 21 02:43:52 activate-component-hdfs[2187]: + local -r max_retry_time=30
<13>Mar 21 02:43:52 google-dataproc-startup[948]: <13>Mar 21 02:43:52 activate-component-hdfs[2187]: + local -r retry_delay=1
<13>Mar 21 02:43:52 google-dataproc-startup[948]: <13>Mar 21 02:43:52 activate-component-hdfs[2187]: + cmd=("${@:3}")
<13>Mar 21 02:43:52 google-dataproc-startup[948]: <13>Mar 21 02:43:52 activate-component-hdfs[2187]: + local -r cmd
<13>Mar 21 02:43:52 google-dataproc-startup[948]: <13>Mar 21 02:43:52 activate-component-hdfs[2187]: + local -r max_retries=30
<13>Mar 21 02:43:52 google-dataproc-startup[948]: <13>Mar 21 02:43:52 activate-component-hdfs[2187]: + set +x
<13>Mar 21 02:43:52 google-dataproc-startup[948]: <13>Mar 21 02:43:52 activate-component-hdfs[2187]: About to run 'systemctl enable hadoop-hdfs-secondarynamenode.service' with retries...
<13>Mar 21 02:43:52 google-dataproc-startup[948]: <13>Mar 21 02:43:52 activate-component-hdfs[2187]: hadoop-hdfs-secondarynamenode.service is not a native service, redirecting to systemd-sysv-install.
<13>Mar 21 02:43:52 google-dataproc-startup[948]: <13>Mar 21 02:43:52 activate-component-hdfs[2187]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-hdfs-secondarynamenode
<13>Mar 21 02:43:52 google-dataproc-startup[948]: <13>Mar 21 02:43:52 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:53 google-dataproc-startup[948]: <13>Mar 21 02:43:53 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:54 google-dataproc-startup[948]: <13>Mar 21 02:43:54 uninstall_artifacts[2147]: Removing zeppelin (0.9.0-1) ...
<13>Mar 21 02:43:54 google-dataproc-startup[948]: <13>Mar 21 02:43:54 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:55 google-dataproc-startup[948]: <13>Mar 21 02:43:55 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: systemctl enable hadoop-hdfs-secondarynamenode.service succeeded.
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + return 0
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + local -r common_restart_drop_in=/etc/systemd/system/common/restart.conf
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + [[ ! -f /etc/systemd/system/common/restart.conf ]]
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + local -r worker_restart_drop_in=/etc/systemd/system/common/worker-restart.conf
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + [[ ! -f /etc/systemd/system/common/worker-restart.conf ]]
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + local -r drop_in_dir=/etc/systemd/system/hadoop-hdfs-secondarynamenode.service.d
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + mkdir -p /etc/systemd/system/hadoop-hdfs-secondarynamenode.service.d
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + local props
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: ++ systemctl show hadoop-hdfs-secondarynamenode.service -p Restart,RemainAfterExit
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + props='Restart=no
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: RemainAfterExit=no'
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + [[ hadoop-hdfs-secondarynamenode != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + [[ hadoop-hdfs-secondarynamenode != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + [[ Restart=no
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + [[ Restart=no
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + ln -s -f /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-hdfs-secondarynamenode.service.d
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + start_service hadoop-hdfs-secondarynamenode
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + local -r service=hadoop-hdfs-secondarynamenode
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + local -r unit=hadoop-hdfs-secondarynamenode.service
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + retry_constant_short systemctl start hadoop-hdfs-secondarynamenode.service
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + retry_constant_custom 30 1 systemctl start hadoop-hdfs-secondarynamenode.service
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + local -r max_retry_time=30
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + local -r retry_delay=1
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + cmd=("${@:3}")
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + local -r cmd
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + local -r max_retries=30
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: + set +x
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: About to run 'systemctl start hadoop-hdfs-secondarynamenode.service' with retries...
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hdfs[2187]: Warning: The unit file, source configuration file or drop-ins of hadoop-hdfs-secondarynamenode.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 uninstall_artifacts[2147]: Removing zookeeper-server (3.4.14-1) ...
<13>Mar 21 02:43:56 google-dataproc-startup[948]: <13>Mar 21 02:43:56 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:57 google-dataproc-startup[948]: <13>Mar 21 02:43:57 uninstall_artifacts[2147]: Removing libfstrm0:amd64 (0.4.0-1) ...
<13>Mar 21 02:43:57 google-dataproc-startup[948]: <13>Mar 21 02:43:57 uninstall_artifacts[2147]: Removing libverto1:amd64 (0.3.0-2) ...
<13>Mar 21 02:43:57 google-dataproc-startup[948]: <13>Mar 21 02:43:57 uninstall_artifacts[2147]: Removing libverto-libev1:amd64 (0.3.0-2) ...
<13>Mar 21 02:43:57 google-dataproc-startup[948]: <13>Mar 21 02:43:57 uninstall_artifacts[2147]: Removing libev4:amd64 (1:4.25-1) ...
<13>Mar 21 02:43:57 google-dataproc-startup[948]: <13>Mar 21 02:43:57 uninstall_artifacts[2147]: Processing triggers for man-db (2.8.5-2) ...
<13>Mar 21 02:43:57 google-dataproc-startup[948]: <13>Mar 21 02:43:57 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:43:59 google-dataproc-startup[948]: <13>Mar 21 02:43:59 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:44:00 google-dataproc-startup[948]: <13>Mar 21 02:44:00 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:44:00 google-dataproc-startup[948]: <13>Mar 21 02:44:00 uninstall_artifacts[2147]: Processing triggers for libc-bin (2.28-10) ...
<13>Mar 21 02:44:01 google-dataproc-startup[948]: <13>Mar 21 02:44:01 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:44:01 google-dataproc-startup[948]: <13>Mar 21 02:44:01 activate-component-hive-metastore[2188]: 'nc -v -z -w 1 student-01-71e17b39d43c-qwiklab-m 9083' attempt 32 failed! Sleeping 1s.
<13>Mar 21 02:44:01 google-dataproc-startup[948]: <13>Mar 21 02:44:01 uninstall_artifacts[2147]: (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 157629 files and directories currently installed.)
<13>Mar 21 02:44:01 google-dataproc-startup[948]: <13>Mar 21 02:44:01 uninstall_artifacts[2147]: Purging configuration files for krb5-admin-server (1.17-3+deb10u1) ...
<13>Mar 21 02:44:02 google-dataproc-startup[948]: <13>Mar 21 02:44:02 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:44:03 google-dataproc-startup[948]: <13>Mar 21 02:44:03 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: systemctl start hadoop-hdfs-secondarynamenode.service succeeded.
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + return 0
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + init_hcfs_dirs
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + [[ hdfs://student-01-71e17b39d43c-qwiklab-m == hdfs://student-01-71e17b39d43c-qwiklab* ]]
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + loginfo 'Waiting for NameNode to listen on RPC port'
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + echo 'Waiting for NameNode to listen on RPC port'
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: Waiting for NameNode to listen on RPC port
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + local namenode_port_binding_timeout
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++ get_dataproc_property_or_default startup.component.service-binding-timeout.hadoop-hdfs-namenode 300
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++ local -r property_name=startup.component.service-binding-timeout.hadoop-hdfs-namenode
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++ local -r default_value=300
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++ local actual_value
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: +++ get_dataproc_property startup.component.service-binding-timeout.hadoop-hdfs-namenode
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: +++ local -r property_name=startup.component.service-binding-timeout.hadoop-hdfs-namenode
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: +++ local property_value
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++++ get_java_property /etc/google-dataproc/dataproc.properties startup.component.service-binding-timeout.hadoop-hdfs-namenode
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++++ local -r property_name=startup.component.service-binding-timeout.hadoop-hdfs-namenode
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++++ local property_value
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: +++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: +++++ cut -d = -f 2-
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: +++++ tail -n 1
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: +++++ grep '^startup.component.service-binding-timeout.hadoop-hdfs-namenode=' /etc/google-dataproc/dataproc.properties
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++++ property_value=
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++++ echo ''
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: +++ property_value=
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: +++ echo ''
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++ actual_value=
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++ [[ -n '' ]]
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++ echo 300
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + namenode_port_binding_timeout=300
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + wait_for_port hadoop-hdfs-namenode student-01-71e17b39d43c-qwiklab-m 8020 300
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + local -r name=hadoop-hdfs-namenode
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + local -r host=student-01-71e17b39d43c-qwiklab-m
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + local -r port=8020
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + local -r timeout=300
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + local -r capped_timeout=300
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + loginfo 'Waiting 300 seconds for service to come up on host=student-01-71e17b39d43c-qwiklab-m port=8020 name=hadoop-hdfs-namenode.'
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + echo 'Waiting 300 seconds for service to come up on host=student-01-71e17b39d43c-qwiklab-m port=8020 name=hadoop-hdfs-namenode.'
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: Waiting 300 seconds for service to come up on host=student-01-71e17b39d43c-qwiklab-m port=8020 name=hadoop-hdfs-namenode.
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + retry_constant_custom 300 1 nc -v -z -w 1 student-01-71e17b39d43c-qwiklab-m 8020
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + local -r max_retry_time=300
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + local -r retry_delay=1
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + cmd=("${@:3}")
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + local -r cmd
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + local -r max_retries=300
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + set +x
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: About to run 'nc -v -z -w 1 student-01-71e17b39d43c-qwiklab-m 8020' with retries...
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: Connection to student-01-71e17b39d43c-qwiklab-m 8020 port [tcp/*] succeeded!
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: nc -v -z -w 1 student-01-71e17b39d43c-qwiklab-m 8020 succeeded.
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + return 0
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + loginfo 'Service up on host=student-01-71e17b39d43c-qwiklab-m port=8020 name=hadoop-hdfs-namenode.'
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + echo 'Service up on host=student-01-71e17b39d43c-qwiklab-m port=8020 name=hadoop-hdfs-namenode.'
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: Service up on host=student-01-71e17b39d43c-qwiklab-m port=8020 name=hadoop-hdfs-namenode.
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + hadoop_users=(hdfs mapred yarn spark pig hive hbase zookeeper)
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + local -r hadoop_users
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + local -a real_users
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + real_users=($(getent passwd | awk -F: '1000 < $3 && $3 < 6000 { print $1}'))
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++ awk -F: '1000 < $3 && $3 < 6000 { print $1}'
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++ getent passwd
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + hdfs_users=("${hadoop_users[@]}" "${real_users[@]}")
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + local -r hdfs_users
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + hdfs_user_dirs=("${hdfs_users[@]/#//user/}")
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + local -r hdfs_user_dirs
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + local spark_eventlog_dir
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++ get_java_property /etc/spark/conf/spark-defaults.conf spark.eventLog.dir
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++ local -r property_file=/etc/spark/conf/spark-defaults.conf
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++ local -r property_name=spark.eventLog.dir
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++ local property_value
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: +++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: +++ cut -d = -f 2-
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: +++ tail -n 1
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: +++ grep '^spark.eventLog.dir=' /etc/spark/conf/spark-defaults.conf
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++ property_value=gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/spark-job-history
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++ echo gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/spark-job-history
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + spark_eventlog_dir=gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/spark-job-history
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + hdfs_dirs=("${hdfs_user_dirs[@]}" "/tmp/hadoop-yarn/staging/history" "${spark_eventlog_dir}")
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + local -r hdfs_dirs
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + loginfo 'Initializing HDFS directories: /user/hdfs /user/mapred /user/yarn /user/spark /user/pig /user/hive /user/hbase /user/zookeeper /tmp/hadoop-yarn/staging/history gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/spark-job-history'
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + echo 'Initializing HDFS directories: /user/hdfs /user/mapred /user/yarn /user/spark /user/pig /user/hive /user/hbase /user/zookeeper /tmp/hadoop-yarn/staging/history gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/spark-job-history'
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: Initializing HDFS directories: /user/hdfs /user/mapred /user/yarn /user/spark /user/pig /user/hive /user/hbase /user/zookeeper /tmp/hadoop-yarn/staging/history gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/spark-job-history
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + escaped_hdfs_dirs=()
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + local escaped_hdfs_dirs
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + local escaped_hdfs_dir
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + for hdfs_dir in "${hdfs_dirs[@]}"
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++ printf %q /user/hdfs
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + escaped_hdfs_dir=/user/hdfs
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + [[ /user/hdfs == *\\* ]]
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + escaped_hdfs_dirs+=("${escaped_hdfs_dir}")
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + for hdfs_dir in "${hdfs_dirs[@]}"
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++ printf %q /user/mapred
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + escaped_hdfs_dir=/user/mapred
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + [[ /user/mapred == *\\* ]]
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + escaped_hdfs_dirs+=("${escaped_hdfs_dir}")
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + for hdfs_dir in "${hdfs_dirs[@]}"
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++ printf %q /user/yarn
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + escaped_hdfs_dir=/user/yarn
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + [[ /user/yarn == *\\* ]]
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + escaped_hdfs_dirs+=("${escaped_hdfs_dir}")
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + for hdfs_dir in "${hdfs_dirs[@]}"
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++ printf %q /user/spark
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + escaped_hdfs_dir=/user/spark
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + [[ /user/spark == *\\* ]]
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + escaped_hdfs_dirs+=("${escaped_hdfs_dir}")
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + for hdfs_dir in "${hdfs_dirs[@]}"
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++ printf %q /user/pig
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + escaped_hdfs_dir=/user/pig
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + [[ /user/pig == *\\* ]]
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + escaped_hdfs_dirs+=("${escaped_hdfs_dir}")
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + for hdfs_dir in "${hdfs_dirs[@]}"
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++ printf %q /user/hive
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + escaped_hdfs_dir=/user/hive
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + [[ /user/hive == *\\* ]]
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + escaped_hdfs_dirs+=("${escaped_hdfs_dir}")
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + for hdfs_dir in "${hdfs_dirs[@]}"
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++ printf %q /user/hbase
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + escaped_hdfs_dir=/user/hbase
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + [[ /user/hbase == *\\* ]]
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + escaped_hdfs_dirs+=("${escaped_hdfs_dir}")
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + for hdfs_dir in "${hdfs_dirs[@]}"
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++ printf %q /user/zookeeper
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + escaped_hdfs_dir=/user/zookeeper
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + [[ /user/zookeeper == *\\* ]]
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + escaped_hdfs_dirs+=("${escaped_hdfs_dir}")
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + for hdfs_dir in "${hdfs_dirs[@]}"
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++ printf %q /tmp/hadoop-yarn/staging/history
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + escaped_hdfs_dir=/tmp/hadoop-yarn/staging/history
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + [[ /tmp/hadoop-yarn/staging/history == *\\* ]]
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + escaped_hdfs_dirs+=("${escaped_hdfs_dir}")
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + for hdfs_dir in "${hdfs_dirs[@]}"
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: ++ printf %q gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/spark-job-history
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + escaped_hdfs_dir=gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/spark-job-history
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + [[ gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/spark-job-history == *\\* ]]
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + escaped_hdfs_dirs+=("${escaped_hdfs_dir}")
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + [[ hdfs://student-01-71e17b39d43c-qwiklab-m == hdfs://* ]]
<13>Mar 21 02:44:04 google-dataproc-startup[948]: <13>Mar 21 02:44:04 activate-component-hdfs[2187]: + su -s /bin/bash hdfs -c 'login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/student-01-71e17b39d43c-qwiklab-m.us-central1-f.c.qwiklabs-gcp-01-d65f02c71c6a.internal &&           hadoop fs -mkdir -p /user/hdfs /user/mapred /user/yarn /user/spark /user/pig /user/hive /user/hbase /user/zookeeper /tmp/hadoop-yarn/staging/history gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/spark-job-history'
<13>Mar 21 02:44:05 google-dataproc-startup[948]: <13>Mar 21 02:44:05 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:44:05 google-dataproc-startup[948]: <13>Mar 21 02:44:05 uninstall_artifacts[2147]: Purging configuration files for xinetd (1:2.3.15.3-1) ...
<13>Mar 21 02:44:06 google-dataproc-startup[948]: <13>Mar 21 02:44:06 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:44:06 google-dataproc-startup[948]: <13>Mar 21 02:44:06 uninstall_artifacts[2147]: Purging configuration files for aufs-tools (1:4.14+20190211-1) ...
<13>Mar 21 02:44:07 google-dataproc-startup[948]: <13>Mar 21 02:44:07 uninstall_artifacts[2147]: Purging configuration files for hive-webhcat-server (3.1.2-1) ...
<13>Mar 21 02:44:07 google-dataproc-startup[948]: <13>Mar 21 02:44:07 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:44:07 google-dataproc-startup[948]: <13>Mar 21 02:44:07 uninstall_artifacts[2147]: Purging configuration files for knox (1.3.0-1) ...
<13>Mar 21 02:44:08 google-dataproc-startup[948]: <13>Mar 21 02:44:08 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:44:09 google-dataproc-startup[948]: <13>Mar 21 02:44:09 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:44:09 google-dataproc-startup[948]: <13>Mar 21 02:44:09 uninstall_artifacts[2147]: dpkg: warning: while removing knox, directory '/var/log/knox' not empty so not removed
<13>Mar 21 02:44:09 google-dataproc-startup[948]: <13>Mar 21 02:44:09 uninstall_artifacts[2147]: Purging configuration files for solr (8.1.1-1) ...
<13>Mar 21 02:44:09 google-dataproc-startup[948]: <13>Mar 21 02:44:09 uninstall_artifacts[2147]: Purging configuration files for update-inetd (4.49) ...
<13>Mar 21 02:44:10 google-dataproc-startup[948]: <13>Mar 21 02:44:10 uninstall_artifacts[2147]: Purging configuration files for hadoop-hdfs-zkfc (3.2.2-1) ...
<13>Mar 21 02:44:10 google-dataproc-startup[948]: <13>Mar 21 02:44:10 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:44:10 google-dataproc-startup[948]: <13>Mar 21 02:44:10 uninstall_artifacts[2147]: Purging configuration files for solr-server (8.1.1-1) ...
<13>Mar 21 02:44:10 google-dataproc-startup[948]: <13>Mar 21 02:44:10 uninstall_artifacts[2147]: Purging configuration files for dkms (2.6.1-4) ...
<13>Mar 21 02:44:10 google-dataproc-startup[948]: <13>Mar 21 02:44:10 uninstall_artifacts[2147]: Purging configuration files for hadoop-yarn-nodemanager (3.2.2-1) ...
<13>Mar 21 02:44:11 google-dataproc-startup[948]: <13>Mar 21 02:44:11 uninstall_artifacts[2147]: Purging configuration files for hadoop-hdfs-datanode (3.2.2-1) ...
<13>Mar 21 02:44:11 google-dataproc-startup[948]: <13>Mar 21 02:44:11 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:44:12 google-dataproc-startup[948]: <13>Mar 21 02:44:11 uninstall_artifacts[2147]: Purging configuration files for krb5-kpropd (1.17-3+deb10u1) ...
<13>Mar 21 02:44:12 google-dataproc-startup[948]: <13>Mar 21 02:44:12 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:44:12 google-dataproc-startup[948]: <13>Mar 21 02:44:12 activate-component-hive-metastore[2188]: 'nc -v -z -w 1 student-01-71e17b39d43c-qwiklab-m 9083' attempt 43 failed! Sleeping 1s.
<13>Mar 21 02:44:13 google-dataproc-startup[948]: <13>Mar 21 02:44:13 uninstall_artifacts[2147]: Purging configuration files for kafka (2.3.1-1) ...
<13>Mar 21 02:44:13 google-dataproc-startup[948]: <13>Mar 21 02:44:13 uninstall_artifacts[2147]: Purging configuration files for flink (1.12.2-1) ...
<13>Mar 21 02:44:13 google-dataproc-startup[948]: <13>Mar 21 02:44:13 uninstall_artifacts[2147]: Purging configuration files for docker-ce (5:19.03.15~3-0~debian-buster) ...
<13>Mar 21 02:44:13 google-dataproc-startup[948]: <13>Mar 21 02:44:13 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:44:14 google-dataproc-startup[948]: <13>Mar 21 02:44:14 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:44:15 google-dataproc-startup[948]: <13>Mar 21 02:44:15 uninstall_artifacts[2147]: Purging configuration files for krb5-config (2.6) ...
<13>Mar 21 02:44:15 google-dataproc-startup[948]: <13>Mar 21 02:44:15 uninstall_artifacts[2147]: Purging configuration files for krb5-kdc (1.17-3+deb10u1) ...
<13>Mar 21 02:44:15 google-dataproc-startup[948]: <13>Mar 21 02:44:15 activate-component-hdfs[2187]: + retry_constant_short sudo -u hdfs hadoop fs -chmod -R 1777 /
<13>Mar 21 02:44:15 google-dataproc-startup[948]: <13>Mar 21 02:44:15 activate-component-hdfs[2187]: + retry_constant_custom 30 1 sudo -u hdfs hadoop fs -chmod -R 1777 /
<13>Mar 21 02:44:15 google-dataproc-startup[948]: <13>Mar 21 02:44:15 activate-component-hdfs[2187]: + local -r max_retry_time=30
<13>Mar 21 02:44:15 google-dataproc-startup[948]: <13>Mar 21 02:44:15 activate-component-hdfs[2187]: + local -r retry_delay=1
<13>Mar 21 02:44:15 google-dataproc-startup[948]: <13>Mar 21 02:44:15 activate-component-hdfs[2187]: + cmd=("${@:3}")
<13>Mar 21 02:44:15 google-dataproc-startup[948]: <13>Mar 21 02:44:15 activate-component-hdfs[2187]: + local -r cmd
<13>Mar 21 02:44:15 google-dataproc-startup[948]: <13>Mar 21 02:44:15 activate-component-hdfs[2187]: + local -r max_retries=30
<13>Mar 21 02:44:15 google-dataproc-startup[948]: <13>Mar 21 02:44:15 activate-component-hdfs[2187]: + set +x
<13>Mar 21 02:44:15 google-dataproc-startup[948]: <13>Mar 21 02:44:15 activate-component-hdfs[2187]: About to run 'sudo -u hdfs hadoop fs -chmod -R 1777 /' with retries...
<13>Mar 21 02:44:15 google-dataproc-startup[948]: <13>Mar 21 02:44:15 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:44:16 google-dataproc-startup[948]: <13>Mar 21 02:44:16 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:44:17 google-dataproc-startup[948]: <13>Mar 21 02:44:17 uninstall_artifacts[2147]: Purging configuration files for containerd.io (1.4.4-1) ...
<13>Mar 21 02:44:17 google-dataproc-startup[948]: <13>Mar 21 02:44:17 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:44:18 google-dataproc-startup[948]: <13>Mar 21 02:44:18 activate-component-hive-metastore[2188]: nc: connect to student-01-71e17b39d43c-qwiklab-m port 9083 (tcp) failed: Connection refused
<13>Mar 21 02:44:19 google-dataproc-startup[948]: <13>Mar 21 02:44:19 uninstall_artifacts[2147]: Purging configuration files for hive-webhcat (3.1.2-1) ...
<13>Mar 21 02:44:19 google-dataproc-startup[948]: <13>Mar 21 02:44:19 uninstall_artifacts[2147]: Purging configuration files for hadoop-hdfs-journalnode (3.2.2-1) ...
<13>Mar 21 02:44:19 google-dataproc-startup[948]: <13>Mar 21 02:44:19 activate-component-hive-metastore[2188]: Connection to student-01-71e17b39d43c-qwiklab-m 9083 port [tcp/*] succeeded!
<13>Mar 21 02:44:19 google-dataproc-startup[948]: <13>Mar 21 02:44:19 activate-component-hive-metastore[2188]: nc -v -z -w 1 student-01-71e17b39d43c-qwiklab-m 9083 succeeded.
<13>Mar 21 02:44:19 google-dataproc-startup[948]: <13>Mar 21 02:44:19 activate-component-hive-metastore[2188]: + return 0
<13>Mar 21 02:44:19 google-dataproc-startup[948]: <13>Mar 21 02:44:19 activate-component-hive-metastore[2188]: + loginfo 'Service up on host=student-01-71e17b39d43c-qwiklab-m port=9083 name=hive-metastore.'
<13>Mar 21 02:44:19 google-dataproc-startup[948]: <13>Mar 21 02:44:19 activate-component-hive-metastore[2188]: + echo 'Service up on host=student-01-71e17b39d43c-qwiklab-m port=9083 name=hive-metastore.'
<13>Mar 21 02:44:19 google-dataproc-startup[948]: <13>Mar 21 02:44:19 activate-component-hive-metastore[2188]: Service up on host=student-01-71e17b39d43c-qwiklab-m port=9083 name=hive-metastore.
<13>Mar 21 02:44:19 google-dataproc-startup[948]: <13>Mar 21 02:44:19 activate-component-hive-metastore[2188]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:44:19 google-dataproc-startup[948]: <13>Mar 21 02:44:19 activate-component-hive-metastore[2188]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh'
<13>Mar 21 02:44:19 google-dataproc-startup[948]: <13>Mar 21 02:44:19 activate-component-hive-metastore[2188]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Mar 21 02:44:19 google-dataproc-startup[948]: <13>Mar 21 02:44:19 activate-component-hive-metastore[2188]: + touch /tmp/dataproc/components/activate/hive-metastore.done
<13>Mar 21 02:44:19 google-dataproc-startup[948]: + (( status != 0 ))
<13>Mar 21 02:44:19 google-dataproc-startup[948]: + tee /tmp/dataproc/commands/2188.done
<13>Mar 21 02:44:19 google-dataproc-startup[948]: + echo 'Command cmd=[activate_component hive-metastore] pid=2188 exited with 0'
<13>Mar 21 02:44:19 google-dataproc-startup[948]: Command cmd=[activate_component hive-metastore] pid=2188 exited with 0
<13>Mar 21 02:44:19 google-dataproc-startup[948]: + (( ++i ))
<13>Mar 21 02:44:19 google-dataproc-startup[948]: + (( i < 14 ))
<13>Mar 21 02:44:19 google-dataproc-startup[948]: + local pid=2187
<13>Mar 21 02:44:19 google-dataproc-startup[948]: + local 'cmd=activate_component hdfs'
<13>Mar 21 02:44:19 google-dataproc-startup[948]: + loginfo 'Waiting on pid=2187 cmd=[activate_component hdfs]'
<13>Mar 21 02:44:19 google-dataproc-startup[948]: + echo 'Waiting on pid=2187 cmd=[activate_component hdfs]'
<13>Mar 21 02:44:19 google-dataproc-startup[948]: Waiting on pid=2187 cmd=[activate_component hdfs]
<13>Mar 21 02:44:19 google-dataproc-startup[948]: + echo 'activate_component hdfs'
<13>Mar 21 02:44:19 google-dataproc-startup[948]: + local status=0
<13>Mar 21 02:44:19 google-dataproc-startup[948]: + wait 2187
<13>Mar 21 02:44:19 google-dataproc-startup[948]: <13>Mar 21 02:44:19 uninstall_artifacts[2147]: Purging configuration files for zookeeper-server (3.4.14-1) ...
<13>Mar 21 02:44:20 google-dataproc-startup[948]: <13>Mar 21 02:44:20 uninstall_artifacts[2147]: Purging configuration files for cgroupfs-mount (1.4) ...
<13>Mar 21 02:44:21 google-dataproc-startup[948]: <13>Mar 21 02:44:21 uninstall_artifacts[2147]: Purging configuration files for ranger (2.0.0-1) ...
<13>Mar 21 02:44:21 google-dataproc-startup[948]: <13>Mar 21 02:44:21 uninstall_artifacts[2147]: Purging configuration files for kafka-server (2.3.1-1) ...
<13>Mar 21 02:44:21 google-dataproc-startup[948]: <13>Mar 21 02:44:21 uninstall_artifacts[2147]: Purging configuration files for zeppelin (0.9.0-1) ...
<13>Mar 21 02:44:21 google-dataproc-startup[948]: <13>Mar 21 02:44:21 activate-component-hdfs[2187]: sudo -u hdfs hadoop fs -chmod -R 1777 / succeeded.
<13>Mar 21 02:44:21 google-dataproc-startup[948]: <13>Mar 21 02:44:21 activate-component-hdfs[2187]: + return 0
<13>Mar 21 02:44:21 google-dataproc-startup[948]: <13>Mar 21 02:44:21 activate-component-hdfs[2187]: + retry_constant_short sudo -u hdfs hadoop fs -chgrp spark gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/spark-job-history
<13>Mar 21 02:44:21 google-dataproc-startup[948]: <13>Mar 21 02:44:21 activate-component-hdfs[2187]: + retry_constant_custom 30 1 sudo -u hdfs hadoop fs -chgrp spark gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/spark-job-history
<13>Mar 21 02:44:21 google-dataproc-startup[948]: <13>Mar 21 02:44:21 activate-component-hdfs[2187]: + local -r max_retry_time=30
<13>Mar 21 02:44:21 google-dataproc-startup[948]: <13>Mar 21 02:44:21 activate-component-hdfs[2187]: + local -r retry_delay=1
<13>Mar 21 02:44:21 google-dataproc-startup[948]: <13>Mar 21 02:44:21 activate-component-hdfs[2187]: + cmd=("${@:3}")
<13>Mar 21 02:44:21 google-dataproc-startup[948]: <13>Mar 21 02:44:21 activate-component-hdfs[2187]: + local -r cmd
<13>Mar 21 02:44:21 google-dataproc-startup[948]: <13>Mar 21 02:44:21 activate-component-hdfs[2187]: + local -r max_retries=30
<13>Mar 21 02:44:21 google-dataproc-startup[948]: <13>Mar 21 02:44:21 activate-component-hdfs[2187]: + set +x
<13>Mar 21 02:44:21 google-dataproc-startup[948]: <13>Mar 21 02:44:21 activate-component-hdfs[2187]: About to run 'sudo -u hdfs hadoop fs -chgrp spark gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/spark-job-history' with retries...
<13>Mar 21 02:44:21 google-dataproc-startup[948]: <13>Mar 21 02:44:21 uninstall_artifacts[2147]: Processing triggers for systemd (241-7~deb10u6) ...
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: bash -c DEBIAN_FRONTEND=noninteractive     apt-get autoremove -y --purge hadoop-hdfs-journalnode docker-ce ranger hadoop-yarn-nodemanager krb5-config hadoop-hdfs-zkfc druid zeppelin flink presto zookeeper-server krb5-kdc knox hive-webhcat-server krb5-kpropd kafka-server solr-server krb5-admin-server xinetd hadoop-hdfs-datanode krb5-user succeeded.
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + return 0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + uninstall_components docker-ce druid flink hbase hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent ranger solr-server zeppelin zookeeper-server
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + components=("$@")
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local components
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + mkdir -p /tmp/dataproc/components/uninstall
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + for component in "${components[@]}"
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + loginfo 'Uninstalling component docker-ce'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Uninstalling component docker-ce'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Uninstalling component docker-ce
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + uninstall_component docker-ce
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r component=docker-ce
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/docker-ce.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/docker-ce.sh ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/docker-ce.sh'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/docker-ce.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + touch /tmp/dataproc/components/uninstall/docker-ce.running
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local exit_code=0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/docker-ce.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/uninstall/docker-ce.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + source /usr/local/share/google/dataproc/bdutil/components/uninstall/../shared/docker.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ readonly DOCKER_PATH=/var/lib/docker
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ DOCKER_PATH=/var/lib/docker
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ readonly GCR_CREDENTIAL_HELPER_VERSION=2.0.0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ GCR_CREDENTIAL_HELPER_VERSION=2.0.0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + delete_docker_bridge
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + ip link show docker0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: 3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default 
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]:     link/ether 02:42:56:66:b3:e5 brd ff:ff:ff:ff:ff:ff
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + ip link del docker0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + rm -rf /var/lib/docker
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + rm -f /usr/local/share/google/dataproc/licenses/DOCKER_LICENSE
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + rm -rf /etc/docker
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + rm -rf /run/docker
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + touch /tmp/dataproc/components/uninstall/docker-ce.done
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + for component in "${components[@]}"
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + loginfo 'Uninstalling component druid'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Uninstalling component druid'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Uninstalling component druid
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + uninstall_component druid
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r component=druid
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/druid.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/druid.sh ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Component druid doesn'\''t have an uninstall script'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Component druid doesn't have an uninstall script
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + for component in "${components[@]}"
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + loginfo 'Uninstalling component flink'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Uninstalling component flink'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Uninstalling component flink
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + uninstall_component flink
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r component=flink
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/flink.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/flink.sh ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/flink.sh'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/flink.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + touch /tmp/dataproc/components/uninstall/flink.running
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local exit_code=0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/flink.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/uninstall/flink.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + source /usr/local/share/google/dataproc/bdutil/components/uninstall/../shared/flink.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ readonly FLINK_INSTALL_DIR=/usr/lib/flink
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ FLINK_INSTALL_DIR=/usr/lib/flink
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ readonly FLINK_WORKING_DIR=/var/lib/flink
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ FLINK_WORKING_DIR=/var/lib/flink
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ readonly FLINK_YARN_SCRIPT=/usr/bin/flink-yarn-daemon
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ FLINK_YARN_SCRIPT=/usr/bin/flink-yarn-daemon
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ readonly HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ readonly FLINK_NETWORK_NUM_BUFFERS=2048
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ FLINK_NETWORK_NUM_BUFFERS=2048
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ readonly FLINK_JOBMANAGER_MEMORY_FRACTION=.95
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ FLINK_JOBMANAGER_MEMORY_FRACTION=.95
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ readonly FLINK_TASKMANAGER_MEMORY_FRACTION=.95
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ FLINK_TASKMANAGER_MEMORY_FRACTION=.95
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ readonly START_FLINK_YARN_SESSION_METADATA_KEY=flink-start-yarn-session
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ START_FLINK_YARN_SESSION_METADATA_KEY=flink-start-yarn-session
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ readonly START_FLINK_YARN_SESSION_DEFAULT=false
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ START_FLINK_YARN_SESSION_DEFAULT=false
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + rm -rf /var/lib/flink
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + touch /tmp/dataproc/components/uninstall/flink.done
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + for component in "${components[@]}"
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + loginfo 'Uninstalling component hbase'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Uninstalling component hbase'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Uninstalling component hbase
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + uninstall_component hbase
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r component=hbase
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/hbase.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/hbase.sh ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/hbase.sh'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/hbase.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + touch /tmp/dataproc/components/uninstall/hbase.running
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local exit_code=0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/hbase.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + set -euxo pipefail
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + touch /tmp/dataproc/components/uninstall/hbase.done
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + for component in "${components[@]}"
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + loginfo 'Uninstalling component hive-webhcat-server'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Uninstalling component hive-webhcat-server'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Uninstalling component hive-webhcat-server
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + uninstall_component hive-webhcat-server
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r component=hive-webhcat-server
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/hive-webhcat-server.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/hive-webhcat-server.sh ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Component hive-webhcat-server doesn'\''t have an uninstall script'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Component hive-webhcat-server doesn't have an uninstall script
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + for component in "${components[@]}"
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + loginfo 'Uninstalling component jupyter'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Uninstalling component jupyter'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Uninstalling component jupyter
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + uninstall_component jupyter
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r component=jupyter
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + touch /tmp/dataproc/components/uninstall/jupyter.running
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local exit_code=0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + set -euxo pipefail
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + source /usr/local/share/google/dataproc/bdutil/components/uninstall/../../bdutil_helpers.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ is_centos
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: +++ . /etc/os-release
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++++ NAME='Debian GNU/Linux'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++++ VERSION_ID=10
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++++ VERSION='10 (buster)'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++++ VERSION_CODENAME=buster
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++++ ID=debian
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: +++ echo debian
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ return 1
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + source /usr/local/share/google/dataproc/bdutil/components/uninstall/../shared/jupyter.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ is_version_at_least 2.0 2.0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ local -r version1=2.0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ local -r version2=2.0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ loginfo 'Comparing if version 2.0 is at least version 2.0 '
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ echo 'Comparing if version 2.0 is at least version 2.0 '
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Comparing if version 2.0 is at least version 2.0 
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ compare_versions 2.0 2.0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ local -r version1=2.0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ local -r version2=2.0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ validate_version 2.0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ local -r version=2.0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ [[ 2.0 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ return 0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ validate_version 2.0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ local -r version=2.0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ [[ 2.0 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ return 0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ [[ 2.0 == \2\.\0 ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ return 0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ case $? in
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ return 0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ export CONDA_DIR=/opt/conda/miniconda3
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ CONDA_DIR=/opt/conda/miniconda3
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ export CONDA_BIN_DIR=/opt/conda/miniconda3/bin
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ CONDA_BIN_DIR=/opt/conda/miniconda3/bin
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ export PATH=/opt/conda/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ PATH=/opt/conda/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ export JUPYTER_DATAPROC_DIR=/opt/dataproc/jupyter
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ JUPYTER_DATAPROC_DIR=/opt/dataproc/jupyter
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ export JUPYTER_WHEEL=/opt/dataproc/jupyter/pip_wheel.whl
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ JUPYTER_WHEEL=/opt/dataproc/jupyter/pip_wheel.whl
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ export JUPYTER_EXTRA_PACKAGES=/opt/dataproc/jupyter/pip_wheel.requirements
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ JUPYTER_EXTRA_PACKAGES=/opt/dataproc/jupyter/pip_wheel.requirements
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ export JUPYTER_EXTRA_PACKAGES_DEPS=/opt/dataproc/jupyter/pip_wheel_deps.requirements
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ JUPYTER_EXTRA_PACKAGES_DEPS=/opt/dataproc/jupyter/pip_wheel_deps.requirements
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ export JUPYTER_ETC_DIR=/etc/jupyter
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ JUPYTER_ETC_DIR=/etc/jupyter
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ export JUPYTER_ENV_FILE=/etc/default/jupyter
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ JUPYTER_ENV_FILE=/etc/default/jupyter
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ export JUPYTER_CONFIG_FILE=/etc/jupyter/jupyter_notebook_config.py
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ JUPYTER_CONFIG_FILE=/etc/jupyter/jupyter_notebook_config.py
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ export PYSPARK_KERNELSPEC=/opt/conda/miniconda3/share/jupyter/kernels/pyspark/kernel.json
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ PYSPARK_KERNELSPEC=/opt/conda/miniconda3/share/jupyter/kernels/pyspark/kernel.json
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ export R_KERNELSPEC=/opt/conda/miniconda3/share/jupyter/kernels/ir/kernel.json
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ R_KERNELSPEC=/opt/conda/miniconda3/share/jupyter/kernels/ir/kernel.json
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ export JUPYTER_SYSTEMD_UNIT=/usr/lib/systemd/system/jupyter.service
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ JUPYTER_SYSTEMD_UNIT=/usr/lib/systemd/system/jupyter.service
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ export JUPYTERHUB_SYSTEMD_UNIT=/usr/lib/systemd/system/jupyterhub.service
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ JUPYTERHUB_SYSTEMD_UNIT=/usr/lib/systemd/system/jupyterhub.service
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + rm -Rf /opt/dataproc/jupyter /etc/jupyter
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + is_version_at_least 2.0 1.5
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r version1=2.0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r version2=1.5
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + loginfo 'Comparing if version 2.0 is at least version 1.5 '
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Comparing if version 2.0 is at least version 1.5 '
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Comparing if version 2.0 is at least version 1.5 
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + compare_versions 2.0 1.5
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r version1=2.0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r version2=1.5
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + validate_version 2.0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r version=2.0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + [[ 2.0 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + return 0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + validate_version 1.5
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r version=1.5
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r 're=^[0-9]+(\.[0-9]+)*$'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + [[ 1.5 =~ ^[0-9]+(\.[0-9]+)*$ ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + return 0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + [[ 2.0 == \1\.\5 ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local IFS=.
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + ver1=($version1)
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + ver2=($version2)
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local i ver1 ver2
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + (( i=2 ))
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + (( i<2 ))
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + (( i=0 ))
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + (( i<2 ))
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + [[ -z 1 ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + (( 10#2 > 10#1 ))
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + return 1
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + case $? in
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + return 0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + rm -rf '/opt/conda/miniconda3/share/jupyter/lab/extensions/*'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + touch /tmp/dataproc/components/uninstall/jupyter.done
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + for component in "${components[@]}"
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + loginfo 'Uninstalling component kafka-server'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Uninstalling component kafka-server'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Uninstalling component kafka-server
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + uninstall_component kafka-server
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r component=kafka-server
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/kafka-server.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/kafka-server.sh ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Component kafka-server doesn'\''t have an uninstall script'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Component kafka-server doesn't have an uninstall script
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + for component in "${components[@]}"
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + loginfo 'Uninstalling component kerberos'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Uninstalling component kerberos'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Uninstalling component kerberos
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + uninstall_component kerberos
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r component=kerberos
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + touch /tmp/dataproc/components/uninstall/kerberos.running
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local exit_code=0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + set -euxo pipefail
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + touch /tmp/dataproc/components/uninstall/kerberos.done
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + for component in "${components[@]}"
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + loginfo 'Uninstalling component knox'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Uninstalling component knox'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Uninstalling component knox
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + uninstall_component knox
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r component=knox
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/knox.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/knox.sh ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Component knox doesn'\''t have an uninstall script'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Component knox doesn't have an uninstall script
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + for component in "${components[@]}"
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + loginfo 'Uninstalling component presto'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Uninstalling component presto'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Uninstalling component presto
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + uninstall_component presto
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r component=presto
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Component presto doesn'\''t have an uninstall script'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Component presto doesn't have an uninstall script
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + for component in "${components[@]}"
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + loginfo 'Uninstalling component proxy-agent'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Uninstalling component proxy-agent'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Uninstalling component proxy-agent
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + uninstall_component proxy-agent
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r component=proxy-agent
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + touch /tmp/dataproc/components/uninstall/proxy-agent.running
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local exit_code=0
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + set -exo pipefail
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + source /usr/local/share/google/dataproc/bdutil/components/uninstall/../shared/proxy-agent.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ readonly PROXY_AGENT_SERVICE_NAME=google-dataproc-component-gateway
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ PROXY_AGENT_SERVICE_NAME=google-dataproc-component-gateway
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ readonly PROXY_AGENT_INSTALL_LOCATION=/usr/bin/proxy-forwarding-agent
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ PROXY_AGENT_INSTALL_LOCATION=/usr/bin/proxy-forwarding-agent
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ readonly PROXY_AGENT_INIT_SCRIPT=/usr/lib/systemd/system/google-dataproc-component-gateway.service
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: ++ PROXY_AGENT_INIT_SCRIPT=/usr/lib/systemd/system/google-dataproc-component-gateway.service
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + rm -f /usr/bin/proxy-forwarding-agent
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + rm -f /usr/lib/systemd/system/google-dataproc-component-gateway.service
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + touch /tmp/dataproc/components/uninstall/proxy-agent.done
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + for component in "${components[@]}"
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + loginfo 'Uninstalling component ranger'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Uninstalling component ranger'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Uninstalling component ranger
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + uninstall_component ranger
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r component=ranger
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/ranger.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/ranger.sh ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Component ranger doesn'\''t have an uninstall script'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Component ranger doesn't have an uninstall script
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + for component in "${components[@]}"
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + loginfo 'Uninstalling component solr-server'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Uninstalling component solr-server'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Uninstalling component solr-server
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + uninstall_component solr-server
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r component=solr-server
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/solr-server.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/solr-server.sh ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Component solr-server doesn'\''t have an uninstall script'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Component solr-server doesn't have an uninstall script
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + for component in "${components[@]}"
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + loginfo 'Uninstalling component zeppelin'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Uninstalling component zeppelin'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Uninstalling component zeppelin
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + uninstall_component zeppelin
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r component=zeppelin
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/zeppelin.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/zeppelin.sh ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Component zeppelin doesn'\''t have an uninstall script'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Component zeppelin doesn't have an uninstall script
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + for component in "${components[@]}"
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + loginfo 'Uninstalling component zookeeper-server'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Uninstalling component zookeeper-server'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Uninstalling component zookeeper-server
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + uninstall_component zookeeper-server
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r component=zookeeper-server
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + local -r uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/zookeeper-server.sh
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/zookeeper-server.sh ]]
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: + echo 'Component zookeeper-server doesn'\''t have an uninstall script'
<13>Mar 21 02:44:24 google-dataproc-startup[948]: <13>Mar 21 02:44:24 uninstall_artifacts[2147]: Component zookeeper-server doesn't have an uninstall script
<13>Mar 21 02:44:25 google-dataproc-startup[948]: <13>Mar 21 02:44:25 activate-component-hdfs[2187]: sudo -u hdfs hadoop fs -chgrp spark gs://dataproc-temp-us-central1-147286270203-nrwqwbuk/1508fba4-d944-4b05-b441-abe36766e1e0/spark-job-history succeeded.
<13>Mar 21 02:44:25 google-dataproc-startup[948]: <13>Mar 21 02:44:25 activate-component-hdfs[2187]: + return 0
<13>Mar 21 02:44:25 google-dataproc-startup[948]: <13>Mar 21 02:44:25 activate-component-hdfs[2187]: + start_after_hdfs_services
<13>Mar 21 02:44:25 google-dataproc-startup[948]: <13>Mar 21 02:44:25 activate-component-hdfs[2187]: + services=(${DATAPROC_START_AFTER_HDFS_SERVICES})
<13>Mar 21 02:44:25 google-dataproc-startup[948]: <13>Mar 21 02:44:25 activate-component-hdfs[2187]: + local services
<13>Mar 21 02:44:25 google-dataproc-startup[948]: <13>Mar 21 02:44:25 activate-component-hdfs[2187]: + for service in "${services[@]}"
<13>Mar 21 02:44:25 google-dataproc-startup[948]: <13>Mar 21 02:44:25 activate-component-hdfs[2187]: + start_service hadoop-mapreduce-historyserver
<13>Mar 21 02:44:25 google-dataproc-startup[948]: <13>Mar 21 02:44:25 activate-component-hdfs[2187]: + local -r service=hadoop-mapreduce-historyserver
<13>Mar 21 02:44:25 google-dataproc-startup[948]: <13>Mar 21 02:44:25 activate-component-hdfs[2187]: + local -r unit=hadoop-mapreduce-historyserver.service
<13>Mar 21 02:44:25 google-dataproc-startup[948]: <13>Mar 21 02:44:25 activate-component-hdfs[2187]: + retry_constant_short systemctl start hadoop-mapreduce-historyserver.service
<13>Mar 21 02:44:25 google-dataproc-startup[948]: <13>Mar 21 02:44:25 activate-component-hdfs[2187]: + retry_constant_custom 30 1 systemctl start hadoop-mapreduce-historyserver.service
<13>Mar 21 02:44:25 google-dataproc-startup[948]: <13>Mar 21 02:44:25 activate-component-hdfs[2187]: + local -r max_retry_time=30
<13>Mar 21 02:44:25 google-dataproc-startup[948]: <13>Mar 21 02:44:25 activate-component-hdfs[2187]: + local -r retry_delay=1
<13>Mar 21 02:44:25 google-dataproc-startup[948]: <13>Mar 21 02:44:25 activate-component-hdfs[2187]: + cmd=("${@:3}")
<13>Mar 21 02:44:25 google-dataproc-startup[948]: <13>Mar 21 02:44:25 activate-component-hdfs[2187]: + local -r cmd
<13>Mar 21 02:44:25 google-dataproc-startup[948]: <13>Mar 21 02:44:25 activate-component-hdfs[2187]: + local -r max_retries=30
<13>Mar 21 02:44:25 google-dataproc-startup[948]: <13>Mar 21 02:44:25 activate-component-hdfs[2187]: + set +x
<13>Mar 21 02:44:25 google-dataproc-startup[948]: <13>Mar 21 02:44:25 activate-component-hdfs[2187]: About to run 'systemctl start hadoop-mapreduce-historyserver.service' with retries...
<13>Mar 21 02:44:32 google-dataproc-startup[948]: <13>Mar 21 02:44:32 activate-component-hdfs[2187]: systemctl start hadoop-mapreduce-historyserver.service succeeded.
<13>Mar 21 02:44:32 google-dataproc-startup[948]: <13>Mar 21 02:44:32 activate-component-hdfs[2187]: + return 0
<13>Mar 21 02:44:32 google-dataproc-startup[948]: <13>Mar 21 02:44:32 activate-component-hdfs[2187]: + return
<13>Mar 21 02:44:32 google-dataproc-startup[948]: <13>Mar 21 02:44:32 activate-component-hdfs[2187]: + [[ 2 == 0 ]]
<13>Mar 21 02:44:32 google-dataproc-startup[948]: <13>Mar 21 02:44:32 activate-component-hdfs[2187]: + [[ 0 -ne 0 ]]
<13>Mar 21 02:44:32 google-dataproc-startup[948]: <13>Mar 21 02:44:32 activate-component-hdfs[2187]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh'
<13>Mar 21 02:44:32 google-dataproc-startup[948]: <13>Mar 21 02:44:32 activate-component-hdfs[2187]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Mar 21 02:44:32 google-dataproc-startup[948]: <13>Mar 21 02:44:32 activate-component-hdfs[2187]: + touch /tmp/dataproc/components/activate/hdfs.done
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + (( status != 0 ))
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + tee /tmp/dataproc/commands/2187.done
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + echo 'Command cmd=[activate_component hdfs] pid=2187 exited with 0'
<13>Mar 21 02:44:32 google-dataproc-startup[948]: Command cmd=[activate_component hdfs] pid=2187 exited with 0
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + (( ++i ))
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + (( i < 14 ))
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + local pid=2185
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + local 'cmd=setup_service hadoop-yarn-timelineserver'
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + loginfo 'Waiting on pid=2185 cmd=[setup_service hadoop-yarn-timelineserver]'
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + echo 'Waiting on pid=2185 cmd=[setup_service hadoop-yarn-timelineserver]'
<13>Mar 21 02:44:32 google-dataproc-startup[948]: Waiting on pid=2185 cmd=[setup_service hadoop-yarn-timelineserver]
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + echo 'setup_service hadoop-yarn-timelineserver'
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + local status=0
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + wait 2185
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + (( status != 0 ))
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + tee /tmp/dataproc/commands/2185.done
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + echo 'Command cmd=[setup_service hadoop-yarn-timelineserver] pid=2185 exited with 0'
<13>Mar 21 02:44:32 google-dataproc-startup[948]: Command cmd=[setup_service hadoop-yarn-timelineserver] pid=2185 exited with 0
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + (( ++i ))
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + (( i < 14 ))
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + local pid=2184
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + local 'cmd=setup_service hadoop-mapreduce-historyserver'
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + loginfo 'Waiting on pid=2184 cmd=[setup_service hadoop-mapreduce-historyserver]'
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + echo 'Waiting on pid=2184 cmd=[setup_service hadoop-mapreduce-historyserver]'
<13>Mar 21 02:44:32 google-dataproc-startup[948]: Waiting on pid=2184 cmd=[setup_service hadoop-mapreduce-historyserver]
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + echo 'setup_service hadoop-mapreduce-historyserver'
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + local status=0
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + wait 2184
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + (( status != 0 ))
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + tee /tmp/dataproc/commands/2184.done
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + echo 'Command cmd=[setup_service hadoop-mapreduce-historyserver] pid=2184 exited with 0'
<13>Mar 21 02:44:32 google-dataproc-startup[948]: Command cmd=[setup_service hadoop-mapreduce-historyserver] pid=2184 exited with 0
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + (( ++i ))
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + (( i < 14 ))
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + local pid=2183
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + local 'cmd=setup_service hadoop-yarn-resourcemanager'
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + loginfo 'Waiting on pid=2183 cmd=[setup_service hadoop-yarn-resourcemanager]'
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + echo 'Waiting on pid=2183 cmd=[setup_service hadoop-yarn-resourcemanager]'
<13>Mar 21 02:44:32 google-dataproc-startup[948]: Waiting on pid=2183 cmd=[setup_service hadoop-yarn-resourcemanager]
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + echo 'setup_service hadoop-yarn-resourcemanager'
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + local status=0
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + wait 2183
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + (( status != 0 ))
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + tee /tmp/dataproc/commands/2183.done
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + echo 'Command cmd=[setup_service hadoop-yarn-resourcemanager] pid=2183 exited with 0'
<13>Mar 21 02:44:32 google-dataproc-startup[948]: Command cmd=[setup_service hadoop-yarn-resourcemanager] pid=2183 exited with 0
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + (( ++i ))
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + (( i < 14 ))
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + local pid=2147
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + local cmd=uninstall_artifacts
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + loginfo 'Waiting on pid=2147 cmd=[uninstall_artifacts]'
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + echo 'Waiting on pid=2147 cmd=[uninstall_artifacts]'
<13>Mar 21 02:44:32 google-dataproc-startup[948]: Waiting on pid=2147 cmd=[uninstall_artifacts]
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + echo uninstall_artifacts
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + local status=0
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + wait 2147
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + (( status != 0 ))
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + tee /tmp/dataproc/commands/2147.done
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + echo 'Command cmd=[uninstall_artifacts] pid=2147 exited with 0'
<13>Mar 21 02:44:32 google-dataproc-startup[948]: Command cmd=[uninstall_artifacts] pid=2147 exited with 0
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + (( ++i ))
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + (( i < 14 ))
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + BACKGROUND_PROCESSES=()
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + BACKGROUND_COMMANDS=()
<13>Mar 21 02:44:32 google-dataproc-startup[948]: + systemctl daemon-reload
<13>Mar 21 02:44:33 google-dataproc-startup[948]: + enable_snap_update
<13>Mar 21 02:44:33 google-dataproc-startup[948]: + command -v snap
<13>Mar 21 02:44:33 google-dataproc-startup[948]: + loginfo 'All done'
<13>Mar 21 02:44:33 google-dataproc-startup[948]: + echo 'All done'
<13>Mar 21 02:44:33 google-dataproc-startup[948]: All done
